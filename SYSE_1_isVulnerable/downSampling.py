import pickle
import os
import numpy as np
import random
import shutil
import gc
""" Binary down sampling methods """
def appendCaseIDLabel0 (vectorTrainPath):
    """
        Takes a single pkl file, divides up the test case ids based on whether they have or don't have a vulnerability, and returns:
        - the case ids with vulnerabilities
        - the case ids without vulnerabilities
        - the count of case ids with vulnerabilities (for use on how much we should down size aka downSampleNum)
    """
    for file in os.listdir(vectorTrainPath):
        if (file.endswith('.pkl')) and ('balanced' not in file):
            print(vectorTrainPath + file)
            with open(vectorTrainPath + file, 'rb') as f:
                data = pickle.load(f)
                print("Elements in each vector .pkl file: ", len(data))
                label_array = data[1]
                caseID_array = data[5]
                caseID_zero = []
                caseID_one = []
                count_1 = 0
                count_0 = 0
                totalsamples = len(label_array)
                for i in range (totalsamples):
                    if  label_array[i] ==1:
                        count_1 = count_1 + 1
                        caseID_one.append(caseID_array[i])
                    else:
                        count_0 = count_0 + 1
                        caseID_zero.append(caseID_array[i])#contain case id with class label 1
          
    return  caseID_one, caseID_zero, count_1

def downsampling (caseID_one, caseID_zero, downsampleNum, seed,  vectorPath, trainpath):
    """
        Reduces the amount of non vulnerable samples to match the amount of vulnerable samples in our train.pkl.
        Outputs an array of arrays to ./data/DLvectors/train/balancedClassTrain.pkl.
        Combines all the [token vectors, labels, functions, filenames, vul' types, test case ids of slices] (one array w/6 nested arrays of these entries)
    """ 	    
    print("Downsampling with seed number: " , seed)
    np.random.seed(seed)
    randomCase = caseID_zero
    np.random.shuffle(randomCase)
    reducedCase_0 = randomCase[0:downsampleNum]
    combinedCaseID = caseID_one + reducedCase_0
    np.random.seed(seed)
    randomCombinedCase = combinedCaseID
    np.random.shuffle(randomCombinedCase)
    
    train_set = [[], [], [], [], [], []]
    
    for foldername in os.listdir(vectorPath):  #./data/vector/
        if foldername.endswith('.pkl') or foldername.endswith('DS_Store'):#loop folder
            continue
        else:
            for caseID in randomCombinedCase:
                if (caseID == foldername):
                    for pkl in os.listdir(vectorPath + foldername):
                        if pkl.endswith('.pkl'):
                            f = open(vectorPath + foldername + '/'  + pkl , 'rb') #'./data/vector/8508/8508.pkl'
                            data = pickle.load(f)
                            train_set[0].append(data[0][0]) # tokens 
                            train_set[1].append(data[1][0]) # labels
                            train_set[2].append(data[2][0]) # function lists
                            train_set[3].append(data[3][0]) # filenames
                            train_set[4].append(data[4][0]) # vulnerability types
                            train_set[5].append(foldername) # test case ids
    print("Total samples in training set: ", len(train_set[0]))
    f_train = open(trainpath + "balancedClassTrain.pkl", 'wb') # './data/DLvectors/train/'
    pickle.dump(train_set, f_train, protocol=pickle.HIGHEST_PROTOCOL)
    f_train.close()
    del train_set
    gc.collect() 
    print("Done! balancedClassTrain.pkl saved in ./data/DLvectors/train/ ")

def isClassBalanced(traindatapath):
    count_1 = 0
    count_0 = 0
    for filename in os.listdir(traindatapath):  #./data/DLvectors/train/
        if filename.endswith('DS_Store'):#loop folder
            continue
        if ("balanced" in filename) and (filename.endswith('.pkl')):#loop for each train.pkl and extrak class.pkl
            print(filename)
            f = open(traindatapath + filename, 'rb') #'./data/DLvectors/train/8508/api8508.pkl'
            data = pickle.load(f)
            label_array = data[1]
            totalsamples = len(label_array)
            for i in range (totalsamples):
                if label_array[i] == 1:
                    count_1 = count_1 + 1
                else:
                    count_0 = count_0 + 1
            print ("total label 0: " ,count_0)
            print ("total label 1: " ,count_1)
            print ("total sample: ", count_0 + count_1 )
    if (count_0 == count_1):
        return True
    else:
        return False

""" Multiclass down sampling methods """
def appendCaseIDLabels (vectorTrainPath):
    """
        Takes a single pkl file, divides up the test case ids based on their vulnerability type category, and returns:
        - an array of arrays, where each inner array contains the category and its count
        - the count of case ids with the smallest samples (for use on how much we should down size aka downSampleNum)
    """
    for file in os.listdir(vectorTrainPath):
        if (file.endswith('.pkl')) and ('balanced' not in file):
            print(vectorTrainPath + file)
            with open(vectorTrainPath + file, 'rb') as f:
                data = pickle.load(f)
                label_array = data[1]
                caseID_array = data[5]
                category_counts = np.unique(label_array, return_counts=True)
                _printCatCounts(category_counts)

                categoryCasesDict = _getCatCaseIdDict(category_counts[0])
                totalsamples = len(label_array)
                for i in range(totalsamples):
                    cat = label_array[i]
                    caseid = caseID_array[i]
                    categoryCasesDict[cat].append(caseid)

    return  categoryCasesDict, min(category_counts[1])

def downsampling2(caseID_one, caseID_zero, downsampleNum, seed,  vectorPath, trainpath):
    """
        Reduces the amount of non vulnerable samples to match the amount of vulnerable samples in our train.pkl.
        Outputs an array of arrays to ./data/DLvectors/train/balancedClassTrain.pkl.
        Combines all the [token vectors, labels, functions, filenames, vul' types, test case ids of slices] (one array w/6 nested arrays of these entries)
    """ 	    
    print("Downsampling with seed number: " , seed)
    np.random.seed(seed)
    randomCase = caseID_zero
    np.random.shuffle(randomCase)
    reducedCase_0 = randomCase[0:downsampleNum]
    combinedCaseID = caseID_one + reducedCase_0
    np.random.seed(seed)
    randomCombinedCase = combinedCaseID
    np.random.shuffle(randomCombinedCase)
    
    train_set = [[], [], [], [], [], []]
    
    for foldername in os.listdir(vectorPath):  #./data/vector/
        if foldername.endswith('.pkl') or foldername.endswith('DS_Store'):#loop folder
            continue
        else:
            for caseID in randomCombinedCase:
                if (caseID == foldername):
                    for pkl in os.listdir(vectorPath + foldername):
                        if pkl.endswith('.pkl'):
                            f = open(vectorPath + foldername + '/'  + pkl , 'rb') #'./data/vector/8508/8508.pkl'
                            data = pickle.load(f)
                            train_set[0].append(data[0][0]) # tokens 
                            train_set[1].append(data[1][0]) # labels
                            train_set[2].append(data[2][0]) # function lists
                            train_set[3].append(data[3][0]) # filenames
                            train_set[4].append(data[4][0]) # vulnerability types
                            train_set[5].append(foldername) # test case ids
    print("Total samples in training set: ", len(train_set[0]))
    f_train = open(trainpath + "balancedClassTrain.pkl", 'wb') # './data/DLvectors/train/'
    pickle.dump(train_set, f_train, protocol=pickle.HIGHEST_PROTOCOL)
    f_train.close()
    del train_set
    gc.collect() 
    print("Done! balancedClassTrain.pkl saved in ./data/DLvectors/train/ ")

def isClassBalanced2(traindatapath):
    count_1 = 0
    count_0 = 0
    for filename in os.listdir(traindatapath):  #./data/DLvectors/train/
        if filename.endswith('DS_Store'):#loop folder
            continue
        if ("balanced" in filename) and (filename.endswith('.pkl')):#loop for each train.pkl and extrak class.pkl
            print(filename)
            f = open(traindatapath + filename, 'rb') #'./data/DLvectors/train/8508/api8508.pkl'
            data = pickle.load(f)
            label_array = data[1]
            totalsamples = len(label_array)
            for i in range (totalsamples):
                if label_array[i] == 1:
                    count_1 = count_1 + 1
                else:
                    count_0 = count_0 + 1
            print ("total label 0: " ,count_0)
            print ("total label 1: " ,count_1)
            print ("total sample: ", count_0 + count_1 )
    if (count_0 == count_1):
        return True
    else:
        return False

def _printCatCounts(classcounts):
    print('GROUP ID\tCOUNT')
    for i in range(len(classcounts[0])):
        print(f'{classcounts[0][i]}\t{classcounts[1][i]}')

def _getCatCaseIdDict(categories):
    categoryCasesDict = {}
    for c in categories:
        categoryCasesDict[c] = []
    return categoryCasesDict
    