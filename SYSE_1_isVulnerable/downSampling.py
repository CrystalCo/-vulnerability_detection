import pickle
import os
import numpy as np
import random
import shutil
import gc

from utils.utils import init_nested_arrays

""" Binary down sampling methods """
def appendCaseIDLabel0 (vectorTrainPath, vType='ALL'):
    """
        Takes a single pkl file, divides up the test case ids based on whether they have or don't have a vulnerability, and returns:
        - the case ids with vulnerabilities
        - the case ids without vulnerabilities
        - the count of case ids with vulnerabilities (for use on how much we should down size aka downSampleNum)
    """
    for file in os.listdir(vectorTrainPath):
        if (file.endswith('.pkl')) and (vType in file):
            print(vectorTrainPath + file)
            with open(vectorTrainPath + file, 'rb') as f:
                data = pickle.load(f)
                print("Elements in each vector .pkl file: ", len(data))
                label_array = data[1]
                caseID_array = data[-1]
                caseID_zero = []
                caseID_one = []
                count_1 = 0
                count_0 = 0
                totalsamples = len(label_array)
                for i in range (totalsamples):
                    if  label_array[i] ==1:
                        count_1 = count_1 + 1
                        caseID_one.append(caseID_array[i])
                    else:
                        count_0 = count_0 + 1
                        caseID_zero.append(caseID_array[i])#contain case id with class label 1
          
    return  caseID_one, caseID_zero, count_1

def downsampling (caseID_one, caseID_zero, downsampleNum, seed,  vectorPath, trainpath):
    """
        Reduces the amount of non vulnerable samples to match the amount of vulnerable samples in our train.pkl.
        Outputs an array of arrays to ./data/DLvectors/train/balancedClassTrain.pkl.
        Combines all the [token vectors, labels, functions, filenames, vul' types, test case ids of slices] (one array w/6 nested arrays of these entries)
    """ 	    
    print("Downsampling with seed number: " , seed)
    np.random.seed(seed)
    randomCase = caseID_zero
    np.random.shuffle(randomCase)
    reducedCase_0 = randomCase[0:downsampleNum]
    combinedCaseID = caseID_one + reducedCase_0
    np.random.seed(seed)
    randomCombinedCase = combinedCaseID
    np.random.shuffle(randomCombinedCase)
    
    train_set = []
    
    for foldername in os.listdir(vectorPath):  #./data/vector/
        if foldername.endswith('.pkl') or foldername.endswith('DS_Store'):#loop folder
            continue
        for caseID in randomCombinedCase:
            if (caseID == foldername):
                for pkl in os.listdir(vectorPath + foldername):
                    if pkl.endswith('.pkl'):
                        f = open(vectorPath + foldername + '/'  + pkl , 'rb') #'./data/vector/8508/8508.pkl'
                        data = pickle.load(f)
                        if train_set == []:
                            train_set = init_nested_arrays(len(data))
                        for i in range(len(data)):
                            train_set[i].append(data[i][0])
    print("Total samples in training set: ", len(train_set[0]))
    f_train = open(trainpath + "balancedClassTrain.pkl", 'wb') # './data/DLvectors/train/'
    pickle.dump(train_set, f_train, protocol=pickle.HIGHEST_PROTOCOL)
    f_train.close()
    del train_set
    gc.collect() 
    print("Done! balancedClassTrain.pkl saved in ./data/DLvectors/train/ ")

def isClassBalanced(traindatapath):
    count_1 = 0
    count_0 = 0
    for filename in os.listdir(traindatapath):  #./data/DLvectors/train/
        if filename.endswith('DS_Store'):#loop folder
            continue
        if ("balanced" in filename) and (filename.endswith('.pkl')):#loop for each train.pkl and extrak class.pkl
            print(filename)
            f = open(traindatapath + filename, 'rb') #'./data/DLvectors/train/8508/api8508.pkl'
            data = pickle.load(f)
            label_array = data[1]
            totalsamples = len(label_array)
            for i in range (totalsamples):
                if label_array[i] == 1:
                    count_1 = count_1 + 1
                else:
                    count_0 = count_0 + 1
            print ("total label 0: " ,count_0)
            print ("total label 1: " ,count_1)
            print ("total sample: ", count_0 + count_1 )
    if (count_0 == count_1):
        return True
    else:
        return False

""" Multiclass down sampling methods """
def appendCaseIDLabels (vectorTrainPath):
    """
        Takes a single pkl file, divides up the test case ids based on their vulnerability type category, and returns:
        - an array of arrays, where each inner array contains the category and its count
        - the count of case ids with the smallest samples (for use on how much we should down size aka downSampleNum)
    """
    for file in os.listdir(vectorTrainPath):
        if (file.endswith('.pkl')) and ('balanced' not in file):
            print(vectorTrainPath + file)
            with open(vectorTrainPath + file, 'rb') as f:
                data = pickle.load(f)
                labels = data[1]
                caseID_array = data[5]
                label_array = [x[0] for x in labels]
                category_counts = np.unique(label_array, return_counts=True)
                _printCatCounts(category_counts)

                categoryCasesDict = _getCatCaseIdDict(category_counts[0])
                totalsamples = len(label_array)
                for i in range(totalsamples):
                    cat = label_array[i]
                    caseid = caseID_array[i]
                    categoryCasesDict[cat].append(caseid)

    return  categoryCasesDict, min(category_counts[1])

def downsamplingMulticlass(categoryCasesDict, downsampleNum, seed,  vectorPath, trainpath):
    """
        Reduces the amount of samples for each class to match the amount of samples in the class with the least amount of samples in train.pkl.
        Outputs an array of arrays to ./data/DLvectors/train/balancedClassTrain.pkl.
        Combines all the [token vectors, labels, functions, filenames, vul' types, test case ids of slices] (one array w/6 nested arrays of these entries)
    """ 	    
    print("Downsampling with seed number: " , seed)
    reducedCatCasesDict = _getCatCaseIdDict(categoryCasesDict.keys())
    combinedCaseID = []

    for cat in categoryCasesDict:
        if len(categoryCasesDict[cat]) == downsampleNum:
            reducedCatCasesDict[cat] = categoryCasesDict[cat]
            combinedCaseID.append(categoryCasesDict[cat])
            continue
        np.random.seed(seed)
        randomCase = categoryCasesDict[cat]
        np.random.shuffle(randomCase)
        reducedCases = randomCase[0:downsampleNum]
        reducedCatCasesDict[cat] = reducedCases
        combinedCaseID.append(reducedCases)

    print(reducedCatCasesDict)

    np.random.seed(seed)
    randomCombinedCase = [c for case in combinedCaseID for c in case]
    np.random.shuffle(randomCombinedCase)

    train_set = []
    
    for foldername in os.listdir(vectorPath):  #./data/vector/
        if os.path.isdir(os.path.join(vectorPath, foldername)):
            for caseID in randomCombinedCase:
                if (caseID == foldername):
                    for pkl in os.listdir(vectorPath + foldername):
                        if pkl.endswith('.pkl'):
                            f = open(vectorPath + foldername + '/'  + pkl , 'rb') #'./data/vector/8508/8508.pkl'
                            data = pickle.load(f)
                            if train_set == []:
                                train_set = init_nested_arrays(len(data))
                            for i in range(len(data)):
                                train_set[i].append(data[i][0])
    print("Total samples in training set: ", len(train_set[0]))
    f_train = open(trainpath + "balancedClassTrain.pkl", 'wb') # './data/DLvectors/train/'
    pickle.dump(train_set, f_train, protocol=pickle.HIGHEST_PROTOCOL)
    f_train.close()
    del train_set
    gc.collect() 
    print("Done! balancedClassTrain.pkl saved in ./data/DLvectors/train/ ")

def isMulticlassBalanced(traindatapath):
    for filename in os.listdir(traindatapath):  #./data/DLvectors/train/
        if ("balanced" in filename) and (filename.endswith('.pkl')):#loop for each train.pkl and extrak class.pkl
            print(filename)
            f = open(traindatapath + filename, 'rb') #'./data/DLvectors/train/8508/api8508.pkl'
            data = pickle.load(f)
            label_array = [x[0] for x in data[1]]
            category_counts = np.unique(label_array, return_counts=True)
            _printCatCounts(category_counts)
            totalSamples = len(label_array)
            print(f'Total Samples:  {totalSamples}')

            ccount = category_counts[1][1]
            for i in category_counts[1]:
                if i != ccount:
                    return False
            return True

def _printCatCounts(classcounts):
    print('GROUP ID\tCOUNT')
    for i in range(len(classcounts[0])):
        print(f'{classcounts[0][i]}\t{classcounts[1][i]}')

def _getCatCaseIdDict(categories):
    categoryCasesDict = {}
    for c in categories:
        categoryCasesDict[c] = []
    return categoryCasesDict
    