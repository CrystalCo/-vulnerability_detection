#Split Data to Train/Test (80/20)
import pickle
import os
import numpy as np
import random
import gc
import shutil

from utils.utils import init_nested_arrays

def splitTrainTest(vType, vectorPath, vectorTrainPath, vectorTestPath, randomSeed, split = 0.8):
    """
        Randomly shuffles vector pkl files to choose from. 
        Combines all the tokens (one array w/6 nested arrays), & creates 2 files with this (training/testing).
    """
    
    folders = os.listdir(vectorPath)#./data/vector
    np.random.seed(randomSeed)
    np.random.shuffle(folders)
    
    folders_train = folders[:int(len(folders)*split)+1]#8090, 8091,...
    folders_test = folders[int(len(folders)*split)+1:]
    
    # train_set = [[], [], [], [], [],[]] # [list of tokens (*for all training pkl files), label*, function list in each program*, filenames, vulnerability types, test case id]
    train_set = [] # [list of tokens (*for all training pkl files), label*, function list in each program*, filenames, vulnerability types, test case id]
    # train_ids = [] # [8090, 8091, ...]
    #splitting method
    for folder_train in folders_train:
        if folder_train.endswith('DS_Store'):
                continue
        for filename in os.listdir(vectorPath+ folder_train + '/'):#./data/vector/8090
            f = open(vectorPath + folder_train + '/' + filename, 'rb')
            data = pickle.load(f)
            if train_set == []:
                train_set = init_nested_arrays(len(data))
            # id_length = len(data[1])
            # for j in range(id_length):
            #     train_ids.append(folder_train)
            for n in range(len(data)):
                train_set[n] = train_set[n] + data[n]
            # train_set[-1] = train_ids
        if train_set[0] == []:
            continue
    print("Samples in Train set: ",len(train_set[-1]))
    f_train = open(vectorTrainPath + vType + "_" + "train.pkl", 'wb')#  './data/DLvectors/train/' api_train.pkl, ALL_train.pkl
    pickle.dump(train_set, f_train, protocol=pickle.HIGHEST_PROTOCOL)
    f_train.close()
    del train_set
    gc.collect()     
                    
   
    test_set = []
    # test_ids = []
    for folder_test in folders_test:
        if folder_test.endswith('DS_Store'):
            continue
        for filename in os.listdir(vectorPath + folder_test + '/'):
            if filename.endswith('DS_Store'):
                continue
            f = open(vectorPath + folder_test + '/' + filename, 'rb')
            data = pickle.load(f)
            if train_set == []:
                train_set = init_nested_arrays(len(data))
            # id_length = len(data[1])
            # for j in range(id_length):
            #     test_ids.append(folder_test)
            for n in range(len(data)):
                test_set[n] = test_set[n] + data[n]
                # test_set[-1] = test_ids#['8090'],['8093'],['8091']
        if test_set[0] == []:
            continue
    print("Samples in Test set: " , len(test_set[-1]))  
    f_test = open(vectorTestPath + vType + "_" + "test.pkl", 'wb')
    pickle.dump(test_set, f_test, protocol=pickle.HIGHEST_PROTOCOL)
    f_test.close()
    del test_set
    gc.collect()
    print("Finished Splitting data with seed number: " , randomSeed)
    print("Train/Test Sets saved in DLVectors folder\n")


def splitTrainTestCategorical(vType, vectorPath, vectorTrainPath, vectorTestPath, randomSeed, case_ids=None, split=0.8):
    """
        Randomly shuffles vector pkl files to choose from. 
        Combines tokens from specified vType (aka group id), & outputs 2 files: one for training, and one for testing.
    """
    
    folders = os.listdir(vectorPath)    # data/vector/
    group_set = [] # [list of tokens (for vType pkl files), labels, function list in each program, filenames, vulnerability types, group ids, test case ids]
    # testcase_ids = [] # [8090, 8091, ...]

    # collect metadata from all vectors within this group (aka vType)
    if case_ids:    # faster
        for folder in folders:
            if folder.endswith('DS_Store'):
                    continue
            if folder.isdigit() and int(folder) in case_ids:
                for filename in os.listdir(vectorPath + folder + '/'): #./data/vector/8090
                    f = open(vectorPath + folder + '/' + filename, 'rb')
                    data = pickle.load(f)
                    if group_set == []:
                        group_set = init_nested_arrays(len(data))
                    for n in range(len(data)):
                        group_set[n].append(data[n][0])
                    # testcase_ids.append(folder)
                if group_set[0] == []:
                    continue
        # group_set[-1] = testcase_ids
    else:
        for folder in folders:
            if folder.endswith('DS_Store'):
                continue
            for filename in os.listdir(vectorPath + folder + '/'): #./data/vector/8090
                f = open(vectorPath + folder + '/' + filename, 'rb')
                data = pickle.load(f)
                if group_set == []:
                    group_set = init_nested_arrays(len(data))
                groupid = data[-2][0][1]
                if str(groupid) == vType:
                    for n in range(len(data)):
                        group_set[n].append(data[n][0])
                    # testcase_ids.append(folder)
            if group_set[0] == []:
                continue
        # group_set[-1] = testcase_ids

    total_samples = len(group_set[-1])
    training_split = int(total_samples*split) + 1
    train_set = [[], [], [], [], [],[], []] # [list of tokens (*for all training pkl files), label*, function list in each program*, filenames, vulnerability types, test case ids]
    test_set = [[], [], [], [], [],[], []] # [list of tokens (*for all training pkl files), label*, function list in each program*, filenames, vulnerability types, test case ids]

    for i in range(len(group_set)):
        np.random.seed(randomSeed)
        np.random.shuffle(group_set[i])

    for i in range(len(group_set)):
        train_set[i] = group_set[i][:training_split]
        test_set[i] = group_set[i][training_split:]
    
    print("Samples in Training set: ", len(train_set[-1]))
    f_train = open(vectorTrainPath + vType + "_" + "train.pkl", 'wb')#  './data/DLvectors/train/' api_train.pkl, ALL_train.pkl
    pickle.dump(train_set, f_train, protocol=pickle.HIGHEST_PROTOCOL)
    f_train.close()
    del train_set
    gc.collect()    
 
    print("Samples in Test set: ", len(test_set[-1]))
    f_test = open(vectorTestPath + vType + "_" + "test.pkl", 'wb')
    pickle.dump(test_set, f_test, protocol=pickle.HIGHEST_PROTOCOL)
    f_test.close()
    del test_set
    gc.collect()

    print("Total Samples: ", total_samples)
    print("Finished Splitting data with seed number: " , randomSeed)
    print("Train/Test Sets saved in DLVectors folder\n")


