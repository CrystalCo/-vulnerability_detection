#Split Data to Train/Test (80/20)
import pickle
import os
import numpy as np
import gc

from utils.utils import init_nested_arrays

def splitTrainTest(vType, vectorPath, vectorTrainPath, vectorTestPath, randomSeed, split = 0.8):
    """
        Randomly shuffles vector pkl files to choose from. 
        Combines all the tokens (one array w/6 nested arrays), & creates 2 files with this (training/testing).
    """
    
    folders = os.listdir(vectorPath)#./data/vector
    np.random.seed(randomSeed)
    np.random.shuffle(folders)
    
    folders_train = folders[:int(len(folders)*split)+1]#8090, 8091,...
    folders_test = folders[int(len(folders)*split)+1:]
    
    #splitting method
    train_set = [] # [list of tokens (*for all training pkl files), label*, function list in each program*, filenames, vulnerability types, test case id]
    for folder_train in folders_train:
        if folder_train.endswith('DS_Store'):
                continue
        for filename in os.listdir(vectorPath+ folder_train + '/'):#./data/vector/8090
            f = open(vectorPath + folder_train + '/' + filename, 'rb')
            data = pickle.load(f)
            if train_set == []:
                train_set = init_nested_arrays(len(data))
            for n in range(len(data)):
                train_set[n] = train_set[n] + data[n]
        if train_set[0] == []:
            continue
    print("Samples in Train set: ",len(train_set[-1]))
    f_train = open(vectorTrainPath + vType + "_" + "train.pkl", 'wb')#  './data/DLvectors/train/' api_train.pkl, ALL_train.pkl
    pickle.dump(train_set, f_train, protocol=pickle.HIGHEST_PROTOCOL)
    f_train.close()
    del train_set
    gc.collect()     

   
    test_set = []
    for folder_test in folders_test:
        if folder_test.endswith('DS_Store'):
            continue
        for filename in os.listdir(vectorPath + folder_test + '/'):
            if filename.endswith('DS_Store'):
                continue
            f = open(vectorPath + folder_test + '/' + filename, 'rb')
            data = pickle.load(f)
            if test_set == []:
                test_set = init_nested_arrays(len(data))
            for n in range(len(data)):
                test_set[n] = test_set[n] + data[n]
        if test_set[0] == []:
            continue
    print("Samples in Test set: " , len(test_set[-1]))  
    f_test = open(vectorTestPath + vType + "_" + "test.pkl", 'wb')
    pickle.dump(test_set, f_test, protocol=pickle.HIGHEST_PROTOCOL)
    f_test.close()
    del test_set
    gc.collect()
    print("Finished Splitting data with seed number: " , randomSeed)
    print("Train/Test Sets saved in DLVectors folder\n")


def splitTrainTestCategorical(vType, vectorTypePath, vectorTrainPath, vectorTestPath, randomSeed, split=0.8):
    """
        Stratify shuffle data
        outputs 2 files: one for training, and one for testing.
    """
    print("Commencing split for training and testing sets...")

    folders = os.listdir(vectorTypePath)    # data/DLvector/
    group_set = [] # [list of tokens (for vType pkl files), labels, function list in each program, filenames, vulnerability types, group ids, test case ids]

    # Open balanced_vectors.pkl to load in the data
    for folder in folders:
        if not folder.endswith('.pkl'):
            continue
        filepath = os.path.join(vectorTypePath, folder)
        f = open(filepath, 'rb')
        group_set = pickle.load(f)
        f.close()
        break
    
    # Get ratios
    classes, y_indices = np.unique(group_set[-2], return_inverse=True)
    class_counts = np.bincount(y_indices)

    if np.min(class_counts) < 2:
        raise ValueError("The least populated class in y has only 1"
                            " member, which is too few. The minimum"
                            " number of groups for any class cannot"
                            " be less than 2.")

    # Find the sorted list of instances for each class:
    # (np.unique above performs a sort, so code is O(n logn) already)
    class_indices = np.split(np.argsort(y_indices, kind='mergesort'),
                            np.cumsum(class_counts)[:-1])

    train_set = [[], [], [], [], [],[], []] # [list of tokens (*for all training pkl files), label*, function list in each program*, filenames, vulnerability types, test case ids]
    test_set = [[], [], [], [], [],[], []] # [list of tokens (*for all training pkl files), label*, function list in each program*, filenames, vulnerability types, test case ids]

    for count, indices in zip(class_counts, class_indices):
        training_split = int(count*split) + 1
        label_indices_train = indices[:training_split]
        label_indices_test = indices[training_split:]

        for i in label_indices_train:
            for group_index in range(len(group_set)):
                train_set[group_index].append(group_set[group_index][i])

        for i in label_indices_test:
            for group_index in range(len(group_set)):
                test_set[group_index].append(group_set[group_index][i])
    
    # randomseed shuffle
    for i in range(len(train_set)):
        np.random.seed(randomSeed)
        np.random.shuffle(train_set[i])

    for i in range(len(test_set)):
        np.random.seed(randomSeed)
        np.random.shuffle(test_set[i])


    print("Samples in Training set: ", len(train_set[-1]))
    newpath = os.path.join(vectorTrainPath, vType + "_train.pkl")
    f_train = open(newpath, 'wb')#  './data/MLvectors/train/' api_train.pkl, ALL_train.pkl
    pickle.dump(train_set, f_train, protocol=pickle.HIGHEST_PROTOCOL)
    f_train.close()
    del train_set
    gc.collect()    
 
    print("Samples in Test set: ", len(test_set[-1]))
    newpath = os.path.join(vectorTestPath, vType + "_test.pkl")
    f_test = open(newpath, 'wb')
    pickle.dump(test_set, f_test, protocol=pickle.HIGHEST_PROTOCOL)
    f_test.close()
    del test_set
    gc.collect()
    print("Train/Test Sets saved in DLVectors folder\n")


