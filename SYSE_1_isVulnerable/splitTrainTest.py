#Split Data to Train/Test (80/20)
import pickle
import os
import numpy as np
import random
import gc
import shutil

def splitTrainTest(vType, vectorPath, vectorTrainPath, vectorTestPath, randomSeed, split = 0.8):
    """
        Randomly shuffles vector pkl files to choose from. 
        Combines all the tokens (one array w/6 nested arrays), & creates 2 files with this (training/testing).
    """
    
    folders = os.listdir(vectorPath)#./data/vector
    np.random.seed(randomSeed)
    np.random.shuffle(folders)
    
    folders_train = folders[:int(len(folders)*split)+1]#8090, 8091,...
    folders_test = folders[int(len(folders)*split)+1:]
    
    train_set = [[], [], [], [], [],[]] # [list of tokens (*for all training pkl files), label*, function list in each program*, filenames, vulnerability types, test case id]
    train_ids = [] # [8090, 8091, ...]
    #splitting method
    for folder_train in folders_train:
        if folder_train.endswith('DS_Store'):
                continue
        for filename in os.listdir(vectorPath+ folder_train + '/'):#./data/vector/8090
            f = open(vectorPath + folder_train + '/' + filename, 'rb')
            data = pickle.load(f)
            id_length = len(data[1])
            for j in range(id_length):
                train_ids.append(folder_train)
            for n in range(5):
                train_set[n] = train_set[n] + data[n]
            train_set[-1] = train_ids
        if train_set[0] == []:
            continue
    print("Samples in Train set: ",len(train_set[-1]))
    f_train = open(vectorTrainPath + vType + "_" + "train.pkl", 'wb')#  './data/DLvectors/train/' api_train.pkl, ALL_train.pkl
    pickle.dump(train_set, f_train, protocol=pickle.HIGHEST_PROTOCOL)
    f_train.close()
    del train_set
    gc.collect()     
                    
   
    test_set = [[], [], [], [], [],[]]
    test_ids = []
    for folder_test in folders_test:
        if folder_test.endswith('DS_Store'):
            continue
        for filename in os.listdir(vectorPath + folder_test + '/'):
            if filename.endswith('DS_Store'):
                continue
            f = open(vectorPath + folder_test + '/' + filename, 'rb')
            data = pickle.load(f)
            id_length = len(data[1])
            for j in range(id_length):
                test_ids.append(folder_test)
            for n in range(5):
                test_set[n] = test_set[n] + data[n]
                test_set[-1] = test_ids#['8090'],['8093'],['8091']
        if test_set[0] == []:
            continue
    print("Samples in Test set: " , len(test_set[-1]))  
    f_test = open(vectorTestPath + vType + "_" + "test.pkl", 'wb')
    pickle.dump(test_set, f_test, protocol=pickle.HIGHEST_PROTOCOL)
    f_test.close()
    del test_set
    gc.collect()
    print("Finished Splitting data with seed number: " , randomSeed)
    print("Train/Test Sets saved in DLVectors folder\n")


def splitTrainTestCategorical(vType, vectorPath, vectorTrainPath, vectorTestPath, randomSeed, case_ids=None, split=0.8):
    """
        Randomly shuffles vector pkl files to choose from. 
        Combines tokens from specified vType (aka group id), & outputs 2 files: one for training, and one for testing.
    """
    
    folders = os.listdir(vectorPath)    # data/vector/
    group_set = [[], [], [], [], [], [], []] # [list of tokens (for vType pkl files), labels, function list in each program, filenames, vulnerability types, group ids, test case ids]
    testcase_ids = [] # [8090, 8091, ...]

    # collect metadata from all vectors within this group (aka vType)
    if case_ids:    # faster
        for folder in folders:
            if folder.endswith('DS_Store'):
                    continue
            if folder.isdigit() and int(folder) in case_ids:
                for filename in os.listdir(vectorPath + folder + '/'): #./data/vector/8090
                    f = open(vectorPath + folder + '/' + filename, 'rb')
                    data = pickle.load(f)
                    # for j in range(len(data[1])):
                    testcase_ids.append(folder)
                    for n in range(len(data)):
                        group_set[n] = group_set[n] + data[n]
                    group_set[-1] = testcase_ids
                if group_set[0] == []:
                    continue
    else:
        for folder in folders:
            if folder.endswith('DS_Store'):
                continue
            for filename in os.listdir(vectorPath + folder + '/'): #./data/vector/8090
                f = open(vectorPath + folder + '/' + filename, 'rb')
                data = pickle.load(f)
                groupid = data[-1][0][1]
                if str(groupid) == vType:
                    testcase_ids.append(folder)
                    for n in range(len(data)):
                        group_set[n] = group_set[n] + data[n]
                    group_set[-1] = testcase_ids
            if group_set[0] == []:
                continue

    print("Samples in set: ",len(group_set[-1]))
    f_train = open(vectorTrainPath + vType + "_" + "train.pkl", 'wb')#  './data/DLvectors/train/' api_train.pkl, ALL_train.pkl
    pickle.dump(group_set, f_train, protocol=pickle.HIGHEST_PROTOCOL)
    f_train.close()
    del group_set
    gc.collect()     
    
    # np.random.seed(randomSeed)
    # np.random.shuffle(folders)
    
    # folders_train = folders[:int(len(folders)*split)+1]#8090, 8091,...
    # folders_test = folders[int(len(folders)*split)+1:]
    
 
    print("Finished Splitting data with seed number: " , randomSeed)
    print("Train/Test Sets saved in DLVectors folder\n")


