import os

import numpy as np
import seaborn as sns
from matplotlib import pyplot as plt
from tabulate import tabulate
from tensorflow.keras.metrics import TruePositives, TrueNegatives,FalsePositives, FalseNegatives
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score

from utils.utils import output_results

"""
#### Input: 	
- predicted label and real label from part I

#### Output:   
- confusion matrix 
"""
def getConfusionMatrix(predicted_labels, reallabels):
    TP = TruePositives()
    TP.update_state(reallabels, predicted_labels)
    TP_count = TP.result().numpy()
    TN = TrueNegatives()
    TN.update_state(reallabels, predicted_labels)
    TN_count = TN.result().numpy()
    FP = FalsePositives()
    FP.update_state(reallabels, predicted_labels)
    FP_count = FP.result().numpy()
    FN = FalseNegatives()
    FN.update_state(reallabels, predicted_labels)
    FN_count = FN.result().numpy()

    totalsamples = (TP_count + TN_count + FP_count + FN_count)
    accuracyrate = (TP_count + TN_count) /totalsamples 
    sensitivity = TP_count/(TP_count + FN_count)
    specificity = TN_count/(TN_count + FP_count)
    precision = TP_count/(TP_count + FP_count)
    negPrediction = TN_count/(TN_count + FN_count)
    print ('Predicted Class')
    print('Total Samples', totalsamples  )
    print(tabulate([['Positive', TP_count, FN_count], ['Negative', FP_count, TN_count ]], headers=['Type', 'Positive', 'Negative'], tablefmt='orgtbl'))
    print()
    print ('Predicted Class')
    print(tabulate([['Positive', TP_count, FN_count, sensitivity, 'Sensitivity' ], 
                ['Negative', FP_count, TN_count,specificity, 'specificity' ],
                ['', precision, negPrediction, accuracyrate, 'Accuracy' ],
                ['', 'Precision', 'NegPrediction', '', '' ]
                
               ], headers=['', 'Positive', 'Negative', 'Rate' , ''], tablefmt='fancy_grid'))



def getConfusionMatrix_Multiclass(predicted_labels, true_labels, saveFig=False, path='', modelName=''):
    # output grid table
    confusionMatrix = confusion_matrix(true_labels, predicted_labels)
    print('\nConfusion Matrix')
    print(confusionMatrix)
    print('\nCLASSIFICATION REPORT')
    print(classification_report(true_labels, predicted_labels, digits=4, zero_division=0))
    

    # display heatmap
    if saveFig:
        plt.figure(figsize = (18,8))
        sns.heatmap(confusionMatrix, annot=True, xticklabels=np.unique(true_labels), yticklabels=np.unique(true_labels), cmap='summer')
        plt.xlabel('Predicted Labels')
        plt.ylabel('True Labels')
        if path != '':
            heatmapPath = os.path.join(path, modelName + 'heatmap')
            plt.savefig(heatmapPath)
        else:
            plt.show()
        plt.clf()

    # Local Metrics - Calculations per class
    print('\nCalculations from CM')
    FP = confusionMatrix.sum(axis=0) - np.diag(confusionMatrix)
    FN = confusionMatrix.sum(axis=1) - np.diag(confusionMatrix)
    TP = np.diag(confusionMatrix)
    TN = confusionMatrix.sum() - (FP + FN + TP)
    FP = FP.astype(float)
    FN = FN.astype(float)
    TP = TP.astype(float)
    TN = TN.astype(float)
    print(f'FP: {FP}')
    print(f'FN: {FN}')
    print(f'TP: {TP}')
    print(f'TN: {TN}')
    print(f'TN: {TN}')
    print('\nTPR = TP/(TP+FN)')
    print('\nFNR = FN/(TP+FN)')
    # Sensitivity, hit rate, recall, or true positive rate
    TPR = TP/(TP+FN) # ERROR HERE
    # Specificity or true negative rate
    TNR = TN/(TN+FP) 
    # Fall out or false positive rate
    FPR = FP/(FP+TN)
    # False negative rate
    FNR = FN/(TP+FN)  # ERROR HERE
    # Overall accuracy for each class (macro)
    ACC = (TP+TN)/(TP+FP+FN+TN)

    NUM_CATEGORIES = len(FP)
    M_FPR = (1/NUM_CATEGORIES) * np.nansum(FPR)
    print(f'Mean FPR: {M_FPR}')
    M_FNR = (1/NUM_CATEGORIES) * np.nansum(FNR)
    print(f'Mean FNR: {M_FNR}')

    # Weighted FPR & FNR
    labels, counts = np.unique(true_labels, return_counts=True)
    total = len(true_labels)

    FPR_sum = 0
    i = 0 # index for FPR
    for label, count in zip(labels, counts):
        result = FPR[i] * count
        FPR_sum += result if np.isnan(result) != True else 0
        i += 1

    W_FPR = FPR_sum/total
    print(f'Weighted FPR: {W_FPR}')

    FNR_sum = 0
    i = 0
    for label, count in zip(labels, counts):
        result = FNR[i] * count
        FNR_sum += result if np.isnan(result) != True else 0
        i +=1 

    W_FNR = FNR_sum/total
    print(f'Weighted FNR: {W_FNR}\n')

    metrics = [TP, FP, TN, FN, TPR, TNR, FPR, FNR, ACC, counts]
    metric_names = ['TP', 'FP', 'TN', 'FN', 'TPR', 'TNR', 'FPR', 'FNR', 'ACC', 'Counts']
    filename = os.path.join(path, modelName + 'LocalMetrics')
    output_results(metrics, labels, filename, index=metric_names)
    for name, m in zip(metric_names, metrics):
        print(f'{name}: {m}')
    print(f'Labels: {labels}\n')


    ## Global Calculations
    # Micro - Calculate metrics globally by counting the total true positives, false negatives and false positives.
    micro_precision = precision_score(true_labels, predicted_labels, average='micro', zero_division=0)
    micro_recall = recall_score(true_labels, predicted_labels, average='micro', zero_division=0)
    micro_fpr = 1 - micro_recall
    micro_fnr = (np.nansum(TN))/(len(labels))
    micro_f1 = f1_score(true_labels, predicted_labels, average='micro', zero_division=0)
    # Macro - Calculate metrics for each label, and find their unweighted mean. This does not take label imbalance into account.
    macro_precision = precision_score(true_labels, predicted_labels, average='macro', zero_division=0)
    macro_recall = recall_score(true_labels, predicted_labels, average='macro', zero_division=0)
    # TODO: macro FPR & FNR are coming out the same.  Something is off. Fix!
    macro_fpr = M_FPR
    macro_fnr = (np.nansum(FNR))/(len(labels))
    macro_f1 = f1_score(true_labels, predicted_labels, average='macro', zero_division=0)
    # Weighted - Calculate metrics for each label, and find their average weighted by support (the number of true instances for each label). This alters ‘macro’ to account for label imbalance; it can result in an F-score that is not between precision and recall. Weighted recall is equal to accuracy.
    weighted_precision = precision_score(true_labels, predicted_labels, average='weighted', zero_division=0)
    weighted_recall = recall_score(true_labels, predicted_labels, average='weighted', zero_division=0)
    weighted_fpr = W_FPR
    weighted_fnr = (np.nansum(W_FNR)/len(labels))
    weighted_f1 = f1_score(true_labels, predicted_labels, average='weighted', zero_division=0)
    acc = accuracy_score(true_labels, predicted_labels)

    metrics = [micro_precision, micro_recall, micro_fpr, micro_fnr, micro_f1, macro_precision, macro_recall, macro_fpr, macro_fnr, macro_f1, weighted_precision, weighted_recall, weighted_fpr, weighted_fnr, weighted_f1, acc]
    metric_names = [ 'Micro Precision', 'Micro Recall', 'Micro FPR', 'Micro FNR', 'Micro F1-score', 'Mean Precision', 'Mean Recall', 'Mean FPR', 'Mean FNR', 'Mean F1-score', 'Weighted Precision', 'Weighted Recall', 'Weighted FPR', 'Weighted FNR', 'Weighted F1-score', 'Accuracy']
    for m, name in zip(metrics, metric_names):
        print('{}: {:.4f}'.format(name, m))
    filename = os.path.join(path, modelName + "GlobalMetrics")
    col_length = len(metrics)
    metrics = np.array(metrics).reshape((1,col_length))
    output_results(metrics, metric_names, filename)


    return metrics

