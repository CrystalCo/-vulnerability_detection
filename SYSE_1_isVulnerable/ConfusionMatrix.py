import numpy as np
from tabulate import tabulate
from tensorflow.keras.metrics import TruePositives, TrueNegatives,FalsePositives, FalseNegatives
from tensorflow.python.keras.metrics import accuracy

"""
#### Input: 	
- predicted label and real label from part I

#### Output:   
- confusion matrix 
"""
def getConfusionMatrix(predicted_labels, reallabels):
    TP = TruePositives()
    TP.update_state(reallabels, predicted_labels)
    TP_count = TP.result().numpy()
    TN = TrueNegatives()
    TN.update_state(reallabels, predicted_labels)
    TN_count = TN.result().numpy()
    FP = FalsePositives()
    FP.update_state(reallabels, predicted_labels)
    FP_count = FP.result().numpy()
    FN = FalseNegatives()
    FN.update_state(reallabels, predicted_labels)
    FN_count = FN.result().numpy()

    totalsamples = (TP_count + TN_count + FP_count + FN_count)
    accuracyrate = (TP_count + TN_count) /totalsamples 
    sensitivity = TP_count/(TP_count + FN_count)
    specificity = TN_count/(TN_count + FP_count)
    precision = TP_count/(TP_count + FP_count)
    negPrediction = TN_count/(TN_count + FN_count)
    print ('Predicted Class')
    print('Total Samples', totalsamples  )
    print(tabulate([['Positive', TP_count, FN_count], ['Negative', FP_count, TN_count ]], headers=['Type', 'Positive', 'Negative'], tablefmt='orgtbl'))
    print()
    print ('Predicted Class')
    print(tabulate([['Positive', TP_count, FN_count, sensitivity, 'Sensitivity' ], 
                ['Negative', FP_count, TN_count,specificity, 'specificity' ],
                ['', precision, negPrediction, accuracyrate, 'Accuracy' ],
                ['', 'Precision', 'NegPrediction', '', '' ]
                
               ], headers=['', 'Positive', 'Negative', 'Rate' , ''], tablefmt='fancy_grid'))


import seaborn as sns
from matplotlib import pyplot as plt
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score

def getConfusionMatrix_Multiclass(predicted_labels, true_labels):
    table = {}

    for y_true in np.unique(true_labels):
        print('Y True: ', y_true)
        table[y_true] = {
            'TP': 0,
            'FP': 0,
            'FN': 0,
            'TN': 0
        }

        for y_pred, y in zip(predicted_labels, true_labels):
            if (y_pred == y) and (y == y_true):
                table[y_true]['TP'] += 1
            elif (y_pred == y) and (y != y_true):
                table[y_true]['FP'] += 1
            elif (y_pred != y) and (y == y_true):
                table[y_true]['FN'] += 1
            elif (y_pred != y) and (y != y_true):
                table[y_true]['TN'] += 1

    print(table)

    confusionMatrix = confusion_matrix(true_labels, predicted_labels)
    print('\nConfusion Matrix')
    print(confusionMatrix)

    plt.figure(figsize = (18,8))
    sns.heatmap(confusionMatrix, annot = True, xticklabels = true_labels.unique(), yticklabels = true_labels.unique(), cmap = 'summer')
    plt.xlabel('Predicted Labels')
    plt.ylabel('True Labels')
    plt.show()

    acc = accuracy_score(true_labels, predicted_labels)
    micro_precision = precision_score(true_labels, predicted_labels, average='micro')
    micro_recall = recall_score(true_labels, predicted_labels, average='micro')
    micro_f1 = f1_score(true_labels, predicted_labels, average='micro')
    macro_precision = precision_score(true_labels, predicted_labels, average='macro')
    macro_recall = recall_score(true_labels, predicted_labels, average='macro')
    macro_f1 = f1_score(true_labels, predicted_labels, average='macro')
    weighted_precision = precision_score(true_labels, predicted_labels, average='weighted')
    weighted_recall = recall_score(true_labels, predicted_labels, average='weighted')
    weighted_f1 = f1_score(true_labels, predicted_labels, average='weighted')

    metrics = [acc, micro_precision, micro_recall, micro_f1, macro_precision, macro_recall, macro_f1, weighted_precision, weighted_recall, weighted_f1]
    metric_names = ['Micro Precision', 'Micro Recall', 'Micro F1-score', 'Macro Precision', 'Macro Recall', 'Macro F1-score', 'Weighted Precision', 'Weighted Recall', 'Weighted F1-score']

    print('\nAccuracy: {:.2f}\n'.format(acc))

    for m, name in zip(metrics, metric_names):
        print('{}: {:.2f}'.format(name, m))

    print('\nCLASSIFICATION REPORT')
    print(classification_report(true_labels, predicted_labels))

    return metrics

