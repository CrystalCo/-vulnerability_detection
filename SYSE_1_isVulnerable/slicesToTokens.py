import os
import pickle
import difflib

import numpy as np
import pandas as pd

import sys
VUL_PATH = os.environ.get('VUL_PATH')
sys.path.insert(1, VUL_PATH)

from SYSE_1_isVulnerable.mapping import mapping, create_tokens
from utils.utils import get_SARD, getDataset

def tokenizeSlices(slicepath, corpuspath, totalSamples):
    """
        Converts the words & symbols from the samples into tokens 
        that will be used to create vectors.  Expects binary labels.
    """    
    mycase_ID = []
    for filename in os.listdir(slicepath):
        if(filename.endswith(".txt") is False):
            continue
        print("Slice Files To be Processed: ", filename)
        if "API" in filename:
            myType = ['API']
        elif "Array" in filename:
            myType = ['ARR']
        elif "Pointer" in filename:
            myType = ['PTR']
        elif "Arithmetic" in filename:
            myType = ['AE']
        else:
            myType = 'Others'
        filepath = os.path.join(slicepath, filename)
        f1 = open(filepath, 'r')
        slicelists = f1.read().split("------------------------------")#aa. split each slide (each program in .txt file)
        f1.close()

        if slicelists[0] == '':
            del slicelists[0]
        if slicelists[-1] == '' or slicelists[-1] == '\n' or slicelists[-1] == '\r\n':
            del slicelists[-1]

        lastprogram_id = 0
        program_id = 0
        index = -1
        file_name = 0
        slicefile_corpus = []
        slicefile_labels = []
        slicefile_func = []
        slicefile_filenames = []
        focuspointer = None 
        count = 0
        for slicelist in slicelists:
            if count == totalSamples:
                continue
            count = count+1
            slice_corpus = []
            index = index + 1
            sentences = slicelist.split('\n')
            if sentences[0] == '\r' or sentences[0] == '':
                del sentences[0]
            if sentences == []:
                continue
            if sentences[-1] == '':
                del sentences[-1]
            if sentences[-1] == '\r':
                del sentences[-1]
            testcase_id = sentences[0].split(' ')[0] 
            if testcase_id in os.listdir(corpuspath):
                # avoid duplicate testcase id numbers
                testcase_id = int(testcase_id) + 1000000
            else:
                testcase_id = int(testcase_id)
            mycase_ID.append(testcase_id)
            label = int(sentences[-1])
            program_id = testcase_id 
            lastprogram_id = testcase_id
            focuspointer = sentences[0].split(" ")[-2:]
            # sliceid = index
            file_name = sentences[0]
            folder_path = os.path.join(corpuspath, str(lastprogram_id))
            filenameCorpus = str(testcase_id)
            savefilename = folder_path + '/' + filenameCorpus + '.pkl'
            if (lastprogram_id not in os.listdir(corpuspath)) and (not os.path.exists(folder_path)):
                os.mkdir(folder_path)
            slicefile_corpus = []
            slicefile_labels = []
            slicefile_filenames = []
            slicefile_func = []
            sentences = sentences[1:]
            for sentence in sentences:

                if sentence.split(" ")[-1] == focuspointer[1] and flag_focus == 0:
                    flag_focus = 1

                sentence = ' '.join(sentence.split(" ")[:-1])

                start = str.find(sentence,r'printf("')
                if start != -1:
                    start = str.find(sentence,r'");')
                    sentence = sentence[:start+2]
                
                fm = str.find(sentence,'/*')
                if fm != -1:
                    sentence = sentence[:fm]
                else:
                    fm = str.find(sentence,'//')
                    if fm != -1:
                        sentence = sentence[:fm]
                sentence = sentence.strip()
                list_tokens = create_tokens(sentence)
                slice_corpus.append(list_tokens)

            slicefile_labels.append(label)
            slicefile_filenames.append(file_name)
            
            # Create list of functions
            slice_corpus = mapping(slice_corpus)
            slice_func = slice_corpus
            slice_func = list(set(slice_func))
                   
            if slice_func == []:
                slice_func = ['main']
            sample_corpus = []
            for sentence in slice_corpus:
                list_tokens = create_tokens(sentence)
                sample_corpus = sample_corpus + list_tokens
            slicefile_corpus.append(sample_corpus)
            slicefile_func.append(slice_func)
            f1 = open(savefilename, 'wb')               
            pickle.dump([slicefile_corpus,slicefile_labels,slicefile_func,slicefile_filenames, myType, [filenameCorpus]], f1)
            f1.close()
        print ("Total Corpus Files: ", count)
        print ("Last Program ID: ", lastprogram_id)
    return mycase_ID

def tokenizeSlices_Multiclass(slicepath, corpuspath, multiclasspath, totalSamples, mergeClasses=None):
    """
        Converts the words & symbols from the samples into tokens 
        that will be used to create vectors.  Expects multiclass labels.
    
        Trying to differentiate between non vulnerable and vulnerability types returned imbalanced data 
        (356,121 non-vulnerable out of 420,627 samples), and low performance scores for training and testing, 
        even with downsampling.
        Thus, since we are trying to predict vulnerability types, we will omit non-vulnerable samples.
        
        tokenizeSlices() and its corresponding file `2_Application_Codes.py` perform significantly well at 
        predicting whether a code slice has a vulnerability or not. 
        This method and the files where it is used in should be considered a supplement, like a 2nd phase
        to run samples that were flagged as vulnerable in the first phase `2_Application_Codes.py`.
        Models perform best when done sequentially instead of trying to predict vulnerability status AND vulnerability type if applicable.
    """

    testcase_ids = []
    testcase_ids_per_group = _getMulticlassDict(multiclasspath) # Divides up test case ids per group; helps in the splitTrainTest process
    all_data = [ [], [], [], [], [], [], [] ]

    for filename in os.listdir(slicepath):
        if(filename.endswith(".txt") is False):
            continue
        print("Slice Files To be Processed: ", filename)
        if "API" in filename:
            myType = ['API']
        elif "Array" in filename:
            myType = ['ARR']
        elif "Pointer" in filename:
            myType = ['PTR']
        elif "Arithmetic" in filename:
            myType = ['AE']
        else:
            myType = 'Others'
        filepath = os.path.join(slicepath, filename)
        f1 = open(filepath, 'r')
        slicelists = f1.read().split("------------------------------")#aa. split each slide (each program in .txt file)
        f1.close()

        if slicelists[0] == '':
            del slicelists[0]
        if slicelists[-1] == '' or slicelists[-1] == '\n' or slicelists[-1] == '\r\n':
            del slicelists[-1]

        lastprogram_id = 0
        count = 0
        for slicelist in slicelists:
            if count == totalSamples:
                break
            count = count+1
            slice_corpus = []
            sentences = slicelist.split('\n')
            if sentences[0] == '\r' or sentences[0] == '':
                del sentences[0]
            if sentences == []:
                continue
            if sentences[-1] == '':
                del sentences[-1]
            if sentences[-1] == '\r':
                del sentences[-1]

            testcase_id = sentences[0].split(' ')[0] 
            if testcase_id in os.listdir(corpuspath):
                # avoid duplicate testcase id numbers
                testcase_id = int(testcase_id) + 1000000
            else:
                testcase_id = int(testcase_id)
            testcase_ids.append(testcase_id)
            label = int(sentences[-1])  # binary label that indicates if slice has a vulnerability or not
            
            # skip non-vulnerable samples
            if label == 0:
                continue 

            # Assign class label (aka the pillar CWE ID assigned)
            sard = get_SARD(sentences[0])
            group_id = _get_multiclass_groupid(sard, multiclasspath)
            if group_id is None:
                continue # skip slices that don't have a group id b/c we won't be able to classify them 

            # Because samples per group are imbalanced, this method allows us to merge one group to anoter.
            # Saves both the merged class and the original in an array; ex. [merged, original]
            if mergeClasses and (group_id in mergeClasses):
                # Combine 2 classes - done for classes with small amount of samples
                group_id = _combine(group_id)
            else:
                group_id = [group_id, group_id]
            testcase_ids_per_group[group_id[1]].append(testcase_id)
            lastprogram_id = testcase_id
            
            focuspointer = get_focus_pointer(sentences[0])
            focuspointer_token = ''
            fp_index = 0
            firstFP = False

            file_name = sentences[0]
            folder_path = os.path.join(corpuspath, str(lastprogram_id))
            filenameCorpus = str(testcase_id)
            savefilename = folder_path + '/' + filenameCorpus + '.pkl'
            if (lastprogram_id not in os.listdir(corpuspath)) and (not os.path.exists(folder_path)):
                os.mkdir(folder_path)
            slicefile_corpus = []
            slicefile_labels = []
            slicefile_filenames = []
            slicefile_func = []
            slicefile_groups = []
            sentences = sentences[1:] # removes header

            for sentence in sentences:
                hasFP = False
                if ';' in sentence.split(" ")[-1]:
                    sentence = sentence.replace(';', '')

                start = str.find(sentence,r'printf("')
                if start != -1:
                    start = str.find(sentence,r'");')
                    sentence = sentence[:start+2]
                
                fm = str.find(sentence,'/*')
                if fm != -1:
                    sentence = sentence[:fm]
                else:
                    fm = str.find(sentence,'//')
                    if fm != -1:
                        sentence = sentence[:fm]
                sentence = sentence.strip()
                # save index of focuspointer so we know somewhat where it was in case it gets tokenized
                if (focuspointer in sentence) and (firstFP == False):
                    fp_index = sentence.split(' ').index(focuspointer)
                    firstFP = True
                    hasFP = True

                list_tokens = create_tokens(sentence)
                # if fp got tokenized, save the word that it was converted into
                if hasFP and (focuspointer not in list_tokens):
                    focuspointer_token = list_tokens[fp_index]
                slice_corpus.append(list_tokens)

            slicefile_labels.append(label)  
            slicefile_filenames.append(file_name)
            slicefile_groups.append(group_id)
            
            # Create list of functions
            slice_corpus = mapping(slice_corpus)
            slice_func = slice_corpus
            slice_func = list(set(slice_func))
                   
            if slice_func == []:
                slice_func = ['main']
            sample_corpus = [] # stores the flattened list of words

            # flattens array of sentence arrays into an 1D array of words
            for sentence in slice_corpus: 
                list_tokens = create_tokens(sentence)
                sample_corpus = sample_corpus + list_tokens
            
            # Save first index occurence of focuspointer
            fp_index = get_focuspointer_index(focuspointer, focuspointer_token, sample_corpus, sentences)
            
            slicefile_corpus.append(sample_corpus)
            slicefile_func.append(slice_func)
            f1 = open(savefilename, 'wb')      
            data = [slicefile_corpus, slicefile_labels, slicefile_func, slicefile_filenames, myType, slicefile_groups, [filenameCorpus]]  
            pickle.dump(data, f1)
            f1.close()

            # save files with focuspointer data attached so we can use the index of 
            # first occurence of focuspointer as our midpoint when adjusting vector lengths
            fp_data = [slicefile_corpus, slicefile_labels, [fp_index], slicefile_filenames, myType, slicefile_groups, [filenameCorpus]]
            for i in range(len(fp_data)):
                all_data[i].append(fp_data[i][0])

        print ("Total Corpus Files: ", count)
        print ("Last Program ID: ", lastprogram_id)
    _printGroupCounts(testcase_ids_per_group)
    all_tokensPath = os.path.join(corpuspath, '..', 'ALL_tokens.pkl')
    f1 = open(all_tokensPath, 'wb')
    pickle.dump(all_data, f1)
    return testcase_ids, testcase_ids_per_group


def get_focus_pointer(sentence):
    focuspointer = sentence.split(".c")[-1].split(' ')[:-1]
    if focuspointer[0] == '':
        focuspointer = focuspointer[1:]
    if len(focuspointer) < 1:
        raise("no focuspointer found")
    elif len(focuspointer) == 1:
        focuspointer = focuspointer[0]
    else:
        focuspointer = ''.join(focuspointer)
    return focuspointer

def get_focuspointer_index(fp, fp_token, tokens, sentences):
    if fp in tokens:
        return tokens.index(fp)
    elif fp_token in tokens:
        return tokens.index(fp)
    elif len(tokens) < 117:
        # slice samples tend to average at around 117
        return 0
    else:
        close_matches = difflib.get_close_matches(fp, sentences)
        sentences_length = 0 
        for i in range(len(sentences)):
            sentences_length += len(sentences[i].split(' '))
            if close_matches[0] in sentences[i]:
                return sentences_length
    print("No focuspointer found for ", fp)
    print(sentences[0])
    return -1


def _combine(label):
    """
        Returns an array of 2 labels:
            First index contains the general label of the merged classes, & the
            second index contains the original class (pre-merged). 
    """
    return [9999, label]

def _get_multiclass_groupid(sard, multiclasspath):
    """
        Matches the SARD ID to its corresponding class (found in multiclasspath).
        The corresponding class is a group ID that categorizes the type of CWE vulnerability the sample would fall into.
    """
    group_id = None
    groups_df = pd.read_csv(multiclasspath, index_col=0)
    try:
        group_id = int(groups_df.loc[groups_df['Original ID'] == sard]['Group ID'].item())
    except:
        print(f'SARD {sard} does not have a group id. Skipping the tokenization of this sample.')
        
    return group_id

def _getMulticlassDict(multiclasspath):
    df = pd.read_csv(multiclasspath, index_col=0)
    categories = np.unique(df['Group ID'])
    categoryDict = {0: []} # 0 represents non-vulnerable samples

    for c in categories:
        categoryDict[c] = []
    return categoryDict

def _printGroupCounts(categoryDict):
    print('Categories found in the slice samples')
    print('GROUP ID\tCOUNT')
    for c in categoryDict:
        count = len(categoryDict[c])
        print(f'{c}\t{count}')



slicePath = os.path.join('data','slicesSource')
all_tokenPath = os.path.join('data', 'tokenFocus')
tokenPath = os.path.join(all_tokenPath,'SARD')
multiclasspath = os.path.join('data','CVE','SARD_CVE_to_groups.csv')

testcase_ids, testcase_ids_per_group = tokenizeSlices_Multiclass(slicePath, tokenPath, multiclasspath, 5)

data = getDataset(all_tokenPath, getBalanced=False)
print(data)
