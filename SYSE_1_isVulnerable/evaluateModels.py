import tabulate
from tabulate import tabulate

def predictValueWithThreshold(threshold, DLOutputs, RealLabel):
    pred_value = []
    metricType = []
    samples = len(DLOutputs)
    for i in range (samples):
        if DLOutputs[i] >= threshold:#0.5
            pred_value.append(1)
            if RealLabel[i] == 1:
                metricType.append("TP")
            else:
                metricType.append("FP")
        else: 
            pred_value.append(0)
            if RealLabel[i] == 0:
                metricType.append("TN")
            else:
                metricType.append("FN")

    return pred_value, metricType

#Loop from threshold array
def combinedPredictions(thresdArray, data):
    mydata = data
    DLOutputs = mydata["DLOutput"] 
    RealLabel = mydata["RealLabel"] 
    recall = []
    precision = []
    specificity = []
    F1 = []
    Accuracy = []
    balanceAccuracy = []
    length = len(thresdArray)
    for i in range (length):
        threshold  = thresdArray[i]
        predStr = "Pred" + str(threshold)
        metricStr = "Metric" +  str(threshold)
        mydata[predStr], mydata[metricStr] = predictValueWithThreshold(threshold, DLOutputs, RealLabel)
        TN = len(mydata[mydata[metricStr] == 'TN'])
        FP = len(mydata[mydata[metricStr] == 'FP'])
        TP = len(mydata[mydata[metricStr] == 'TP'])
        FN = len(mydata[mydata[metricStr] == 'FN'])
        try:
            recall.append((TP)/(TP + FN))
        except ZeroDivisionError:
            recall.append(0)
        try:
            precision.append((TP)/(TP + FP))
        except ZeroDivisionError:
            precision.append(0)
        try:
            specificity.append((TN)/(TN + FP))
        except ZeroDivisionError:
            specificity.append(0)
        try:
            F1.append(2*((precision[i]*recall[i])/(precision[i]+recall[i])))
        except ZeroDivisionError:
            F1.append(0)
        try:
            Accuracy.append((TP + TN)/(TP + TN+FP + FN))
        except ZeroDivisionError:
            Accuracy.append(0)
        try:
            balanceAccuracy.append((recall[i]+specificity[i])/2)
        except ZeroDivisionError:
            balanceAccuracy.append(0)
        
    return mydata, recall, precision, specificity, F1, Accuracy, balanceAccuracy 

def generateMetricTabel(thresdArray, recall, precision, specificity, F1, Accuracy, balanceAccuracy):
    print ('Predicted Class')
    resArray = [[]]
    length = len(thresdArray)
    for i in range(length):
        resArray.append([thresdArray[i], recall[i], precision[i], specificity[i], F1[i], Accuracy[i],balanceAccuracy[i]])

    myTable  = tabulate(resArray, headers=['thresdArray', 'recall', 'precision', 'specificity' , 'F1', "Accuracy",'balanceAccuracy'], tablefmt='fancy_grid')
    return myTable



# source:  https://www.kaggle.com/nkitgupta/evaluation-metrics-for-multi-class-classification?scriptVersionId=57461587&cellId=55
from sklearn.metrics import roc_auc_score

def roc_auc_score_multiclass(actual_class, pred_class, average = "macro"):
    
    #creating a set of all the unique classes using the actual class list
    unique_class = set(actual_class)
    roc_auc_dict = {}
    for per_class in unique_class:
        
        #creating a list of all the classes except the current class 
        other_class = [x for x in unique_class if x != per_class]

        #marking the current class as 1 and all other classes as 0
        new_actual_class = [0 if x in other_class else 1 for x in actual_class]
        new_pred_class = [0 if x in other_class else 1 for x in pred_class]

        #using the sklearn metrics method to calculate the roc_auc_score
        roc_auc = roc_auc_score(new_actual_class, new_pred_class, average = average)
        roc_auc_dict[per_class] = roc_auc

    return roc_auc_dict
