from tensorflow.keras.metrics import TruePositives, TrueNegatives,FalsePositives, FalseNegatives
from keras.preprocessing import sequence
from tensorflow.keras.optimizers import SGD, RMSprop, Adagrad, Adam, Adadelta
from keras.models import Sequential, load_model
from keras.layers.core import Masking, Dense, Dropout, Activation
from keras.layers.recurrent import LSTM,GRU
from keras.layers.wrappers import Bidirectional
from collections import Counter
from keras import optimizers 
from SYSE_1_isVulnerable.preprocess_dl_Input_version5 import *
import numpy as np
import pickle
import random
import time
import math
import os

from utils.utils import get_category

""" Binary Class """

def buildBGRU(maxlen, vector_dim, layers, dropout, myoptimizer): 
    print('\nBuild model...')
    """
        A Sequential model is appropriate for a plain stack of layers where each layer has exactly one input tensor and one output tensor. 
        In mathematics, a tensor is an algebraic object that describes a multilinear relationship between sets of algebraic objects related to a vector space.
    """
    model = Sequential()
    
    """
        All layers in Keras need to know the shape of their inputs in order to be able to create their weights. (avg*30)
        In input_shape, you don't need to specify batch size.  Instead takes in (timesteps, features) --> (avg, 30)
        This masking layer will skip any token rows with all zeros (which happens when the # of tokens in that program are < the avg size).
    """
    model.add(Masking(mask_value=0.0, input_shape=(maxlen, vector_dim)))

    """    
        layers are used to construct a deeplearning model; tensors are used to define the dataflow thru the model
        layers=2 in our case, since end exclusive, so we should have only 1 layer in this for loop.
        This is our first layer which takes in the original input sequence 
    """
    for i in range(1, layers):
        model.add(Bidirectional(GRU(units=256, activation='tanh', recurrent_activation='hard_sigmoid', return_sequences=True)))
        model.add(Dropout(dropout))
    
    # 2nd later which i think has a reversed copy of the input sequence? But maybe the # of Dense() fns called determines the # of layers, which then would mean we only have 1 lare total...
    model.add(Bidirectional(GRU(units=256, activation='tanh', recurrent_activation='hard_sigmoid')))
    model.add(Dropout(dropout))
    
    # 1 = units. 2D input of (batch_size, input_dim) would return (batch_size, units)
    # For example, if our input is (25, 30), our output would be (25, 1)
    model.add(Dense(1, activation='sigmoid')) # Dense() outputs a function which takes a tensor as input and outputs a tensor. Might be our 'last' layer...
    
    TP_count = TruePositives()
    TN_count = TrueNegatives() 
    FP_count = FalsePositives() 
    FN_count = FalseNegatives()
          
    #model.compile(loss='binary_crossentropy', optimizer='sgd', metrics= ['accuracy', TP_count, TN_count, FP_count, FN_count])
    #sgd = optimizers.SGD(learning_rate=0.01, decay=1e-6, momentum=0.9, nesterov=True)
    
    model.compile(loss='binary_crossentropy', optimizer = myoptimizer, metrics=['accuracy'])
    model.summary()
 
    return model


def buildBLSTM(maxlen, vector_dim, layers, dropout, myoptimizer): 
    print('\nBuild model...')
    model = Sequential()
    
    model.add(Masking(mask_value=0.0, input_shape=(maxlen, vector_dim)))
    
    for i in range(1, layers):
        model.add(Bidirectional(LSTM(units = 256, activation='tanh', recurrent_activation='sigmoid', use_bias=True, kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal', bias_initializer='zeros', unit_forget_bias=True, kernel_regularizer=None, recurrent_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, recurrent_constraint=None, bias_constraint=None, dropout=0.0, recurrent_dropout=0.0, implementation=2, return_sequences=True, return_state=False, go_backwards=False, stateful=False, unroll=False)))
        
    model.add(Bidirectional(GRU(units=256, activation='tanh', recurrent_activation='hard_sigmoid')))
    model.add(Dropout(dropout))
    
    model.add(Dense(1, activation='sigmoid'))
    
    TP_count = TruePositives()
    TN_count = TrueNegatives() 
    FP_count = FalsePositives() 
    FN_count = FalseNegatives()
          
    #model.compile(loss='binary_crossentropy', optimizer='sgd', metrics= ['accuracy', TP_count, TN_count, FP_count, FN_count])
    #sgd = optimizers.SGD(learning_rate=0.01, decay=1e-6, momentum=0.9, nesterov=True)
    
    model.compile(loss='binary_crossentropy', optimizer = myoptimizer, metrics=['accuracy'])
    model.summary()
 
    return model


def fitModel(myKerasModel,  weightpath, traindataSet_path, batch_size, maxlen, vector_dim, RANDOMSEED):
    
    print("\nFit model with Training set (binary)...")
    dataset = []
    labels = []

    for filename in os.listdir(traindataSet_path):
        if not filename.endswith(".DS_Store"):
            print(filename)
            f = open(os.path.join(traindataSet_path, filename),"rb")
            data = pickle.load(f)
            f.close()
            dataset += data[0]
            labels += data[1]
    print(len(dataset), len(labels))

    bin_labels = []
    for label in labels:
        bin_labels.append(multi_labels_to_two(label))
    labels = bin_labels
    
    np.random.seed(RANDOMSEED)
    np.random.shuffle(dataset)
    np.random.seed(RANDOMSEED)
    np.random.shuffle(labels)
   
   # Generator is used when too much data to fit in memory. 
   # Generates batch_size number of the samples of the data at a time.
    train_generator = generator_of_data(dataset, labels, batch_size, maxlen, vector_dim)   
    all_train_samples = len(dataset)
    steps_epoch = int(all_train_samples / batch_size)
    t1 = time.time()
    print(f'Fitting model start: {t1}')
    myKerasModel.fit(train_generator, steps_per_epoch=steps_epoch, epochs=10)
    t2 = time.time()
    train_time = t2 - t1
    print(f'Fitting model time total: {train_time}\n')
    myKerasModel.save_weights(weightpath)
    return myKerasModel



""" Multiclass """

def buildBGRU2(maxlen, vector_dim, layers, dropout, myoptimizer, density=9): 
    print('\nBuild model...')
    """
        A Sequential model is appropriate for a plain stack of layers where each layer has exactly one input tensor and one output tensor. 
        In mathematics, a tensor is an algebraic object that describes a multilinear relationship between sets of algebraic objects related to a vector space.
    """
    model = Sequential()
    
    """
        All layers in Keras need to know the shape of their inputs in order to be able to create their weights. (avg*30)
        In input_shape, you don't need to specify batch size.  Instead takes in (timesteps, features) --> (avg, 30)
        This masking layer will skip any token rows with all zeros (which happens when the # of tokens in that program are < the avg size).
    """
    model.add(Masking(mask_value=0.0, input_shape=(maxlen, vector_dim)))

    """    
        layers are used to construct a deeplearning model; tensors are used to define the dataflow thru the model
        layers=2 in our case, since end exclusive, so we should have only 1 layer in this for loop.
        This is our first layer which takes in the original input sequence.
        Sigmoid activation for multiclass labels.
    """
    for i in range(1, layers):
        model.add(Bidirectional(GRU(units=256, activation='sigmoid', recurrent_activation='hard_sigmoid', return_sequences=True)))
        model.add(Dropout(dropout))
    
    # 2nd later which i think has a reversed copy of the input sequence? But maybe the # of Dense() fns called determines the # of layers, which then would mean we only have 1 layer total...
    model.add(Bidirectional(GRU(units=256, activation='sigmoid', recurrent_activation='hard_sigmoid')))
    model.add(Dropout(dropout))
    
    """
        9 = units. 
        [[1. 0. 0. 0. 0. 0. 0. 0. 0.]
        [0. 1. 0. 0. 0. 0. 0. 0. 0.]
        [0. 0. 1. 0. 0. 0. 0. 0. 0.]
        [0. 0. 0. 1. 0. 0. 0. 0. 0.]
        [0. 0. 0. 0. 1. 0. 0. 0. 0.]
        [0. 0. 0. 0. 0. 1. 0. 0. 0.]
        [0. 0. 0. 0. 0. 0. 1. 0. 0.]
        [0. 0. 0. 0. 0. 0. 0. 1. 0.]
        [0. 0. 0. 0. 0. 0. 0. 0. 1.]]
        
        2D input of (batch_size, input_dim) would return (batch_size, units)
        For example, if our input is (25, 30), our output would be (25, 8)
    """
    model.add(Dense(density, activation='sigmoid')) # Dense() outputs a function which takes a tensor as input and outputs a tensor. Might be our 'last' layer...
    
    TP_count = TruePositives()
    TN_count = TrueNegatives() 
    FP_count = FalsePositives() 
    FN_count = FalseNegatives()

    model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics= ['accuracy', TP_count, TN_count, FP_count, FN_count]) # shows all the metric scores in each training epoch
    sgd = SGD(learning_rate=0.01, decay=1e-6, momentum=0.9, nesterov=True)
    
    # model.compile(loss='categorical_crossentropy', optimizer = myoptimizer, metrics=['accuracy'])
    model.summary()
 
    return model

def buildBLSTM2(maxlen, vector_dim, layers, dropout, myoptimizer, density=9): 
    print('\nBuild model...')
    model = Sequential()
    
    model.add(Masking(mask_value=0.0, input_shape=(maxlen, vector_dim)))
    
    # Sigmoid activation for multiclass labels.
    for i in range(1, layers):
        model.add(Bidirectional(LSTM(units = 256, activation='sigmoid', recurrent_activation='sigmoid', use_bias=True, kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal', bias_initializer='zeros', unit_forget_bias=True, kernel_regularizer=None, recurrent_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, recurrent_constraint=None, bias_constraint=None, dropout=0.0, recurrent_dropout=0.0, implementation=2, return_sequences=True, return_state=False, go_backwards=False, stateful=False, unroll=False)))
        
    model.add(Bidirectional(GRU(units=256, activation='sigmoid', recurrent_activation='hard_sigmoid')))
    model.add(Dropout(dropout))
    
    model.add(Dense(density, activation='sigmoid'))
    
    TP_count = TruePositives()
    TN_count = TrueNegatives() 
    FP_count = FalsePositives() 
    FN_count = FalseNegatives()
          
    model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics= ['accuracy', TP_count, TN_count, FP_count, FN_count])
    sgd = optimizers.SGD(learning_rate=0.01, decay=1e-6, momentum=0.9, nesterov=True)
    
    # model.compile(loss='categorical_crossentropy', optimizer = myoptimizer, metrics=['accuracy'])
    model.summary()
 
    return model

def fitModel2(myKerasModel,  weightpath, traindataSet_path, batch_size, maxlen, vector_dim, RANDOMSEED):
    """
        Fit with multiclass labels.
    """
    print("\nFit model with Training set (multiclass)...")
    dataset = []
    labels = []

    for filename in os.listdir(traindataSet_path):
        if filename.endswith(".pkl"):
            print('filename: ',filename)
            f = open(os.path.join(traindataSet_path, filename),"rb")
            data = pickle.load(f)
            f.close()
            dataset = data[0]
            labels = data[-2]
    print(len(dataset), len(labels))
    print(labels)
    
    np.random.seed(RANDOMSEED)
    np.random.shuffle(dataset)
    np.random.seed(RANDOMSEED)
    np.random.shuffle(labels)
   
   # Generator is used when too much data to fit in memory. 
   # Generates batch_size number of the samples of the data at a time.
    train_generator = generator_of_data(dataset, labels, batch_size, maxlen, vector_dim)   
    all_train_samples = len(dataset)
    steps_epoch = int(all_train_samples / batch_size)
    print("start")
    myKerasModel.fit(train_generator, steps_per_epoch=steps_epoch, epochs=15)
    myKerasModel.save_weights(weightpath)
    return myKerasModel
