import pickle
import time, datetime
import os

import numpy as np
import tensorflow as tf
from tensorflow.keras.metrics import TruePositives, TrueNegatives,FalsePositives, FalseNegatives
from keras.preprocessing import sequence
from tensorflow.keras.optimizers import SGD, RMSprop, Adagrad, Adam, Adadelta
from keras.models import Sequential
from keras.layers.core import Masking, Dense, Dropout
from keras.layers.recurrent import LSTM,GRU
from keras.layers.wrappers import Bidirectional

from SYSE_1_isVulnerable.preprocess_dl_Input_version5 import *

""" Binary Class """

def buildBGRU(maxlen, vector_dim, layers, dropout, myoptimizer): 
    print('\nBuild model...')

    model = Sequential()
    model.add(Masking(mask_value=0.0, input_shape=(maxlen, vector_dim)))

    for i in range(1, layers):
        model.add(Bidirectional(GRU(units=256, activation='tanh', recurrent_activation='hard_sigmoid', return_sequences=True)))
        model.add(Dropout(dropout))
    
    model.add(Bidirectional(GRU(units=256, activation='tanh', recurrent_activation='hard_sigmoid')))
    model.add(Dropout(dropout))
    
    model.add(Dense(1, activation='sigmoid'))
    
    # TP_count = TruePositives()
    # TN_count = TrueNegatives() 
    # FP_count = FalsePositives() 
    # FN_count = FalseNegatives()
          
    #model.compile(loss='binary_crossentropy', optimizer='sgd', metrics= ['accuracy', TP_count, TN_count, FP_count, FN_count])
    #sgd = SGD(learning_rate=0.01, decay=1e-6, momentum=0.9, nesterov=True)
    
    model.compile(loss='binary_crossentropy', optimizer = myoptimizer, metrics=['accuracy'])
    model.summary()
 
    return model


def buildBLSTM(maxlen, vector_dim, layers, dropout, myoptimizer): 
    print('\nBuild model...')
    model = Sequential()
    
    model.add(Masking(mask_value=0.0, input_shape=(maxlen, vector_dim)))
    
    for i in range(1, layers):
        model.add(Bidirectional(LSTM(units = 256, activation='tanh', recurrent_activation='sigmoid', use_bias=True, kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal', bias_initializer='zeros', unit_forget_bias=True, kernel_regularizer=None, recurrent_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, recurrent_constraint=None, bias_constraint=None, dropout=0.0, recurrent_dropout=0.0, implementation=2, return_sequences=True, return_state=False, go_backwards=False, stateful=False, unroll=False)))
        
    model.add(Bidirectional(GRU(units=256, activation='tanh', recurrent_activation='hard_sigmoid')))
    model.add(Dropout(dropout))
    
    model.add(Dense(1, activation='sigmoid'))
    
    # TP_count = TruePositives()
    # TN_count = TrueNegatives() 
    # FP_count = FalsePositives() 
    # FN_count = FalseNegatives()
          
    #model.compile(loss='binary_crossentropy', optimizer='sgd', metrics= ['accuracy', TP_count, TN_count, FP_count, FN_count])
    #sgd = SGD(learning_rate=0.01, decay=1e-6, momentum=0.9, nesterov=True)
    
    model.compile(loss='binary_crossentropy', optimizer = myoptimizer, metrics=['accuracy'])
    model.summary()
 
    return model


def fitModel(myKerasModel,  weightpath, traindataSet_path, batch_size, maxlen, vector_dim, RANDOMSEED):
    
    print("\nFit model with Training set (binary)...")
    dataset = []
    labels = []

    for filename in os.listdir(traindataSet_path):
        if not filename.endswith(".DS_Store"):
            print(filename)
            f = open(os.path.join(traindataSet_path, filename),"rb")
            data = pickle.load(f)
            f.close()
            dataset += data[0]
            labels += data[1]
    print(len(dataset), len(labels))

    bin_labels = []
    for label in labels:
        bin_labels.append(multi_labels_to_two(label))
    labels = bin_labels
    
    np.random.seed(RANDOMSEED)
    np.random.shuffle(dataset)
    np.random.seed(RANDOMSEED)
    np.random.shuffle(labels)
   
   # Generator is used when too much data to fit in memory. 
   # Generates batch_size number of the samples of the data at a time.
    train_generator = generator_of_data(dataset, labels, batch_size, maxlen, vector_dim)   
    all_train_samples = len(dataset)
    steps_epoch = int(all_train_samples / batch_size)
    t1 = time.time()
    print(f'Fitting model start: {t1}')
    myKerasModel.fit(train_generator, steps_per_epoch=steps_epoch, epochs=10)
    t2 = time.time()
    train_time = t2 - t1
    print(f'Fitting model time total: {train_time}\n')
    myKerasModel.save_weights(weightpath)
    return myKerasModel



""" Multiclass """

def buildBGRU2(maxlen, vector_dim, layers, dropout, myoptimizer, density=9, activation_fn='sigmoid', metrics=['CategoricalAccuracy', 'Recall']): 
    print('\nBuild model BGRU...')
    """
        A Sequential model is appropriate for a plain stack of layers where each layer has exactly one input tensor and one output tensor. 
        In mathematics, a tensor is an algebraic object that describes a multilinear relationship between sets of algebraic objects related to a vector space.
    """
    model = Sequential()
    
    """
        All layers in Keras need to know the shape of their inputs in order to be able to create their weights. (avg*30)
        In input_shape, you don't need to specify batch size.  Instead takes in (timesteps, features) --> (avg, 30)
        This masking layer will skip any token rows with all zeros (which happens when the # of tokens in that program are < the avg size).
    """
    model.add(Masking(mask_value=0.0, input_shape=(maxlen, vector_dim)))

    """    
        layers are used to construct a deeplearning model; tensors are used to define the dataflow thru the model
        layers=2 in our case, since end exclusive, so we should have only 1 layer in this for loop.
        This is our first layer which takes in the original input sequence.
        Sigmoid activation for multiclass labels.
    """
    for _ in range(1, layers):
        model.add(Bidirectional(GRU(units=256, activation='sigmoid', recurrent_activation='hard_sigmoid', return_sequences=True)))
        model.add(Dropout(dropout))
    
    # 2nd later which i think has a reversed copy of the input sequence? But maybe the # of Dense() fns called determines the # of layers, which then would mean we only have 1 layer total...
    model.add(Bidirectional(GRU(units=256, activation='sigmoid', recurrent_activation='hard_sigmoid')))
    model.add(Dropout(dropout))
    
    """        
        2D input of (batch_size, input_dim) would return (batch_size, units)
        For example, if our input is (25, 30), our output would be (25, 8)
    """
    model.add(Dense(density, activation=activation_fn)) # Dense() outputs a function which takes a tensor as input and outputs a tensor.
    
    model.compile(loss='categorical_crossentropy', optimizer=myoptimizer, metrics=metrics)
    model.summary()
 
    return model

def buildBLSTM2(maxlen, vector_dim, layers, dropout, myoptimizer, density=9, activation_fn='sigmoid', metrics=['CategoricalAccuracy', 'Recall']): 
    print('\nBuild model BLSTM...')
    model = Sequential()
    
    model.add(Masking(mask_value=0.0, input_shape=(maxlen, vector_dim)))
    
    # Sigmoid activation for multiclass labels.
    for _ in range(1, layers):
        model.add(Bidirectional(LSTM(units = 256, activation='sigmoid', recurrent_activation='sigmoid', use_bias=True, kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal', bias_initializer='zeros', unit_forget_bias=True, kernel_regularizer=None, recurrent_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, recurrent_constraint=None, bias_constraint=None, dropout=0.0, recurrent_dropout=0.0, implementation=2, return_sequences=True, return_state=False, go_backwards=False, stateful=False, unroll=False)))

    model.add(Bidirectional(LSTM(units=256, activation='sigmoid', recurrent_activation='hard_sigmoid')))
    model.add(Dropout(dropout))
    model.add(Dense(density, activation=activation_fn))
    
    model.compile(loss='categorical_crossentropy', optimizer=myoptimizer, metrics=metrics)
    model.summary()
 
    return model

def fitModel2(myKerasModel,  weightpath, traindataSet_path, batch_size, maxlen, vector_dim, RANDOMSEED, epochs=10, useGenerator=True, callbacks=None):
    """
        Fit with multiclass labels.
        Keras model .fit and .compile methods will handle the distribution of dataset across replicas 
        for us with MirroredStrategy. https://www.tensorflow.org/api_docs/python/tf/distribute/MirroredStrategy
    """
    print("\nFit model with Training set (multiclass)...")
    dataset = []
    labels = []

    for filename in os.listdir(traindataSet_path):
        if filename.endswith(".pkl"):
            print('filename: ',filename)
            f = open(os.path.join(traindataSet_path, filename),"rb")
            data = pickle.load(f)
            f.close()
            dataset = data[0]
            labels = data[-2]
    all_train_samples = len(dataset)
    print(all_train_samples, len(labels))
    print(labels)
    
    np.random.seed(RANDOMSEED)
    np.random.shuffle(dataset)
    np.random.seed(RANDOMSEED)
    np.random.shuffle(labels)

    if useGenerator:
        # Generator is used when too much data to fit in memory. 
        # Generates batch_size number of the samples of the data at a time.
        train_generator = generator_of_data(dataset, labels, batch_size, maxlen, vector_dim)
        # options = tf.data.Options()
        # options.experimental_distribute.auto_shard_policy = tf.data.experimental.AutoShardPolicy.DATA
        dataset = tf.data.Dataset.from_generator(lambda: train_generator, 
                                        output_signature=(
                                            tf.TensorSpec(shape=(batch_size, maxlen, vector_dim), dtype=tf.float32),
                                            tf.TensorSpec(shape=(batch_size, len(labels[0])), dtype=tf.float32) 
                                        ))
        steps_epoch = int(all_train_samples / batch_size)
        t1 = datetime.datetime.now()
        print(f'Fitting model start: {t1}')
        myKerasModel.fit(dataset, steps_per_epoch=steps_epoch, epochs=epochs, callbacks=callbacks)
        t2 = datetime.datetime.now()
        train_time = t2 - t1
        print(f'Fitting model time total: {train_time}\n')
        myKerasModel.save_weights(weightpath)
        return myKerasModel
    else:
        # MirroredStrategy Approach on all data
        # Make sure you have enough training samples to do it this way.
        try:
            x_train = np.reshape(dataset, (len(dataset), maxlen, vector_dim))
        except Exception as e:
            x_train = process_sequences_shape(dataset, maxlen, vector_dim)
            pass
        y_train = np.array(labels)
        
        options = tf.data.Options()
        options.experimental_distribute.auto_shard_policy = tf.data.experimental.AutoShardPolicy.DATA

        ## Transform to tf Dataset type for MirroredStrategy
        train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))
        train_dataset = train_dataset.with_options(options)
        train_dataset = train_dataset.batch(batch_size)

        steps_epoch = int(all_train_samples / batch_size)
        myKerasModel.fit(train_dataset, steps_per_epoch=steps_epoch, epochs=epochs, callbacks=callbacks)
        myKerasModel.save_weights(weightpath)
        return myKerasModel






