import gc, os, sys
import numpy as np

VUL_PATH = os.environ.get('VUL_PATH')
sys.path.insert(1, VUL_PATH)

from SYSE_2_vulnerabilityType.VulnerabilityClassification import VulnerabilityClassification
from utils.Word2VecModel import Word2VecModel
from utils.DLCustomModels import create_blstm_model
from utils.utils import drop_non_vulnerable_samples, getDataset, preprocess_data_filter_by_count

def main(vectorsALLdir='ALL_vectors_granular_vulnerable_only', min_num=None):
    RANDOMSEED = 1099
    CLASS_TYPE = 'ALL_vul'
    VECTOR_TRANSFORMER='w2v'
    BATCHSIZE = 64
    EPOCHS = 60
    MODEL_TYPE = 'blstm'
    VUL_TYPE='all'

    vectorRootPath = os.path.join('data', 'DLvectors')
    vectorsALLPath = os.path.join(vectorRootPath, vectorsALLdir)
    vectorTrainPath = os.path.join(vectorRootPath, f'train')
    vectorTestPath = os.path.join(vectorRootPath, f'test')

    model_name = '%s_%s_batch_%s_seed_%s_epochs_%s_%s' % (MODEL_TYPE.upper(), VECTOR_TRANSFORMER.upper(), BATCHSIZE, RANDOMSEED, EPOCHS, CLASS_TYPE)
    metrics_path = os.path.join(f'metrics', MODEL_TYPE)
    w2vmodelPath = os.path.join('w2vModel','model', f'w2vModel_{VUL_TYPE}')
    all_tokens_path = os.path.join('data', 'tokens')

    dvt = VulnerabilityClassification(build_model=create_blstm_model,
        transformer_model=Word2VecModel, transformerPath=w2vmodelPath,
        metricsPath=metrics_path, randomSeed=RANDOMSEED, window=3, m_epochs=EPOCHS,
        modelName=model_name, tokensPath=all_tokens_path, vectorsALLPath=vectorsALLPath,
        vectorTrainPath=vectorTrainPath, vectorTestPath=vectorTestPath)

    print('Starting %s & %s' % (CLASS_TYPE, MODEL_TYPE))

    # Reset if need be
    print('Resetting checkpoint and weights...')
    dvt.reset_checkpoint_and_weights(dvt.weightpath)

    # tokenize the vulnerable samples
    # print('\nTokenizing slices...')
    # dvt.tokenizeSlicesPerFile(slicefile, dvt.tokensPath, focusPointerIndex=-1)

    # Fit W2V transformer & transform our data
    print('\nTraining W2V model...')
    dvt.init_transformer()
    dvt.fit_transform(dvt.transformerPath, dvt.tokensPath, dvt.vectorsALLPath, getBalanced=False)

    # Drop non-vulnerable samples
    outputpath = os.path.join(vectorRootPath, 'ALL_vulnerable_vectors')
    drop_non_vulnerable_samples(dvt.vectorsALLPath, outputpath)
    dvt.vectorsALLPath = outputpath

    # Get ALL samples
    data = getDataset(dvt.vectorsALLPath, False)

    # Drop ones < 100 samples
    new_data = preprocess_data_filter_by_count(min_num, data)
    X = new_data[0]
    del data
    gc.collect()

    # Get average
    meanLen = 214

    # List the IDs & class of samples whose avg is > avg
    subset = [[], []]

    # Count how many samples > avg
    count = 0
    for i in range(len(X)):
        if len(X[i]) > meanLen:
            count += 1
            # List the IDs & class of samples whose avg is > avg
            # print(f'ID: {new_data[-1][i]}\tClass: {new_data[-2][i]}')
            subset[0].append(new_data[-1][i])
            subset[1].append(new_data[-2][i])

    print('Number of samples that have length greater than the avg: ', count)
    
    # GROUP BY class to see count
    # Get ratios
    classes, y_indices = np.unique(subset[1], return_inverse=True)
    class_counts = np.bincount(y_indices)
    # Find the sorted list of instances for each class
    class_indices = np.split(np.argsort(y_indices, kind='mergesort'), np.cumsum(class_counts)[:-1])
    # summarize distribution
    for k,v in zip(classes, class_counts):
        per = v / sum(class_counts) * 100
        print('Encoded Label=%d,\tn=%d (%.3f%%)' % (k, v, per))

    print()
    for class_indxs in class_indices:
        for i in class_indxs:
            print(f'ID: {subset[0][i]}\tClass: {subset[1][i]}')

    



vectorsALLdir='ALL_vectors'
min_number = 100
main(min_num=min_number, vectorsALLdir=vectorsALLdir)


