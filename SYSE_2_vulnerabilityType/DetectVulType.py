import os, sys
from shutil import rmtree

from numpy.lib.function_base import vectorize

VUL_PATH = os.environ.get('VUL_PATH')
sys.path.insert(1, VUL_PATH)

import keras
import numpy as np
import tensorflow as tf

from SYSE_1_isVulnerable.slicesToTokens import tokenizeSlices_Multiclass
from SYSE_1_isVulnerable.splitTrainTest import splitTrainTestCategorical
from SYSE_1_isVulnerable.adjustVectorLen import meanLen
from SYSE_1_isVulnerable.saveKeyData import saveKeyDataMulticlass
from SYSE_1_isVulnerable.DLModel import buildBGRU2, fitModel2
from SYSE_1_isVulnerable.ConfusionMatrix import getConfusionMatrix_Multiclass
from SYSE_1_isVulnerable.DLPrediction import predictMulticlassLabel
from SYSE_1_isVulnerable.evaluateModels import roc_auc_score_multiclass
from utils.Word2VecModel import Word2VecModel
from utils.DLCustomModels import make_or_restore_model
from utils.transformDataDimensions import tranformDimsByFocus
from utils.utils import downsample, flatten_categories, getDataset, encode_target, save_data_to_file

vType = "ALL"
randomSeed = 1099
tokenPath = os.path.join('data', 'tokenD2V')
vectorPath = os.path.join('data','vector')
vectorBalancedPath = os.path.join('data','DLvectors', 'ALL_balanced')
vectorTypePath = os.path.join('data','DLvectors')
vectorTrainPath = os.path.join(vectorTypePath,'train')
vectorTestPath = os.path.join(vectorTypePath,'test')
dlInputsTrainPath = os.path.join('data','DLinputs','train')
dlInputsTestPath  = os.path.join('data','DLinputs','test')


class DetectVulType():
    
    def __init__(self, build_model, transformer_model=None, metrics=[], cv=1, vector_size=30, tokenPath=tokenPath, vectorBalancedPath=vectorBalancedPath, vectorTypePath=vectorTypePath, vectorTrainPath=vectorTrainPath, vectorTestPath=vectorTestPath, inputsTrainPath=dlInputsTrainPath, inputsTestPath=dlInputsTestPath) -> None:
        self.build_model = build_model
        self.model = None
        self.transformer_model = transformer_model
        self.transformer = None
        self.metrics = metrics
        self.cv = cv
        self.avg = 0
        self.randomSeed = None
        self.tokenPath = tokenPath # files in here replaced the functions index with focus pointer index; ALL_tokens.pkl
        self.vector_size = vector_size
        self.vectorTypePath = vectorTypePath
        self.vectorBalancedPath = vectorBalancedPath
        self.vectorTrainPath = vectorTrainPath
        self.vectorTestPath = vectorTestPath
        self.inputsTrainPath = inputsTrainPath
        self.inputsTestPath = inputsTestPath
        self.checkpoint_dir = "./ckpt"
        
    def tokenizeSlices(self, numSamples=420627):
        """  
            Initializes the dataset by turning the source code slices into tokens. 
            numSamples := int; Max Num of slice samples from each file
        """
        slicePath = os.path.join('data','slicesSource')
        tokenPath = os.path.join(self.tokenPath,'SARD')
        multiclasspath = os.path.join('data','CVE','SARD_CVE_to_groups.csv')
        if os.path.exists(tokenPath) and os.listdir(tokenPath):
            rmtree(tokenPath)
            os.mkdir(tokenPath)
        tokenizeSlices_Multiclass(slicePath, tokenPath, multiclasspath, numSamples)

    def downsample(self):        
        print('\nDOWNSAMPLING THE DATA...')
        all_data = getDataset(self.tokenPath, False, self.randomSeed)
        downsampled_data = downsample(all_data, 664, 1000)
        downsampled_data = downsample(downsampled_data, 710, 1000)
        downsampled_data = downsample(downsampled_data, 707, 1000)
        downsampled_data = downsample(downsampled_data, 682, 1000)

        print('SAVING BALANCED, DOWNSAMPLES DATA TO FILE...')
        save_data_to_file(self.vectorTypePath, 'balanced_vectors.pkl', downsampled_data)

    def flatten_categories(self, dirname, filename):
        """
            Since we are not merging any classes, we'll replace the group id section with the flattened version containing the original label
            Flattens groupid array for all samples into the assigned groupids only.
            Example: [[664, 664], [703, 555]] --> [664, 703]
        """
        all_data = getDataset(dirname, False, self.randomSeed)
        all_data[-2] = flatten_categories(all_data[-2])
        save_data_to_file(dirname, filename, all_data)

    def init_transformer(self, alpha, negative, sample, epochs, window, workers):
        print(f'Fitting transformer with paramaters: Seed: {self.randomSeed}, Alpha: {alpha}, Epochs: {epochs}, Sample: {sample}, Negative: {negative}, Window: {window}, Vector length: {self.vector_size}.')
        self.transformer = self.transformer_model(vector_size=self.vector_size, alpha=alpha, negative=negative, sample=sample, epochs=epochs, seed=self.randomSeed, window=window, workers=workers)

    def fit_transform(self, modelPath, dataset_path, getBalanced=True):
        # Fit on all the data & transform to vectors
        data = getDataset(dataset_path=dataset_path, getBalanced=getBalanced)
        X = data[0]
        self.transformer.fit(X)
        self.transformer.save_model(modelPath)
        X = self.transformer.transform(X)
        data[0] = X

        # Save data to a file
        save_data_to_file(self.vectorBalancedPath, 'balanced_vectors.pkl', data)
        
    def splitTrainTest(self):
        # Split into train/test
        splitTrainTestCategorical('balanced', self.vectorBalancedPath, self.vectorTrainPath, self.vectorTestPath, randomSeed=self.randomSeed)
        
        # TODO: Implement CV?
        # Split training set into training/validation set
        # splitTrainTestCategorical('balanced', self.vectorTrainPath, self.vectorTrainPath, self.vectorTestPath, randomSeed=self.randomSeed)


    def adjustVectorLength(self):
        # Adjust vector length TODO: Why do we do the avg instead of all the lines? Will our models perform worse if too many zero-padded vectors?
        self.avg = meanLen(self.vectorTrainPath)

        tranformDimsByFocus(self.vectorTrainPath, self.vectorTestPath, self.inputsTrainPath, self.inputsTestPath, self.avg, self.vector_size)
        print(f'New Vector Length (rows x cols): {self.avg} x {self.vector_size}\n')

    def encodeLabels(self):
        # ### Get number of unique classes for density value
        train_data = getDataset(self.vectorTrainPath, getBalanced=True)
        categories = np.unique(train_data[-2])
        self.labelEncoder = encode_target(categories)[1]
        self.density_units = categories.shape[0]

        saveKeyDataMulticlass(self.inputsTrainPath, self.labelEncoder)
        saveKeyDataMulticlass(self.inputsTestPath, self.labelEncoder)

    def build_and_fit(self):
        """
            builds & fits the estimator model
        """
        self.optimizer = 'adam' #can be changed to ‘adamax’
        self.layers = 2
        self.dropout = 0.2 
        self.batch_size = 16
        epochs = 20
        self.activation_fn = 'softmax'

        # Prepare a directory to store all the checkpoints.
        if not os.path.exists(self.checkpoint_dir):
            os.makedirs(self.checkpoint_dir)

        # Open a strategy scope
        strategy = tf.distribute.MirroredStrategy()
        print('Number of devices: {}'.format(strategy.num_replicas_in_sync))
        with strategy.scope():
            myKerasModel = make_or_restore_model(self.checkpoint_dir, self.get_compiled_model)

        callbacks = [
            # This callback saves a SavedModel every epoch
            # We include the current epoch in the folder name.
            keras.callbacks.ModelCheckpoint(
                filepath=self.checkpoint_dir + "/ckpt-{epoch}", save_freq="epoch"
                )
        ]

        # Fit BGRU Model with trained data and save the model for later use
        self.weightpath = os.path.join('model', 'BRGU_ALL' + self.optimizer +str(self.randomSeed))
        self.model = fitModel2(myKerasModel, self.weightpath, self.inputsTrainPath, self.batch_size, self.avg, self.vector_size, self.randomSeed, epochs, useGenerator=True, callbacks=callbacks)

    def get_compiled_model(self):
        return self.build_model(self.avg, self.vector_size, self.layers, self.dropout, self.optimizer, self.density_units, activation_fn=self.activation_fn)

    def predict_and_score(self):
        modelName = 'BGRU%s_' % self.randomSeed

        # Prediction
        self.model.load_weights(self.weightpath)
        thresholds_dl_labels, mypredicted_labels, myreallabels, outputs_dict = predictMulticlassLabel(self.model, self.inputsTestPath, self.avg, self.vector_size, self.optimizer, modelName, self.randomSeed, self.labelEncoder, saveOutput=False)

        # Confusion Matrix
        metrics = getConfusionMatrix_Multiclass(mypredicted_labels, myreallabels, saveFig=False, path='None')
        accuracy = metrics[0]
        weighted_precision = metrics[-3]
        weighted_recall = metrics[-2]
        weighted_f1 = metrics[-1]

        # ROC
        roc_auc_dict = roc_auc_score_multiclass(myreallabels, mypredicted_labels)
        print(f'\n{modelName} ROC AUC Score\n{roc_auc_dict}\n')

        return outputs_dict, accuracy, weighted_precision, weighted_recall, weighted_f1


    def run_all(self, w2vmodelPath, startOver=True):
        # W2V params
        alpha = 0.05
        epochs = 15
        sample = 0.001
        workers = -1
        negative = 10
        window = 3
        self.vector_size = 30

        if startOver and os.path.exists(self.checkpoint_dir):
            rmtree(self.checkpoint_dir)

        # Init transformer
        self.init_transformer(alpha, negative, sample, epochs, window, workers)

        # Fit transformer & transform our data
        self.fit_transform(w2vmodelPath, dataset_path=self.tokenPath, getBalanced=False)

        # Split data into training and test set
        self.splitTrainTest()

        # Average out the row length per sample based on focuspointer
        self.adjustVectorLength()

        # Hot encode labels
        self.encodeLabels()

        # Build & fit the model
        self.build_and_fit()

        # # Predict and score on test set
        outputs_dict, accuracy, weighted_precision, weighted_recall, weighted_f1 = self.predict_and_score()

        # Remove our checkpoint so the new model won't load our previous model
        rmtree(self.checkpoint_dir)



gs = DetectVulType(build_model=buildBGRU2, transformer_model=Word2VecModel, vector_size=30)
# gs.tokenizeSlices(numSamples=100) # Initializes the dataset (if need be)
# gs.flatten_categories(tokenPath, 'ALL_tokens.pkl')
w2vmodelPath = os.path.join('w2vModel','model','w2vModel_ALL')
gs.run_all(w2vmodelPath, startOver=True)

