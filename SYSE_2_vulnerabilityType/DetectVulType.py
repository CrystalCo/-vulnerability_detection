import os, sys
from shutil import rmtree

import keras
import numpy as np
import tensorflow as tf

VUL_PATH = os.environ.get('VUL_PATH')
sys.path.insert(1, VUL_PATH)

from SYSE_1_isVulnerable.slicesToTokens import tokenizeSlices_Multiclass
from SYSE_1_isVulnerable.splitTrainTest import splitTrainTestCategorical
from SYSE_1_isVulnerable.adjustVectorLen import meanLen
from SYSE_1_isVulnerable.saveKeyData import saveKeyDataMulticlass
from utils.DLCustomModels import fit_custom_dl_model, predictMulticlassLabel
from SYSE_1_isVulnerable.ConfusionMatrix import getConfusionMatrix_Multiclass
from SYSE_1_isVulnerable.evaluateModels import roc_auc_score_multiclass
from utils.DLCustomModels import make_or_restore_model
from utils.transformDataDimensions import tranformDimsByFocus
from utils.utils import downsample, flatten_categories, get_classes_gt, get_unique_cwes, getDataset, encode_target, print_num_samples, save_data_to_file


class DetectVulType():
   
    def __init__(self, build_model, useGenerator=True, transformer_model=None, transformerPath='',  metricsPath='', vector_size=30, randomSeed=1099, alpha=0.05, t_epochs=15, sample=0.001, workers=-1, negative=10, window=3, m_epochs=10, modelName='BGRU_ALL', batch_size=32, mask=True, dropout=0.2, layers=2, optimizer='adam', metrics=['CategoricalAccuracy', 'Recall'], tokensPath='', vectorsALLPath='', vectorRootPath='', vectorTrainPath='', vectorTestPath='', inputsTrainPath='', inputsTestPath='',checkpoint_dir='./ckpt', weightpath=''):
        # Transformer variables
        self.transformer_model = transformer_model
        self.transformer = None
        self.avg = 0
        self.randomSeed = randomSeed
        self.tokensPath = tokensPath 
        self.vector_size = vector_size
        self.alpha = alpha
        self.t_epochs = t_epochs
        self.sample = sample
        self.workers = workers
        self.negative = negative
        self.window = window
        # Estimator Variables
        self.model = None
        self.build_model = build_model
        self.modelName = modelName
        self.useGenerator = useGenerator
        self.optimizer = optimizer #can be changed to ‘adamax’
        self.layers = layers
        self.dropout = dropout
        self.mask = mask # Set to False if using D2V
        self.batch_size = batch_size
        self.m_epochs = m_epochs
        self.activation_fn = 'softmax'
        self.metrics = metrics
        # Paths
        self.multiclasspath = os.path.join('data', 'CVE', 'SARD_CVE_to_CWE.csv')
        self.metricsPath = metricsPath
        self.weightpath = weightpath if weightpath != '' else os.path.join('model', modelName + self.optimizer + str(self.randomSeed))
        self.vectorRootPath = vectorRootPath
        self.vectorsALLPath = vectorsALLPath
        self.vectorTrainPath = vectorTrainPath
        self.vectorTestPath = vectorTestPath
        self.inputsTrainPath = inputsTrainPath
        self.inputsTestPath = inputsTestPath
        self.transformerPath = transformerPath
        self.checkpoint_dir = checkpoint_dir

    def tokenizeSlices(self, numSamples=420627):
        """  
            Initializes the dataset by turning the source code slices into tokens. 
            numSamples := int; Max Num of slice samples from each file
        """
        slicePath = os.path.join('data', 'slicesSource')
        tokenPath = os.path.join('data', 'token', 'SARD')
        if os.path.exists(tokenPath) and os.listdir(tokenPath):
            rmtree(tokenPath)
            os.mkdir(tokenPath)
        tokenizeSlices_Multiclass(slicePath, tokenPath, self.multiclasspath, numSamples)

    def downsample(self, datapath, downsample_num=1000):
        print('\nDOWNSAMPLING THE DATA...')
        all_data = getDataset(datapath, False, self.randomSeed)
        downsampled_data = downsample(all_data, 664, downsample_num)
        downsampled_data = downsample(downsampled_data, 710, downsample_num)
        downsampled_data = downsample(downsampled_data, 707, downsample_num)
        downsampled_data = downsample(downsampled_data, 682, downsample_num)

        print('SAVING BALANCED, DOWNSAMPLES DATA TO FILE...')
        save_data_to_file(self.vectorRootPath, 'balanced_vectors.pkl', downsampled_data)

    def get_dropped_classes(self, minSamples):
        """
            Drop classes that don't have more than minSamples (i.e. number of samples.
            399 & 189 are categories in other views, not obsolete.  Hard coding for now due to time crunch.
        """
        obsolete = [264, 17, 534, 16, 399, 189]
        # Get all classes
        cwes = get_unique_cwes(obsolete)
        # Get Classes > minSamples
        classes_gt = get_classes_gt(self.multiclasspath, minSamples)

        # Filter out classes that aren't in this list
        dropClasses = [c for c in cwes if c not in classes_gt]
        dropClasses = dropClasses + obsolete
        dropClasses = list(set(dropClasses))
        print(f'Dropclasses: ', dropClasses)
        return dropClasses

    def flatten_categories(self, input_data_path, filename, index, isBalanced=False):
        """ 
            Overwrites file with flattened categories. 
            Index := int; 0 = group label; 1 = individual label
        """
        all_data = getDataset(input_data_path, isBalanced, self.randomSeed)
        if type(all_data[-2][0]) != list:
            print(f'Already flattened!  data[-2][0] is of type: {type(all_data[-2][0])}')
            return
        all_data[-2] = flatten_categories(all_data[-2], index)
        save_data_to_file(input_data_path, filename, all_data)

    def init_transformer(self):
        print(f'Fitting transformer with paramaters: Seed: {self.randomSeed}, Alpha: {self.alpha}, Epochs: {self.t_epochs}, Sample: {self.sample}, Negative: {self.negative}, Window: {self.window}, Vector length: {self.vector_size}.')
        self.transformer = self.transformer_model(vector_size=self.vector_size, alpha=self.alpha, negative=self.negative, sample=self.sample, epochs=self.t_epochs, seed=self.randomSeed, window=self.window, workers=self.workers)

    def fit_transform(self, modelPath, dataset_path, vectorALLPath, getBalanced=False, index=1):
        """
            Fit on all the data & transform to vectors
            index = 0 = group ids
            index = 1 = original CWE ID
            Example: [[664, 664], [703, 555], ...] --> [664, 703, ...]
        """
        data = getDataset(dataset_path, getBalanced)
        X = data[0]
        self.transformer.fit(X)
        print(f'Transformer fit complete. Transforming tokens to vectors next...')
        self.transformer.save_model(modelPath)
        X = self.transformer.transform(X)
        data[0] = X
        print(f'Tokens to vectors complete.  Saving to {vectorALLPath} as ALL_vectors.pkl')
        save_data_to_file(vectorALLPath, 'ALL_vectors.pkl', data)
        self.flatten_categories(vectorALLPath, 'ALL_vectors.pkl', index, isBalanced=False)
        
    def splitTrainTest(self, vectorALLPath, minSamples=None):
        # Filter out classes that aren't in our final target classes
        dropClasses = self.get_dropped_classes(minSamples) if minSamples is not None else []
        # Split into train/test
        splitTrainTestCategorical('balanced', vectorALLPath, self.vectorTrainPath, self.vectorTestPath, randomSeed=self.randomSeed, dropClasses=dropClasses)

    def get_avg_length(self):
        self.avg = meanLen(self.vectorTrainPath)
        print_num_samples(self.vectorTrainPath, True)

    def get_full_length(self):
        data = getDataset(self.vectorTrainPath, True)
        self.avg = len(data[0])
        print(f'Total number of samples in datapath set: {len(data[-2])}')

    def adjustVectorLength(self):
        self.get_avg_length()
        tranformDimsByFocus(self.vectorTrainPath, self.vectorTestPath, self.inputsTrainPath, self.inputsTestPath, self.avg, self.vector_size)
        print(f'New Vector Length (rows x cols): {self.avg} x {self.vector_size}\n')

    def encodeLabels(self):
        """ Get number of unique classes for density value. """
        print('\nEncoding labels...')
        train_data = getDataset(self.vectorTrainPath, getBalanced=True)
        categories = np.unique(train_data[-2])
        self.labelEncoder = encode_target(categories)[1]
        self.density_units = categories.shape[0]

    def saveKeyData(self, fromPath, output_path=None):
        saveKeyDataMulticlass(fromPath, self.labelEncoder, output_path)

    def build_and_fit(self):
        """ builds & fits the estimator """
        print(f'BUILDING MODEL {self.modelName}')
        # Prepare a directory to store all the checkpoints.
        if not os.path.exists(self.checkpoint_dir):
            os.makedirs(self.checkpoint_dir)

        # Reduces memory usage by restricting TF to use only 2 GPUs
        gpus = tf.config.experimental.list_physical_devices('GPU')
        for gpu in gpus:
            tf.config.experimental.set_memory_growth(gpu, True)

        # Open a strategy scope
        strategy = tf.distribute.MirroredStrategy(['GPU:0', 'GPU:1'])
        print('\nNumber of devices: {}'.format(strategy.num_replicas_in_sync))
        with strategy.scope():
            latest_epoch, myKerasModel = make_or_restore_model(self.checkpoint_dir, self.get_compiled_model)


        # Fit BGRU Model with trained data and save the model for later use
        # This callback saves a SavedModel every epoch
        # We include the current epoch in the folder name.
        checkpointCallback = [
            keras.callbacks.ModelCheckpoint(
                filepath=os.path.join(self.checkpoint_dir, 'ckpt-{epoch}'),
                monitor='val_acc',
                verbose=1,
                save_best_only=False, # default
                save_weights_only=False, # default
                mode='auto', # default
                save_freq='epoch'
            )
        ]
        self.model = fit_custom_dl_model(myKerasModel, self.weightpath, self.inputsTrainPath, self.batch_size, self.avg, self.vector_size, self.randomSeed, self.m_epochs, useGenerator=self.useGenerator, callbacks=checkpointCallback, latest_epoch=latest_epoch)

    def get_compiled_model(self):
        return self.build_model(self.avg, self.vector_size, self.layers, self.dropout, self.optimizer, self.density_units, activation_fn=self.activation_fn, metrics=self.metrics, mask=self.mask)
    
    def load_weights(self):
        try:
            self.model.load_weights(self.weightpath)
        except Exception as e:
            raise(f'Could not load weight path {self.weightpath}.\n{e}')

    def predict_and_score(self):
        self.load_weights()
        _, mypredicted_labels, myreallabels, _ = predictMulticlassLabel(self.model, self.inputsTestPath, self.avg, self.vector_size, self.optimizer, self.modelName, self.randomSeed, self.labelEncoder, saveOutput=True)

        # Confusion Matrix
        getConfusionMatrix_Multiclass(mypredicted_labels, myreallabels, saveFig=False, path=self.metricsPath, modelName=self.modelName)

        # ROC
        roc_auc_dict = roc_auc_score_multiclass(myreallabels, mypredicted_labels)
        print(f'\n{self.modelName} ROC AUC Score\n{roc_auc_dict}\n')

    def reset_checkpoint(self):
        """Resets saved DL model"""
        if os.path.exists(self.checkpoint_dir):
            rmtree(self.checkpoint_dir)

    def reset_model(self, filename):
        """Resets saved DL model's weights"""
        for f in os.listdir('model'):
            if filename in f:
                fpath = os.path.join('model', f)
                os.remove(fpath)

    def reset_check_weights(self, filename):
        """Resets saved DL model and their weights"""
        self.reset_checkpoint()
        self.reset_model(filename)
    
    def reset_all_models(self):
        """
            Removes all the model weights and current checkpoint. 
            Model weights can be restored from other checkpoints
            that have not been deleted.
        """
        self.reset_checkpoint()
        if len(os.listdir('model')) > 0:
            rmtree('model')
            os.makedirs('model')

    def run_all_w2v(self, numSamples=420627, minSamples=900, index=1):
        """
            Resets everything and initializes all data, W2V transformers, and models.

            numSamples := int; number of samples to extract from slices
            minSamples := int or None; if None, no classes are dropped. If int, drops classes that don't have > minSamples number of samples
            index := int; 1 is for granular classes; 0 is for group classes
        """
        # Delete saved DL models so we don't load in old ones
        self.reset_all_models()

        # Initializes the dataset
        self.tokenizeSlices(numSamples)

        # Init transformer, fit, and split train test sets
        # adjustVectors = True b/c W2V transformed data
        self.run_starting_from_transformer(datapath=self.tokensPath, vectorALLPath=self.vectorsALLPath, getBalanced=False, adjustVectors=True, minSamples=minSamples)
        
        # Hot Encode labels. Build, fit, & predict.
        self.run_starting_from_saveKeyData(self.inputsTrainPath, self.inputsTestPath)

    def run_starting_from_transformer(self, datapath, vectorALLPath, getBalanced=False, adjustVectors=True, minSamples=900, index=1):
        self.init_transformer()
        # Fit transformer & transform our data
        self.fit_transform(self.transformerPath, datapath, vectorALLPath, getBalanced, index=index)
        # Split data into training and test set
        self.splitTrainTest(vectorALLPath, minSamples)
        # Average out the row length per sample based on focuspointer
        if adjustVectors:
            self.adjustVectorLength()

    def run_starting_from_saveKeyData(self, trainPath, testPath, inputTrain=None, inputTest=None):
        # Hot encode labels
        self.encodeLabels()
        self.saveKeyData(trainPath, inputTrain)
        self.saveKeyData(testPath, inputTest)
        # Build & fit the model
        self.build_and_fit()
        # Predict and score on test set
        self.predict_and_score()

    def run_starting_from_model(self):
        self.encodeLabels()
        self.build_and_fit()
        self.predict_and_score()



