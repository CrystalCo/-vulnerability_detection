import os, sys
from shutil import rmtree

import keras
import numpy as np
import tensorflow as tf


VUL_PATH = os.environ.get('VUL_PATH')
sys.path.insert(1, VUL_PATH)

from SYSE_1_isVulnerable.slicesToTokens import save_tokens_to_file, tokenizeSlices_Multiclass, tokenizeSlicesPerFile
from SYSE_1_isVulnerable.splitTrainTest import splitTrainTestCategorical
from SYSE_1_isVulnerable.adjustVectorLen import meanLen
from SYSE_1_isVulnerable.saveKeyData import saveKeyDataMulticlass
from utils.DLCustomModels import fit_custom_dl_model, predictMulticlassLabel
from utils.MLMethods import convert_nested_lists_to_numpy_arrays, fit_transform_PCA, test_and_get_n_for_PCA
from SYSE_1_isVulnerable.ConfusionMatrix import getConfusionMatrix_Multiclass
from SYSE_1_isVulnerable.evaluateModels import roc_auc_score_multiclass
from utils.DLCustomModels import make_or_restore_model
from utils.transformDataDimensions import getAvgLength, tranformDimsByFocus, truncateRows
from utils.utils import downsample, flatten_categories, get_classes_gt, get_unique_cwes, getDataset, encode_target, print_num_samples, save_data_to_file


class DetectVulType():
   
    def __init__(self, build_model, useGenerator=True, transformer_model=None, transformerPath='',  metricsPath='', vector_size=30, randomSeed=1099, alpha=0.05, t_epochs=15, sample=0.001, workers=-1, negative=10, window=3, m_epochs=10, modelName='BGRU_ALL', batch_size=32, mask=True, dropout=0.2, layers=2, optimizer='adam', metrics=['CategoricalAccuracy', 'Recall'], tokensPath='', vectorsALLPath='', vectorRootPath='', vectorTrainPath='', vectorTestPath='', inputsTrainPath='', inputsTestPath='',checkpoint_dir='./ckpt', weightpath=''):
        # Transformer variables
        self.transformer_model = transformer_model
        self.transformer = None
        self.avg = 0
        self.randomSeed = randomSeed
        self.tokensPath = tokensPath 
        self.vector_size = vector_size
        self.alpha = alpha
        self.t_epochs = t_epochs
        self.sample = sample
        self.workers = workers
        self.negative = negative
        self.window = window
        # Estimator Variables
        self.model = None
        self.build_model = build_model
        self.modelName = modelName
        self.useGenerator = useGenerator
        self.optimizer = optimizer #can be changed to ‘adamax’
        self.layers = layers
        self.dropout = dropout
        self.mask = mask # Set to False if using D2V
        self.batch_size = batch_size
        self.m_epochs = m_epochs
        self.activation_fn = 'softmax'
        self.metrics = metrics
        # Paths
        self.multiclasspath = os.path.join('data', 'CVE', 'SARD_CVE_to_CWE.csv')
        self.metricsPath = metricsPath
        self.weightpath = weightpath if weightpath != '' else os.path.join('model', modelName + self.optimizer + str(self.randomSeed))
        self.vectorRootPath = vectorRootPath # Path of the root directory for the vectorized data samples (e.g. data/DLvectors/)
        self.vectorsALLPath = vectorsALLPath # Path that contains a file with all the data samples 
        self.vectorTrainPath = vectorTrainPath # Path that contains the file with the training samples
        self.vectorTestPath = vectorTestPath # Path that contains the file with the test samples
        self.inputsTrainPath = inputsTrainPath # Path that contains the file with the encoded target for the training samples 
        self.inputsTestPath = inputsTestPath # Path that contains the file with the encoded target for the test samples 
        self.transformerPath = transformerPath
        self.checkpoint_dir = checkpoint_dir
        # PCA transformation
        self.best_n = None

    def tokenizeSlices(self, numSamples=420627):
        """  
            Initializes the dataset by turning the source code slices into tokens. 
            numSamples := int; Max Num of slice samples from each file
        """
        slicePath = os.path.join('data', 'slicesSource')
        tokenPath = os.path.join('data', 'token', 'SARD')
        if os.path.exists(tokenPath) and os.listdir(tokenPath):
            rmtree(tokenPath)
            os.mkdir(tokenPath)
        tokenizeSlices_Multiclass(slicePath, tokenPath, self.multiclasspath, numSamples)

    def tokenizeSlicesPerFile(self, filename, all_tokensPath, numSamples=420627, saveIndividualFiles=False):
        """  
            Initializes the dataset by turning the source code slices into tokens. 
            numSamples := int; Max Num of slice samples from each file
            saveIndividualFiles := boolean; When false, will only save to a single output file with all the vectors. When true, will also create individual folders & files per vector, which is more memory.
        """
        slicePath = os.path.join('data', 'slicesSource')
        if os.path.exists(self.tokensPath) and os.listdir(self.tokensPath):
            rmtree(self.tokensPath)
            os.mkdir(self.tokensPath)
        
        all_data = [ [], [], [], [], [], [], [] ]
        count = tokenizeSlicesPerFile(slicePath, filename, self.tokensPath, self.multiclasspath, numSamples, all_data, saveIndividualFiles)
        save_tokens_to_file(all_data, all_tokensPath, count)

    def downsample(self, datapath, downsample_num=1000):
        print('\nDOWNSAMPLING THE DATA...')
        all_data = getDataset(datapath, False, self.randomSeed)
        downsampled_data = downsample(all_data, 664, downsample_num)
        downsampled_data = downsample(downsampled_data, 710, downsample_num)
        downsampled_data = downsample(downsampled_data, 707, downsample_num)
        downsampled_data = downsample(downsampled_data, 682, downsample_num)

        print('SAVING BALANCED, DOWNSAMPLES DATA TO FILE...')
        save_data_to_file(self.vectorRootPath, 'balanced_vectors.pkl', downsampled_data)

    def get_dropped_classes(self, minSamples):
        """
            Drop classes that don't have more than minSamples (i.e. number of samples.
            399 & 189 are categories in other views, not obsolete.  Hard coding for now due to time crunch.
        """
        obsolete = [264, 17, 534, 16, 399, 189]
        # Get all classes
        cwes = get_unique_cwes(obsolete)
        # Get Classes > minSamples
        classes_gt = get_classes_gt(self.multiclasspath, minSamples)

        # Filter out classes that aren't in this list
        dropClasses = [c for c in cwes if c not in classes_gt]
        dropClasses = dropClasses + obsolete
        dropClasses = list(set(dropClasses))
        print(f'Dropclasses: ', dropClasses)
        return dropClasses

    def flatten_categories(self, input_data_path, filename, index, isBalanced=False):
        """ 
            Overwrites file with flattened categories. 
            Index := int; 0 = group label; 1 = individual label
        """
        all_data = getDataset(input_data_path, isBalanced, self.randomSeed)
        if type(all_data[-2][0]) != list:
            print(f'Already flattened!  data[-2][0] is of type: {type(all_data[-2][0])}')
            return
        all_data[-2] = flatten_categories(all_data[-2], index)
        save_data_to_file(input_data_path, filename, all_data)
    
    def flatten_vectors(self, inOutPath, pcaTransformVulType=None):
        """
            inOutPath  := string to input train or test path; overwrites the file
            pcaTransformVulType := string or None
        """
        # Flatten 3D vectors to 2D
        print('\nFlattening vectors to 2D')
        data = getDataset(inOutPath, getBalanced=True)
        assert len(data[0][0][0]) == 30, f'inputs data vector length {self.vector_size} != vector size 30'
        print(f'Avg: {self.avg}\tVector length: {self.vector_size}')

        # Reshape from 3D to 2D
        input_shape_1 = self.avg * self.vector_size
        x = convert_nested_lists_to_numpy_arrays(data[0], self.avg, self.vector_size)
        x = np.reshape(x, (x.shape[0], input_shape_1))
        print(f'New vector size will be: {self.avg}x{self.vector_size}={str(input_shape_1)}')
        data[0] = x
        filename = os.listdir(inOutPath)[0]
        save_data_to_file(inOutPath, filename, data)

        if pcaTransformVulType:
            # Get best n for PCA transformation 
            self.best_n = self.best_n if self.best_n else test_and_get_n_for_PCA(x, sum_of_variance_threshold=0.92)

            # Save PCA transformed data
            data[0] = fit_transform_PCA(x, self.best_n)
            PCAPath = os.path.join(os.path.dirname(inOutPath), f'{os.path.basename(inOutPath)}_PCA')
            save_data_to_file(PCAPath, filename, data)
            if 'train' in inOutPath:
                self.inputsTrainPath = PCAPath
            else:
                self.inputsTestPath = PCAPath
            print(f'PCA TRANSFORMATION SAVED TO {PCAPath}.')

    def init_transformer(self):
        print(f'Fitting transformer with paramaters: Seed: {self.randomSeed}, Alpha: {self.alpha}, Epochs: {self.t_epochs}, Sample: {self.sample}, Negative: {self.negative}, Window: {self.window}, Vector length: {self.vector_size}.')
        self.transformer = self.transformer_model(vector_size=self.vector_size, alpha=self.alpha, negative=self.negative, sample=self.sample, epochs=self.t_epochs, seed=self.randomSeed, window=self.window, workers=self.workers)

    def fit_transform(self, tranformer_path, tokens_path, vectorALLPath, getBalanced=False, index=1):
        """
            Fit on all the data & transform to vectors
            index = 0 = group ids
            index = 1 = original CWE ID
            Example: [[664, 664], [703, 555], ...] --> [664, 703, ...]
        """
        data = getDataset(tokens_path, getBalanced)
        X = data[0]
        self.transformer.fit(X)
        print(f'Transformer fit complete. Transforming tokens to vectors next...')
        self.transformer.save_model(tranformer_path)
        X = self.transformer.transform(X)
        data[0] = X
        print(f'Tokens to vectors complete.  Saving to {vectorALLPath} as ALL_vectors.pkl')
        save_data_to_file(vectorALLPath, 'ALL_vectors.pkl', data)
        self.flatten_categories(vectorALLPath, 'ALL_vectors.pkl', index, isBalanced=False)
        
    def splitTrainTest(self, vectorALLPath, minSamples=None):
        # Filter out classes that aren't in our final target classes
        dropClasses = self.get_dropped_classes(minSamples) if minSamples is not None else []
        # Split into train/test
        splitTrainTestCategorical('balanced', vectorALLPath, self.vectorTrainPath, self.vectorTestPath, randomSeed=self.randomSeed, dropClasses=dropClasses)

    def get_avg_length(self):
        self.avg = meanLen(self.vectorTrainPath)
        print_num_samples(self.vectorTrainPath, True)

    def get_full_length(self):
        data = getDataset(self.vectorTrainPath, True)
        self.avg = len(data[0])
        print(f'Total number of samples in datapath set: {len(data[-2])}')

    def adjustVectorLength(self):
        """
            Outputs results to input paths
        """
        self.get_avg_length()
        tranformDimsByFocus(self.vectorTrainPath, self.vectorTestPath, self.inputsTrainPath, self.inputsTestPath, self.avg, self.vector_size)
        print(f'New Vector Length (rows x cols): {self.avg} x {self.vector_size}\n')

    def tranformDimsByFocus(self):
        tranformDimsByFocus(self.vectorTrainPath, self.vectorTestPath, self.inputsTrainPath, self.inputsTestPath, self.avg, self.vector_size)
        print(f'New Vector Length (rows x cols): {self.avg} x {self.vector_size}\n')

    def encodeLabels(self):
        """ Get number of unique classes for density value. """
        print('\nEncoding labels...')
        train_data = getDataset(self.vectorTrainPath, getBalanced=True)
        categories = np.unique(train_data[-2])
        self.labelEncoder = encode_target(categories)[1]
        self.density_units = categories.shape[0]

    def saveKeyData(self, fromPath, output_path=None):
        saveKeyDataMulticlass(fromPath, self.labelEncoder, output_path)

    def build_and_fit(self):
        """ builds & fits the estimator """
        print(f'BUILDING MODEL {self.modelName}')
        # Prepare a directory to store all the checkpoints.
        if not os.path.exists(self.checkpoint_dir):
            os.makedirs(self.checkpoint_dir)

        # Reduces memory usage by restricting TF to use only 2 GPUs
        gpus = tf.config.experimental.list_physical_devices('GPU')
        for gpu in gpus:
            tf.config.experimental.set_memory_growth(gpu, True)

        # Open a strategy scope
        strategy = tf.distribute.MirroredStrategy(['GPU:0', 'GPU:1'])
        print('\nNumber of devices: {}'.format(strategy.num_replicas_in_sync))
        with strategy.scope():
            latest_epoch, myKerasModel = make_or_restore_model(self.checkpoint_dir, self.get_compiled_model)


        # Fit BGRU Model with trained data and save the model for later use
        # This callback saves a SavedModel every epoch
        # We include the current epoch in the folder name.
        checkpointCallback = [
            keras.callbacks.ModelCheckpoint(
                filepath=os.path.join(self.checkpoint_dir, 'ckpt-{epoch}'),
                monitor='val_acc',
                verbose=1,
                save_best_only=False, # default
                save_weights_only=False, # default
                mode='auto', # default
                save_freq='epoch'
            )
        ]
        self.model = fit_custom_dl_model(myKerasModel, self.weightpath, self.inputsTrainPath, self.batch_size, self.avg, self.vector_size, self.randomSeed, self.m_epochs, useGenerator=self.useGenerator, callbacks=checkpointCallback, latest_epoch=latest_epoch)

    def get_compiled_model(self):
        return self.build_model(self.avg, self.vector_size, self.layers, self.dropout, self.optimizer, self.density_units, activation_fn=self.activation_fn, metrics=self.metrics, mask=self.mask)
    
    def load_weights(self):
        try:
            self.model.load_weights(self.weightpath)
        except Exception as e:
            raise(f'Could not load weight path {self.weightpath}.\n{e}')

    def predict_and_score(self):
        self.load_weights()
        _, y_pred, y_true, _ = predictMulticlassLabel(self.model, self.inputsTestPath, self.avg, self.vector_size, self.optimizer, self.modelName, self.randomSeed, self.labelEncoder, saveOutput=True)
        self.print_label_differences(y_true, y_pred)

        # Confusion Matrix
        getConfusionMatrix_Multiclass(y_pred, y_true, saveFig=False, path=self.metricsPath, modelName=self.modelName)

        # ROC
        roc_auc_dict = roc_auc_score_multiclass(y_true, y_pred)
        print(f'\n{self.modelName} ROC AUC Score\n{roc_auc_dict}\n')

    def print_label_differences(self, y_true, y_pred):
        all_classes = set(self.labelEncoder.classes_)
        true_classes = set(y_true)
        pred_classes = set(y_pred)
        
        print(f'\nLength of all classes: {len(all_classes)}')
        print(f'Length of true test classes: {len(true_classes)}.  Classes missing from true test classes: {all_classes - true_classes}')
        print(f'Length of predicted test classes: {len(pred_classes)}.  Classes missing from predicted test classes: {all_classes - pred_classes}')
        print(f'Classes in true labels that are not in predicted labels: {true_classes - pred_classes}')
        print(f'Classes in predicted labels that are not in true labels: {pred_classes - true_classes}')

    def reset_checkpoint(self):
        """Resets saved DL model"""
        if os.path.exists(self.checkpoint_dir):
            rmtree(self.checkpoint_dir)

    def reset_model(self, filename):
        """Resets saved DL model's weights"""
        for f in os.listdir('model'):
            if filename in f:
                fpath = os.path.join('model', f)
                os.remove(fpath)

    def reset_checkpoint_and_weights(self, filename):
        """Resets saved DL model and their weights"""
        self.reset_checkpoint()
        self.reset_model(filename)
    
    def reset_all_models(self):
        """
            Removes all the model weights and current checkpoint. 
            Model weights can be restored from other checkpoints
            that have not been deleted.
        """
        self.reset_checkpoint()
        if len(os.listdir('model')) > 0:
            rmtree('model')
            os.makedirs('model')

    def run_all_w2v(self, minSamples, numSamples=420627, index=1):
        """
            Resets everything and initializes all data, W2V transformers, and models.

            numSamples := int; number of samples to extract from slices
            minSamples := int or None; if None, no classes are dropped. If int, drops classes that don't have > minSamples number of samples.  Example values: 900.
            index := int; 1 is for granular classes; 0 is for group classes
        """
        # Delete saved DL models so we don't load in old ones
        self.reset_checkpoint_and_weights(self.weightpath)

        # Initializes the dataset
        self.tokenizeSlices(numSamples)

        # Init transformer, fit, and split train test sets
        # adjustVectors = True b/c W2V transformed data
        self.run_starting_from_transformer(tokens_path=self.tokensPath, vectorALLPath=self.vectorsALLPath, minSamples=minSamples, getBalanced=False, adjustVectors=True, index=index)
        
        # Hot Encode labels. Build, fit, & predict.
        self.run_starting_from_saveKeyData(self.inputsTrainPath, self.inputsTestPath)

    def run_starting_from_transformer(self, tokens_path, vectorALLPath, minSamples, getBalanced=False, adjustVectors=True, index=1):
        self.init_transformer()
        # Fit transformer & transform our data
        self.fit_transform(self.transformerPath, tokens_path, vectorALLPath, getBalanced, index=index)
        # Split data into training and test set
        self.splitTrainTest(vectorALLPath, minSamples)
        # Average out the row length per sample based on focuspointer
        if adjustVectors:
            self.adjustVectorLength()

    def run_starting_from_saveKeyData(self, trainPath, testPath, inputTrain=None, inputTest=None):
        # Hot encode labels
        self.encodeLabels()
        self.saveKeyData(trainPath, inputTrain)
        self.saveKeyData(testPath, inputTest)
        # Build & fit the model
        self.build_and_fit()
        # Predict and score on test set
        self.predict_and_score()

    def run_starting_from_model(self):
        self.encodeLabels()
        self.build_and_fit()
        self.predict_and_score()



