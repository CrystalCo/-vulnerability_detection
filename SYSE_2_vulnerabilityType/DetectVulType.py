import os, sys
from shutil import rmtree

VUL_PATH = os.environ.get('VUL_PATH')
sys.path.insert(1, VUL_PATH)

import keras
import numpy as np
import tensorflow as tf

from SYSE_1_isVulnerable.slicesToTokens import tokenizeSlices_Multiclass
from SYSE_1_isVulnerable.splitTrainTest import splitTrainTestCategorical
from SYSE_1_isVulnerable.adjustVectorLen import meanLen
from SYSE_1_isVulnerable.saveKeyData import saveKeyDataMulticlass
from utils.DLCustomModels import create_bgru_model, create_blstm_model, fit_custom_dl_model, predictMulticlassLabel
from SYSE_1_isVulnerable.ConfusionMatrix import getConfusionMatrix_Multiclass
from SYSE_1_isVulnerable.evaluateModels import roc_auc_score_multiclass
from utils.Doc2VecModel import Doc2VecModel
from utils.Word2VecModel import Word2VecModel
from utils.DLCustomModels import make_or_restore_model
from utils.transformDataDimensions import tranformDimsByFocus
from utils.utils import downsample, flatten_categories, getDataset, encode_target, save_data_to_file


class DetectVulType():
    
    def __init__(self, build_model, useGenerator=True, transformer_model=None, transformerPath='',  metricsPath='', vector_size=30, randomSeed=1099, alpha=0.05, t_epochs=15, sample=0.001, workers=-1, negative=10, window=3, m_epochs=10, modelName='BGRU_ALL', batch_size=32, mask=True, dropout=0.2, layers=2, optimizer='adam', metrics=['CategoricalAccuracy', 'Recall'], tokensPath='', vectorsALLPath='', vectorTypePath='', vectorTrainPath='', vectorTestPath='', inputsTrainPath='', inputsTestPath='',checkpoint_dir='./ckpt'):
        # Transformer variables
        self.transformer_model = transformer_model
        self.transformer = None
        self.avg = 0
        self.randomSeed = randomSeed
        self.tokensPath = tokensPath 
        self.vector_size = vector_size
        self.alpha = alpha
        self.t_epochs = t_epochs
        self.sample = sample
        self.workers = workers
        self.negative = negative
        self.window = window
        # Estimator Variables
        self.model = None
        self.build_model = build_model
        self.modelName = modelName
        self.useGenerator = useGenerator
        self.optimizer = optimizer #can be changed to ‘adamax’
        self.layers = layers
        self.dropout = dropout
        self.mask = mask # Set to False if using D2V
        self.batch_size = batch_size
        self.m_epochs = m_epochs
        self.activation_fn = 'softmax'
        self.metrics = metrics
        # Paths
        self.metricsPath = metricsPath
        self.weightpath = os.path.join('model', modelName + self.optimizer + str(self.randomSeed))
        self.vectorTypePath = vectorTypePath
        self.vectorsALLPath = vectorsALLPath
        self.vectorTrainPath = vectorTrainPath
        self.vectorTestPath = vectorTestPath
        self.inputsTrainPath = inputsTrainPath
        self.inputsTestPath = inputsTestPath
        self.transformerPath = transformerPath
        self.checkpoint_dir = checkpoint_dir
        
    def tokenizeSlices(self, numSamples=420627):
        """  
            Initializes the dataset by turning the source code slices into tokens. 
            numSamples := int; Max Num of slice samples from each file
        """
        slicePath = os.path.join('data', 'slicesSource')
        tokenPath = os.path.join('data', 'token', 'SARD')
        multiclasspath = os.path.join('data', 'CVE', 'SARD_CVE_to_CWE.csv')
        if os.path.exists(tokenPath) and os.listdir(tokenPath):
            rmtree(tokenPath)
            os.mkdir(tokenPath)
        tokenizeSlices_Multiclass(slicePath, tokenPath, multiclasspath, numSamples)

    def downsample(self, datapath, downsample_num=1000):        
        print('\nDOWNSAMPLING THE DATA...')
        all_data = getDataset(datapath, False, self.randomSeed)
        downsampled_data = downsample(all_data, 664, downsample_num)
        downsampled_data = downsample(downsampled_data, 710, downsample_num)
        downsampled_data = downsample(downsampled_data, 707, downsample_num)
        downsampled_data = downsample(downsampled_data, 682, downsample_num)

        print('SAVING BALANCED, DOWNSAMPLES DATA TO FILE...')
        save_data_to_file(self.vectorTypePath, 'balanced_vectors.pkl', downsampled_data)

    def flatten_categories(self, dirname, filename, isBalanced=False):
        """
            Since we are not merging any classes, we'll replace the group id section with the flattened version containing the original label
            Flattens groupid array for all samples into the assigned groupids only.
            Example: [[664, 664], [703, 555]] --> [664, 703]
        """
        all_data = getDataset(dirname, isBalanced, self.randomSeed)
        all_data[-2] = flatten_categories(all_data[-2])
        save_data_to_file(dirname, filename, all_data)

    def init_transformer(self):
        print(f'Fitting transformer with paramaters: Seed: {self.randomSeed}, Alpha: {self.alpha}, Epochs: {self.t_epochs}, Sample: {self.sample}, Negative: {self.negative}, Window: {self.window}, Vector length: {self.vector_size}.')
        self.transformer = self.transformer_model(vector_size=self.vector_size, alpha=self.alpha, negative=self.negative, sample=self.sample, epochs=self.t_epochs, seed=self.randomSeed, window=self.window, workers=self.workers)

    def fit_transform(self, modelPath, dataset_path, vectorTransformedPath, getBalanced=False):
        # Fit on all the data & transform to vectors
        data = getDataset(dataset_path, getBalanced)
        X = data[0]
        self.transformer.fit(X)
        print(f'Transformer fit complete. Transforming tokens to vectors next...')
        self.transformer.save_model(modelPath)
        X = self.transformer.transform(X)
        data[0] = X
        print(f'Tokens to vectors complete.  Saving to {vectorTransformedPath} as ALL_vectors.pkl')
        save_data_to_file(vectorTransformedPath, 'ALL_vectors.pkl', data)

    def fit_transpose_transform(self, modelPath, dataset_path, vectorTransformedPath, getBalanced=False):
        # Fit on all the data & transform to vectors
        data = getDataset(dataset_path, getBalanced)
        X = data[0]
        self.transformer.fit(X)
        print(f'Transformer fit complete. Transforming tokens to vectors next...')
        self.transformer.save_model(modelPath)
        X = self.transformer.transform(X)
        X_t = np.reshape(X, (len(X), len(X[0]), 1))
        data[0] = X_t
        print(f'Tokens to vectors complete.  Saving to {vectorTransformedPath} as ALL_vectors.pkl')
        save_data_to_file(vectorTransformedPath, 'ALL_vectors.pkl', data)
        
    def splitTrainTest(self, vectorTransformedPath):
        # Split into train/test
        splitTrainTestCategorical('balanced', vectorTransformedPath, self.vectorTrainPath, self.vectorTestPath, randomSeed=self.randomSeed)

    def get_avg_length(self):
        self.avg = meanLen(self.vectorTrainPath)

    def get_full_length(self):
        data = getDataset(self.vectorTrainPath, True)
        self.avg = len(data[0])

    def adjustVectorLength(self):
        # Adjust vector length TODO: Why do we do the avg instead of all the lines? Will our models perform worse if too many zero-padded vectors?
        self.get_avg_length()
        tranformDimsByFocus(self.vectorTrainPath, self.vectorTestPath, self.inputsTrainPath, self.inputsTestPath, self.avg, self.vector_size)
        print(f'New Vector Length (rows x cols): {self.avg} x {self.vector_size}\n')

    def encodeLabels(self):
        """ Get number of unique classes for density value. """
        train_data = getDataset(self.vectorTrainPath, getBalanced=True)
        categories = np.unique(train_data[-2])
        self.labelEncoder = encode_target(categories)[1]
        self.density_units = categories.shape[0]

    def saveKeyData(self, fromPath, output_path=None):
        saveKeyDataMulticlass(fromPath, self.labelEncoder, output_path)

    def build_and_fit(self):
        """ builds & fits the estimator """
        # Prepare a directory to store all the checkpoints.
        if not os.path.exists(self.checkpoint_dir):
            os.makedirs(self.checkpoint_dir)

        # Reduces memory usage by restricting TF to allocate only a fraction of GPU memory 
        gpu_options = tf.compat.v1.GPUOptions(per_process_gpu_memory_fraction=0.60)
        keras.backend.set_session(tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(gpu_options=gpu_options)))

        # Open a strategy scope
        strategy = tf.distribute.MirroredStrategy()
        print('\nNumber of devices: {}'.format(strategy.num_replicas_in_sync))
        with strategy.scope():
            didRestore, latest_epoch, myKerasModel = make_or_restore_model(self.checkpoint_dir, self.get_compiled_model)

        if didRestore:
            self.model = myKerasModel
            self.latest_epoch = latest_epoch
        else:
            # Fit BGRU Model with trained data and save the model for later use
            callbacks = [
                # This callback saves a SavedModel every epoch
                # We include the current epoch in the folder name.
                keras.callbacks.ModelCheckpoint(
                    filepath=self.checkpoint_dir + "/ckpt-{epoch}", save_freq="epoch"
                    )
            ]
            self.model = fit_custom_dl_model(myKerasModel, self.weightpath, self.inputsTrainPath, self.batch_size, self.avg, self.vector_size, self.randomSeed, self.m_epochs, useGenerator=self.useGenerator, callbacks=callbacks)

    def get_compiled_model(self):
        return self.build_model(self.avg, self.vector_size, self.layers, self.dropout, self.optimizer, self.density_units, activation_fn=self.activation_fn, metrics=self.metrics, mask=self.mask)
        
    def fit_from_checkpoint(self, latest_epoch):
        filepath = self.checkpoint_dir + "/ckpt-{epoch}",
        callbacks = [
            # This callback saves a SavedModel every epoch
            # We include the current epoch in the folder name.
            keras.callbacks.ModelCheckpoint(
                filepath=filepath, save_freq="epoch"
                )
        ]
        newModel = fit_custom_dl_model(self.model, self.weightpath, self.inputsTrainPath, self.batch_size, self.avg, self.vector_size, self.randomSeed, self.m_epochs, useGenerator=self.useGenerator, callbacks=callbacks, latest_epoch=latest_epoch)
        self.model = newModel
    
    def load_weights(self):
        try:
            self.model.load_weights(self.weightpath)
        except Exception as e:
            print(f'Could not find weight path.  Will look for it in checkpoints. {e}')
            self.model.load_weights(self.checkpoint_dir)

    def predict_and_score(self):
        # Prediction
        try:
            self.load_weights()
        except Exception as e:
            print(e)
            pass
        _, mypredicted_labels, myreallabels, _ = predictMulticlassLabel(self.model, self.inputsTestPath, self.avg, self.vector_size, self.optimizer, self.modelName, self.randomSeed, self.labelEncoder, saveOutput=True)

        # Confusion Matrix
        getConfusionMatrix_Multiclass(mypredicted_labels, myreallabels, saveFig=False, path=self.metricsPath, modelName=self.modelName)

        # ROC
        roc_auc_dict = roc_auc_score_multiclass(myreallabels, mypredicted_labels)
        print(f'\n{self.modelName} ROC AUC Score\n{roc_auc_dict}\n')

    def reset_checkpoint(self):
        """Resets saved DL model"""
        if os.path.exists(self.checkpoint_dir):
            rmtree(self.checkpoint_dir)

    def reset_model(self, filename):
        """Resets saved DL model's weights"""
        for f in os.listdir('model'):
            if filename in f:
                fpath = os.path.join('model', f)
                os.remove(fpath)

    def reset(self, filename):
        """Resets saved DL model and their weights"""
        self.reset_checkpoint()
        self.reset_model(filename)
    
    def reset_all_models(self):
        self.reset_checkpoint()
        if len(os.listdir('model')) > 0:
            rmtree('model')
            os.makedirs('model')

    def run_all(self, numSamples=1000):
        """  Runs all the functions in order for initial run. """
        # Delete saved DL models so we don't load in old ones
        self.reset_all_models()

        # Initializes the dataset
        self.tokenizeSlices(numSamples=numSamples) 
        self.flatten_categories(self.tokensPath, 'ALL_tokens.pkl')

        # Init transformer
        self.init_transformer()

        # Fit transformer & transform our data
        self.fit_transform(self.transformerPath, dataset_path=self.tokensPath, vectorTransformedPath=self.vectorsALLPath, getBalanced=False)

        # Split data into training and test set
        self.splitTrainTest(self.vectorsALLPath)

        # Average out the row length per sample based on focuspointer
        self.adjustVectorLength()
        
        # Hot encode labels
        self.encodeLabels()
        self.saveKeyData(self.inputsTrainPath)
        self.saveKeyData(self.inputsTestPath)

        # Build & fit the model
        self.build_and_fit()

        # Predict and score on test set
        self.predict_and_score()

    def run_starting_from_transformer(self, datapath, vectorTransformedPath, getBalanced=False, adjustVectors=True):
        self.init_transformer()
        self.fit_transform(self.transformerPath, datapath, vectorTransformedPath, getBalanced)
        self.splitTrainTest(vectorTransformedPath)
        if adjustVectors:
            self.adjustVectorLength()

    def run_starting_from_saveKeyData(self, trainPath, testPath, inputTrain=None, inputTest=None):
        self.encodeLabels()
        self.saveKeyData(trainPath, inputTrain)
        self.saveKeyData(testPath, inputTest)
        self.build_and_fit()
        self.predict_and_score()

    def run_starting_from_model(self):
        self.encodeLabels()
        self.build_and_fit()
        self.predict_and_score()


tokensPath = os.path.join('data', 'tokens') # Contains ALL_tokens.pkl
metrics = ['CategoricalAccuracy', 'Recall', 'TruePositives', 'TrueNegatives', 'FalsePositives', 'FalseNegatives']

############# W2V + BGRU #############
DLVectors = os.path.join('data','DLvectors')
vectorsALLPath = os.path.join(DLVectors, 'ALL_vectors')
vectorTrainPath = os.path.join(DLVectors,'train')
vectorTestPath = os.path.join(DLVectors,'test')
dlInputsTrainPath = os.path.join('data','DLinputs','train')
dlInputsTestPath  = os.path.join('data','DLinputs','test')
randomSeed = 1099
batch_size = 64
dl_epochs = 20

w2vmetricPath = os.path.join('w2vModel','metrics', 'bgru')
w2vmodelPath = os.path.join('w2vModel','model','w2vModel_ALL')
modelName = 'BGRU_W2V_batch=%s_seed=%s' % (batch_size, randomSeed)
print(f'\n\n\nBUILDING MODEL {modelName}...')

dvt = DetectVulType(build_model=create_bgru_model, transformer_model=Word2VecModel, 
                    transformerPath=w2vmodelPath, metricsPath=w2vmetricPath, vector_size=30, 
                    randomSeed=randomSeed, alpha=0.05, t_epochs=15, sample=0.001, workers=-1, 
                    negative=10, window=3, batch_size=batch_size, m_epochs=dl_epochs, modelName=modelName, 
                    metrics=metrics, tokensPath=tokensPath, vectorsALLPath=vectorsALLPath, 
                    vectorTypePath=DLVectors, vectorTrainPath=vectorTrainPath, 
                    vectorTestPath=vectorTestPath, inputsTrainPath=dlInputsTrainPath, 
                    inputsTestPath=dlInputsTestPath, checkpoint_dir='./ckpt_w2v_bgru')
# dvt.reset_all_models()
# dvt.get_avg_length()
# dvt.modelName = modelName + str(dvt.avg) + 'x' + str(dvt.vector_size)
# dvt.run_starting_from_model()
dvt.run_all(420627)




############# W2V + BLSTM #############
w2vmetricPath = os.path.join('w2vModel','metrics', 'blstm')
modelName = 'BLSTM_W2V_batch=%s_' % batch_size
print(f'RUNNING {modelName}...')

dvt = DetectVulType(build_model=create_blstm_model, transformer_model=Word2VecModel,
                    metricsPath=w2vmetricPath, vector_size=30, randomSeed=randomSeed, 
                    m_epochs=dl_epochs, modelName=modelName, batch_size=batch_size, metrics=metrics, 
                    vectorsALLPath=vectorsALLPath, vectorTypePath=DLVectors, 
                    vectorTrainPath=vectorTrainPath, vectorTestPath=vectorTestPath, 
                    inputsTrainPath=dlInputsTrainPath, inputsTestPath=dlInputsTestPath,
                    checkpoint_dir='./ckpt_w2v_blstm')
dvt.reset_checkpoint()
dvt.get_avg_length()
dvt.modelName = modelName + str(dvt.avg) + 'x' + str(dvt.vector_size)
dvt.run_starting_from_model()


############# D2V + BGRU #############
D2Vectors = os.path.join('data','D2Vectors')
vectorsALLPath = os.path.join(D2Vectors, 'ALL_vectors')
vectorTrainPath = os.path.join(D2Vectors,'train')
vectorTestPath = os.path.join(D2Vectors,'test')
d2InputsTrainPath = os.path.join('data','D2inputs','train')
d2InputsTestPath  = os.path.join('data','D2inputs','test')
d2vmetricPath = os.path.join('d2vModel','metrics', 'bgru')
d2vmodelPath = os.path.join('d2vModel','model','d2vModel_ALL')

def d2v_bgru(avg, vectorSize, reshapeX=False):
    modelName = 'BGRU_D2V_' + str(avg) + 'x' + str(vectorSize)
    print(f'\n\n\nBUILDING MODEL {modelName}...')
    dvt = DetectVulType(build_model=create_bgru_model, useGenerator=True, 
                        transformer_model=Doc2VecModel, transformerPath=d2vmodelPath, 
                        metricsPath=d2vmetricPath, vector_size=256, randomSeed=randomSeed, 
                        alpha=0.05, t_epochs=15, sample=0.001, workers=-1, negative=10, 
                        window=3, m_epochs=dl_epochs, batch_size=batch_size, modelName=modelName, 
                        mask=False, metrics=metrics, tokensPath=tokensPath, 
                        vectorsALLPath=vectorsALLPath, vectorTypePath=D2Vectors, 
                        vectorTrainPath=vectorTrainPath, vectorTestPath=vectorTestPath, 
                        inputsTrainPath=d2InputsTrainPath, inputsTestPath=d2InputsTestPath, 
                        checkpoint_dir='./ckpt_d2v_bgru')
    dvt.reset_checkpoint()
    dvt.init_transformer()
    if reshapeX:
        dvt.fit_transpose_transform(dvt.transformerPath, tokensPath, dvt.vectorsALLPath, False)
    else:
        dvt.fit_transform(dvt.transformerPath, tokensPath, dvt.vectorsALLPath, False)
    dvt.splitTrainTest(dvt.vectorsALLPath)
    dvt.encodeLabels()
    dvt.saveKeyData(dvt.vectorTrainPath, dvt.inputsTrainPath)
    dvt.saveKeyData(dvt.vectorTestPath, dvt.inputsTestPath)
    dvt.avg = avg
    dvt.vector_size = vectorSize
    dvt.run_starting_from_model()

try:
    d2v_bgru(16, 16)
except Exception as e:
    print(f'ERROR!! {e}\n\n\n')

try:
    d2v_bgru(1, 256)
except Exception as e:
    print(f'ERROR!! {e}\n\n\n')

try:
    d2v_bgru(256, 1, reshapeX=True)
except Exception as e:
    print(f'ERROR!! {e}\n\n\n')




############# D2V + BLSTM #############
d2vmetricPath = os.path.join('d2vModel','metrics', 'blstm')

def d2v_blstm(avg, vectorSize, reshapeX=False):
    modelName = 'BLSTM_D2V_' + str(avg) + 'x' + str(vectorSize)
    print(f'\n\n\nBUILDING MODEL {modelName}...')
    dvt = DetectVulType(build_model=create_blstm_model, useGenerator=True, 
                        transformer_model=Doc2VecModel, transformerPath=d2vmodelPath, 
                        metricsPath=d2vmetricPath, vector_size=256, randomSeed=randomSeed, 
                        alpha=0.05, t_epochs=15, sample=0.001, workers=-1, negative=10, 
                        window=3, m_epochs=dl_epochs, batch_size=batch_size, modelName=modelName, 
                        mask=False, metrics=metrics, tokensPath=tokensPath, 
                        vectorsALLPath=vectorsALLPath, vectorTypePath=D2Vectors, 
                        vectorTrainPath=vectorTrainPath, vectorTestPath=vectorTestPath, 
                        inputsTrainPath=d2InputsTrainPath, inputsTestPath=d2InputsTestPath, 
                        checkpoint_dir='./ckpt_d2v_bgru')
    dvt.reset_checkpoint()
    dvt.init_transformer()
    if reshapeX:
        dvt.fit_transpose_transform(dvt.transformerPath, tokensPath, dvt.vectorsALLPath, False)
    else:
        dvt.fit_transform(dvt.transformerPath, tokensPath, dvt.vectorsALLPath, False)
    dvt.splitTrainTest(dvt.vectorsALLPath)
    dvt.encodeLabels()
    dvt.saveKeyData(dvt.vectorTrainPath, dvt.inputsTrainPath)
    dvt.saveKeyData(dvt.vectorTestPath, dvt.inputsTestPath)
    dvt.avg = avg
    dvt.vector_size = vectorSize
    dvt.run_starting_from_model()

try:
    d2v_blstm(16, 16)
except Exception as e:
    print(f'ERROR!! {e}\n\n\n')


try:
    d2v_blstm(1, 256)
except Exception as e:
    print(f'ERROR!! {e}\n\n\n')


try:
    d2v_blstm(256, 1, reshapeX=True)
except Exception as e:
    print(f'ERROR!! {e}\n\n\n')

