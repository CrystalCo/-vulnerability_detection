import os, sys
from shutil import rmtree

import keras
import numpy as np
import tensorflow as tf
from sklearn.preprocessing import LabelEncoder

VUL_PATH = os.environ.get('VUL_PATH')
sys.path.insert(1, VUL_PATH)

from utils.slicesToTokensMulticlass import save_tokens_to_file, tokenizeSlices_Multiclass, tokenizeSlicesPerFile
from SYSE_1_isVulnerable.adjustVectorLen import meanLen
from utils.saveKeyDataMulticlass import saveKeyDataMulticlass
from utils.DLCustomModels import decode_predictions, fit_custom_dl_model, fit_with_generator, fit_without_generator, predictMulticlassLabel, predictMulticlassLabelEncoderToHotEncoded
from utils.MLMethods import convert_nested_lists_to_numpy_arrays, fit_transform_PCA, test_and_get_n_for_PCA
from utils.MetricsEvaluationLib import getConfusionMatrix_Multiclass, roc_auc_score_multiclass
from utils.SplitTrainTestMulticlass import splitTrainTestCategorical
from utils.DLCustomModels import make_or_restore_model
from utils.transformDataDimensions import getAvgLength, tranformDimsByFocus, truncateRows
from utils.utils import getDataset, hot_encode_target, getDatasetXY, getDatasetSectionByIndex, print_num_samples, save_data_to_file, summarize_distribution


class DetectVulType():
   
    def __init__(self, build_model, useGenerator=True, transformer_model=None, transformerPath='',  metricsPath='', vector_size=30, randomSeed=1099, alpha=0.05, t_epochs=15, sample=0.001, workers=-1, negative=10, window=3, m_epochs=10, modelName='BGRU_ALL', batch_size=32, mask=True, dropout=0.2, layers=2, optimizer='adam', metrics=['CategoricalAccuracy', 'Recall'], tokensPath='', vectorsALLPath='', vectorRootPath='', vectorTrainPath='', vectorTestPath='', inputsTrainPath='', inputsTestPath='',checkpoint_dir='./ckpt', weightpath=''):
        # Transformer variables
        self.transformer_model = transformer_model
        self.transformer = None
        self.avg = 0
        self.randomSeed = randomSeed
        self.tokensPath = tokensPath 
        self.vector_size = vector_size
        self.alpha = alpha
        self.t_epochs = t_epochs
        self.sample = sample
        self.workers = workers
        self.negative = negative
        self.window = window
        # Estimator Variables
        self.model = None
        self.build_model = build_model
        self.modelName = modelName
        self.useGenerator = useGenerator
        self.optimizer = optimizer #can be changed to ‘adamax’
        self.layers = layers
        self.dropout = dropout
        self.mask = mask # Set to False if using D2V
        self.batch_size = batch_size
        self.m_epochs = m_epochs
        self.activation_fn = 'softmax'
        self.metrics = metrics
        # Paths
        self.multiclasspath = os.path.join('data', 'CVE', 'SARD_CVE_to_CWE.csv')
        self.metricsPath = metricsPath
        self.weightpath = weightpath if weightpath != '' else os.path.join('model', modelName + self.optimizer + str(self.randomSeed))
        self.vectorRootPath = vectorRootPath # Path of the root directory for the vectorized data samples (e.g. data/DLvectors/)
        self.vectorsALLPath = vectorsALLPath # Path that contains a file with all the data samples 
        self.vectorTrainPath = vectorTrainPath # Path that contains the file with the training samples
        self.vectorTestPath = vectorTestPath # Path that contains the file with the test samples
        self.inputsTrainPath = inputsTrainPath # Path that contains the file with the encoded target for the training samples 
        self.inputsTestPath = inputsTestPath # Path that contains the file with the encoded target for the test samples 
        self.transformerPath = transformerPath
        self.checkpoint_dir = checkpoint_dir
        # PCA transformation
        self.best_n = None

    def tokenizeSlices(self, numSamples=420627):
        """  
            Initializes the dataset by turning the source code slices into tokens. 
            numSamples := int; Max Num of slice samples from each file
        """
        slicePath = os.path.join('data', 'slicesSource')
        tokenPath = os.path.join('data', 'token', 'SARD')
        if os.path.exists(tokenPath) and os.listdir(tokenPath):
            rmtree(tokenPath)
            os.mkdir(tokenPath)
        tokenizeSlices_Multiclass(slicePath, tokenPath, self.multiclasspath, numSamples)

    def tokenizeSlicesPerFile(self, filename, all_tokensPath, numSamples=420627, saveIndividualFiles=False):
        """  
            Initializes the dataset by turning the source code slices into tokens. 
            numSamples := int; Max Num of slice samples from each file
            saveIndividualFiles := boolean; When false, will only save to a single output file with all the vectors. When true, will also create individual folders & files per vector, which is more memory.
        """
        slicePath = os.path.join('data', 'slicesSource')
        if os.path.exists(self.tokensPath) and os.listdir(self.tokensPath):
            rmtree(self.tokensPath)
            os.mkdir(self.tokensPath)
        
        all_data = [ [], [], [], [], [], [], [] ]
        count = tokenizeSlicesPerFile(slicePath, filename, self.tokensPath, self.multiclasspath, numSamples, all_data, saveIndividualFiles)
        save_tokens_to_file(all_data, all_tokensPath, count)
    
    def flatten_vectors(self, inOutPath, pcaTransformVulType=None):
        """
            inOutPath  := string to input train or test path; overwrites the file
            pcaTransformVulType := string or None
        """
        # Flatten 3D vectors to 2D
        print('\nFlattening vectors to 2D')
        data = getDataset(inOutPath, getBalanced=True)
        assert len(data[0][0][0]) == 30, f'inputs data vector length {self.vector_size} != vector size 30'
        print(f'Avg: {self.avg}\tVector length: {self.vector_size}')

        # Reshape from 3D to 2D
        input_shape_1 = self.avg * self.vector_size
        x = convert_nested_lists_to_numpy_arrays(data[0], self.avg, self.vector_size)
        x = np.reshape(x, (x.shape[0], input_shape_1))
        print(f'New vector size will be: {self.avg}x{self.vector_size}={str(input_shape_1)}')
        data[0] = x
        filename = os.listdir(inOutPath)[0]
        save_data_to_file(inOutPath, filename, data)

        if pcaTransformVulType:
            # Get best n for PCA transformation 
            self.pca_transformation(inOutPath, True)

    def pca_transformation(self, inOutPath, isBalanced=False):
        data = getDataset(inOutPath, getBalanced=isBalanced)
        x = np.array(data[0])
        # Get best n for PCA transformation 
        self.best_n = self.best_n if self.best_n else test_and_get_n_for_PCA(x, sum_of_variance_threshold=0.92)

        # Save PCA transformed data
        data[0] = fit_transform_PCA(x, self.best_n)
        filename = os.listdir(inOutPath)[0]
        save_data_to_file(inOutPath, filename, data)

    def init_transformer(self):
        print(f'Fitting transformer with paramaters: Seed: {self.randomSeed}, Alpha: {self.alpha}, Epochs: {self.t_epochs}, Sample: {self.sample}, Negative: {self.negative}, Window: {self.window}, Vector length: {self.vector_size}.')
        self.transformer = self.transformer_model(vector_size=self.vector_size, alpha=self.alpha, negative=self.negative, sample=self.sample, epochs=self.t_epochs, seed=self.randomSeed, window=self.window, workers=self.workers)

    def fit_transform(self, tranformer_path, tokens_path, vectorALLPath, getBalanced=False, index=1):
        """
            Fit on all the data & transform to vectors
            index = 0 = group ids
            index = 1 = original CWE ID
            Example: [[664, 664], [703, 555], ...] --> [664, 703, ...]
        """
        data = getDataset(tokens_path, getBalanced)
        X = data[0]
        self.transformer.fit(X)
        print(f'Transformer fit complete. Transforming tokens to vectors next...')
        self.transformer.save_model(tranformer_path)
        X = self.transformer.transform(X)
        data[0] = X
        print(f'Tokens to vectors complete.  Saving to {vectorALLPath} as ALL_vectors.pkl')
        save_data_to_file(vectorALLPath, 'ALL_vectors.pkl', data)
        
    def splitTrainTest(self, vectorALLPath, min_num=None, vType='balanced', dropOtherVtypes=False):
        """
            Split the data into train/test by vulnerability syntax characteristic
            vectorALLPath   := str; input path containing the full dataset to be split
            min_num         := int; minimum number of samples a class should have. Classes that dont meet this threshold are dropped.
            vType           := str; if dropOtherVtypes is true and vType in [AE, ARR, API, or PTR], then drops classes outside the chosen vType. 
            dropOtherVtypes := bool; whether to only test on the vType passed in.
        """
        # Split into train/test
        splitTrainTestCategorical(vType, vectorALLPath, self.vectorTrainPath, self.vectorTestPath, randomSeed=self.randomSeed, min_num=min_num, dropNonVtypes=dropOtherVtypes)

    def get_avg_length(self):
        self.avg = meanLen(self.vectorTrainPath)
        print_num_samples(self.vectorTrainPath, True)

    def get_full_length(self):
        data = getDataset(self.vectorTrainPath, True)
        self.avg = len(data[0])
        print(f'Total number of samples in datapath set: {len(data[-2])}')

    def adjustVectorLength(self):
        """
            Outputs results to input paths
        """
        self.get_avg_length()
        tranformDimsByFocus(self.vectorTrainPath, self.vectorTestPath, self.inputsTrainPath, self.inputsTestPath, self.avg, self.vector_size)
        print(f'New Vector Length (rows x cols): {self.avg} x {self.vector_size}\n')

    def tranformDimsByFocus(self):
        tranformDimsByFocus(self.vectorTrainPath, self.vectorTestPath, self.inputsTrainPath, self.inputsTestPath, self.avg, self.vector_size)
        print(f'New Vector Length (rows x cols): {self.avg} x {self.vector_size}\n')

    def hotEncodeLabels(self):
        """ Encodes labels using LabelBinizer. """
        print('\nEncoding labels using LabelBinarizer...')
        train_data = getDataset(self.vectorTrainPath, getBalanced=True)
        categories = np.unique(train_data[-2])
        self.labelEncoder = hot_encode_target(categories)[1]
        self.density_units = categories.shape[0]
    
    def encodeLabelsByLabelEncoder(self):
        print('\nEncoding labels...')
        categories = getDatasetSectionByIndex(self.vectorTrainPath, getBalanced=True, index=-2)
        categories = np.unique(categories)
        # label encode the target variable
        self.labelEncoder = LabelEncoder()
        y = self.labelEncoder.fit_transform(categories)
        print(f'Encoded classes:\n{y}\n')
        mapping = {}
        for i in range(len(categories)):
            mapping[y[i]] = self.labelEncoder.classes_[i]
        self.density_units = categories.shape[0]
        return mapping

    def saveKeyData(self, fromPath, output_path=None):
        saveKeyDataMulticlass(fromPath, self.labelEncoder, output_path)

    def build_estimator(self):
        """ builds the estimator """
        print(f'BUILDING MODEL {self.modelName}')
        # Prepare a directory to store all the checkpoints.
        if not os.path.exists(self.checkpoint_dir):
            os.makedirs(self.checkpoint_dir)

        # Reduces memory usage by restricting TF to use only 2 GPUs
        gpus = tf.config.experimental.list_physical_devices('GPU')
        for gpu in gpus:
            tf.config.experimental.set_memory_growth(gpu, True)

        # Open a strategy scope
        strategy = tf.distribute.MirroredStrategy(['GPU:0', 'GPU:1'])
        print('\nNumber of devices: {}'.format(strategy.num_replicas_in_sync))
        with strategy.scope():
            latest_epoch, myKerasModel = make_or_restore_model(self.checkpoint_dir, self.get_compiled_model)


        # Fit BGRU Model with trained data and save the model for later use
        # This callback saves a SavedModel every epoch
        # We include the current epoch in the folder name.
        checkpointCallback = [
            keras.callbacks.ModelCheckpoint(
                filepath=os.path.join(self.checkpoint_dir, 'ckpt-{epoch}'),
                monitor='val_acc',
                verbose=1,
                save_best_only=False, # default
                save_weights_only=False, # default
                mode='auto', # default
                save_freq='epoch'
            )
        ]
        return myKerasModel, checkpointCallback, latest_epoch

    def smote(self, strategy='auto', mapping={}):
        """
            Upsampling using Synthetic Minority Oversampling Technique (SMOTE) on our training set.
            By default, classes will be upsampled to match the count of the class with the most samples.
        """
        from imblearn.over_sampling import SMOTE

        X, y = getDatasetXY(self.inputsTrainPath, True, self.randomSeed, y_index=-2)
        print('Before SMOTE upsampling: ')
        summarize_distribution(y, mapping)

        # transform the dataset
        oversample = SMOTE(sampling_strategy=strategy)
        X, y = oversample.fit_resample(X, y)

        print('After SMOTE upsampling: ')
        summarize_distribution(y, mapping)

        return X, y

    def fit_estimator_with_generator(self, X, y, myKerasModel, checkpointCallback, latest_epoch):
        assert(self.useGenerator == True)
        self.model = fit_with_generator(myKerasModel, self.weightpath, X, y, self.batch_size, self.avg, self.vector_size, self.m_epochs, checkpointCallback, latest_epoch, output_signature_shape_y=self.density_units)

    def fit_estimator_without_generator(self, X, y, myKerasModel, checkpointCallback):
        assert(not self.useGenerator)
        self.model = fit_without_generator(myKerasModel, self.weightpath, X, y, self.batch_size, self.avg, self.vector_size, self.m_epochs, checkpointCallback)

    def build_and_fit(self):
        """ builds & fits the estimator """
        myKerasModel, checkpointCallback, latest_epoch = self.build_estimator()

        self.model = fit_custom_dl_model(myKerasModel, self.weightpath, self.inputsTrainPath, self.batch_size, self.avg, self.vector_size, self.randomSeed, self.m_epochs, useGenerator=self.useGenerator, callbacks=checkpointCallback, latest_epoch=latest_epoch)

    def get_compiled_model(self):
        return self.build_model(self.avg, self.vector_size, self.layers, self.dropout, self.optimizer, self.density_units, activation_fn=self.activation_fn, metrics=self.metrics, mask=self.mask)
    
    def load_weights(self):
        try:
            self.model.load_weights(self.weightpath)
        except Exception as e:
            raise(f'Could not load weight path {self.weightpath}.\n{e}')

    def predict_and_score(self):
        print('\nPredicting & Scoring...')
        self.load_weights()
        y_true, predicted_labels, testids, output_y_pred, vtype_labels, outputs_dict = predictMulticlassLabel(self.model, self.inputsTestPath, self.avg, self.vector_size)
        _, y_pred, y_true, _ = decode_predictions(self.labelEncoder, y_true, predicted_labels, testids, output_y_pred, vtype_labels, self.modelName, outputs_dict)
        self.print_label_differences(y_true, y_pred)
        self.score_predictions(y_true, y_pred)
    
    def predict_using_encoders(self, binarizerEncoder):
        print('\nPredicting...')
        self.load_weights()
        X, y = getDatasetXY(self.inputsTestPath, True)

        # Hot encode our y for our model to work
        hot_encoded_y = binarizerEncoder.transform(y)

        # Predict our test set
        y_true, predicted_labels, output_y_pred, outputs_dict = predictMulticlassLabelEncoderToHotEncoded(self.model, X, hot_encoded_y, self.avg, self.vector_size)

        # Output our predictions & extra stats like our unique test IDs and vtype labels
        vtype_labels = getDatasetSectionByIndex(self.inputsTestPath, True, index=4)
        testids = getDatasetSectionByIndex(self.inputsTestPath, True, index=-1)
        _, y_pred, y_true, _ = decode_predictions(binarizerEncoder, y_true, predicted_labels, testids, output_y_pred, vtype_labels, self.modelName, outputs_dict)

        return y_true, y_pred

    def score_predictions(self, y_true, y_pred):
        print('Scoring predictions...')
        # Confusion Matrix
        getConfusionMatrix_Multiclass(y_pred, y_true, saveFig=False, path=self.metricsPath, modelName=self.modelName)

        # ROC
        roc_auc_dict = roc_auc_score_multiclass(y_true, y_pred)
        print(f'\n{self.modelName} ROC AUC Score\n{roc_auc_dict}\n')

    def print_label_differences(self, y_true, y_pred):
        all_classes = set(self.labelEncoder.classes_)
        true_classes = set(y_true)
        pred_classes = set(y_pred)
        
        print(f'\nLength of all classes: {len(all_classes)}')
        print(f'Length of true test classes: {len(true_classes)}.  Classes missing from true test classes: {all_classes - true_classes}')
        print(f'Length of predicted test classes: {len(pred_classes)}.  Classes missing from predicted test classes: {all_classes - pred_classes}')
        print(f'Classes in true labels that are not in predicted labels: {true_classes - pred_classes}')
        print(f'Classes in predicted labels that are not in true labels: {pred_classes - true_classes}')

    def reset_checkpoint(self):
        """Resets saved DL model"""
        if os.path.exists(self.checkpoint_dir):
            rmtree(self.checkpoint_dir)

    def reset_model(self, filename):
        """Resets saved DL model's weights"""
        for f in os.listdir('model'):
            if filename in f:
                fpath = os.path.join('model', f)
                os.remove(fpath)

    def reset_checkpoint_and_weights(self, filename):
        """Resets saved DL model and their weights"""
        self.reset_checkpoint()
        self.reset_model(filename)
    
    def reset_all_models(self):
        """
            Removes all the model weights and current checkpoint. 
            Model weights can be restored from other checkpoints
            that have not been deleted.
        """
        self.reset_checkpoint()
        if len(os.listdir('model')) > 0:
            rmtree('model')
            os.makedirs('model')

    def run_all_w2v(self, minSamples, numSamples=420627, index=1):
        """
            Resets everything and initializes all data, W2V transformers, and models.

            numSamples := int; number of samples to extract from slices
            minSamples := int or None; if None, no classes are dropped. If int, drops classes that don't have > minSamples number of samples.  Example values: 900.
            index := int; 1 is for granular classes; 0 is for group classes
        """
        # Delete saved DL models so we don't load in old ones
        self.reset_checkpoint_and_weights(self.weightpath)

        # Initializes the dataset
        self.tokenizeSlices(numSamples)

        # Init transformer, fit, and split train test sets
        # adjustVectors = True b/c W2V transformed data
        self.run_starting_from_transformer(tokens_path=self.tokensPath, vectorALLPath=self.vectorsALLPath, minSamples=minSamples, getBalanced=False, adjustVectors=True, index=index)
        
        # Hot Encode labels. Build, fit, & predict.
        self.run_starting_from_saveKeyData(self.inputsTrainPath, self.inputsTestPath)

    def run_starting_from_transformer(self, tokens_path, vectorALLPath, minSamples, getBalanced=False, adjustVectors=True, index=1):
        self.init_transformer()
        # Fit transformer & transform our data
        self.fit_transform(self.transformerPath, tokens_path, vectorALLPath, getBalanced, index=index)
        # Split data into training and test set
        self.splitTrainTest(vectorALLPath, minSamples)
        # Average out the row length per sample based on focuspointer
        if adjustVectors:
            self.adjustVectorLength()

    def run_starting_from_saveKeyData(self, trainPath, testPath, inputTrain=None, inputTest=None):
        # Hot encode labels
        self.hotEncodeLabels()
        self.saveKeyData(trainPath, inputTrain)
        self.saveKeyData(testPath, inputTest)
        # Build & fit the model
        self.build_and_fit()
        # Predict and score on test set
        self.predict_and_score()

    def run_starting_from_model(self):
        self.hotEncodeLabels()
        self.build_and_fit()
        self.predict_and_score()



