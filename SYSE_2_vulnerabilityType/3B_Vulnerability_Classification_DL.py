#!/usr/bin/env python
# coding: utf-8

# # Vulnerability Type Classification using DL
# Uses BGRU & BLSTM models to classify vulnerability type
# Categories contain non-vulnerable samples as well

# ### Setting variables
import os, sys
from shutil import rmtree

import keras
import numpy as np
import pandas as pd
import tensorflow as tf


VUL_PATH = os.environ.get('VUL_PATH')
sys.path.insert(1, VUL_PATH)

from SYSE_1_isVulnerable.slicesToTokens import tokenizeSlices_Multiclass
from SYSE_1_isVulnerable.splitTrainTest import splitTrainTestCategorical
from SYSE_1_isVulnerable.adjustVectorLen import meanLen, tranformVectorLen
from SYSE_1_isVulnerable.saveKeyData import saveKeyDataMulticlass
from SYSE_1_isVulnerable.ConfusionMatrix import getConfusionMatrix_Multiclass
from SYSE_1_isVulnerable.evaluateModels import roc_auc_score_multiclass
from SYSE_2_vulnerabilityType.DetectVulType import DetectVulType
from utils.Doc2VecModel import Doc2VecModel
from utils.Word2VecModel import Word2VecModel
from utils.DLCustomModels import make_or_restore_model
from utils.transformDataDimensions import tranformDimsByFocus
from utils.utils import downsample, flatten_categories, getDataset, encode_target, save_data_to_file
from utils.DLCustomModels import create_bgru_model, create_blstm_model, fit_custom_dl_model, predictMulticlassLabel

vType = "ALL"
randomSeed = 1099
numSamples = 420627 #Max Num of slice samples from each file
vectorDim = 30 #num of vector cols
multiclasspath = os.path.join('data','CVE','SARD_CVE_to_groups.csv')
slicePath = os.path.join('data','slicesSource')
tokenPath = os.path.join('data', 'token', 'SARD') 
tokensPath = os.path.join('data', 'tokens') # Contains ALL_tokens.pkl
metrics = ['CategoricalAccuracy', 'Recall', 'TruePositives', 'TrueNegatives', 'FalsePositives', 'FalseNegatives']
DLVectors = os.path.join('data','DLvectors')
vectorsALLPath = os.path.join(DLVectors, 'ALL_vectors')
vectorBalancedPath=os.path.join(DLVectors, 'ALL_balanced')
vectorTrainPath = os.path.join(DLVectors,'train')
vectorTestPath = os.path.join(DLVectors,'test')
dlInputsTrainPath = os.path.join('data','DLinputs','train')
dlInputsTestPath  = os.path.join('data','DLinputs','test')
w2vmetricPath = os.path.join('w2vModel','metrics', 'bgru')
w2vmodelPath = os.path.join('w2vModel','model','w2vModel_ALL')


# ### Slices To Tokens
# from SYSE_1_isVulnerable.slicesToTokens import tokenizeSlices_Multiclass
# testcase_ids, testcase_ids_per_group = tokenizeSlices_Multiclass(slicePath, tokenPath, multiclasspath, numSamples)


### Tokens To Vectors
# transformer = Word2VecModel(vector_size=vectorDim)
# data = getDataset(dataset_path=tokensPath, getBalanced=True)
# X = data[0]
# X = transformer.fit_transform(X) # takes ~1min 30sec
# data[0] = X
# save_data_to_file(vectorBalancedPath, 'balanced_vectors.pkl', data)

### Split into training/testing sets
# splitTrainTestCategorical('balanced', vectorBalancedPath, vectorTrainPath, vectorTestPath, randomSeed)

### Get input shapes, category encoders, and number of unique classes for density value
X_train = getDataset(vectorTrainPath, getBalanced=True)
original_labels = X_train[-2]
categories = np.unique(original_labels)
labelEncoder = encode_target(categories)
density_units = categories.shape[0]

# ### Adjust Vector Length - Ensure each program (aka slice) contains the same amount of rows (i.e. tokens) in the data matrix.
maxlen = avg = meanLen(vectorTrainPath)
# tranformVectorLen(vectorTrainPath, vectorTestPath, dlInputsTrainPath, dlInputsTestPath, avg, vectorDim)
# print(f'New Vector Length (rows x cols): {avg} x {vectorDim}\n')

### Replaces true labels with encoded labels in datafile from which our DL will train from
# saveKeyDataMulticlass(vectorTrainPath, labelEncoder, dlInputsTrainPath)
# saveKeyDataMulticlass(vectorTestPath, labelEncoder, dlInputsTestPath)



                        ### Network Architecture
#################################################################################
                        ### BASE MODELS
#################################################################################
                        ### BGRU


print("\n\n===============================NEW BASE MODELS===============================\n\n")

# Initiate keras model checkpoint
checkpoint_dir = './ckpt_w2v_bgru'
if not os.path.exists(checkpoint_dir):
    os.makedirs(checkpoint_dir)
# Reset keras model checkpoint
# if os.path.exists(checkpoint_dir):
#     rmtree(checkpoint_dir)

# Model values
optimizer = 'adam'
activation_fn = 'softmax'
layers = 2
dropout = 0.2 
batch_size = 16
vector_size = 30
epochs = 20
randomSeed = 1099
mask = True
modelName = 'BGRU_W2V_'
weightpath = os.path.join('model', modelName + optimizer + str(randomSeed))
dvt = DetectVulType(create_bgru_model, m_epochs=epochs, batch_size=batch_size, mask=mask, dropout=dropout, vectorTrainPath=vectorTrainPath, vectorTestPath=vectorTestPath, inputsTrainPath=dlInputsTrainPath, inputsTestPath=dlInputsTestPath, checkpoint_dir=checkpoint_dir)
dvt.avg = avg

# Open a strategy scope
strategy = tf.distribute.MirroredStrategy()
print('\nNumber of devices: {}'.format(strategy.num_replicas_in_sync))
with strategy.scope():
    didRestore, latest_epoch, myKerasModel = make_or_restore_model(checkpoint_dir, dvt.get_compiled_model)

callbacks = [
    keras.callbacks.ModelCheckpoint(
        filepath=os.path.join(checkpoint_dir, 'ckpt-{epoch}'),
        monitor='val_acc',
        verbose=1,
        save_freq='epochs'
    )
]

model = fit_custom_dl_model(myKerasModel, weightpath, dlInputsTrainPath, batch_size, avg, vector_size, randomSeed, epochs, useGenerator=True, callbacks=callbacks, latest_epoch=latest_epoch)

## Prediction
model.load_weights(weightpath)
_, mypredicted_labels, myreallabels, _ = predictMulticlassLabel(model, dlInputsTestPath, avg, vector_size, optimizer, modelName, randomSeed, labelEncoder, saveOutput=True)

# Confusion Matrix
getConfusionMatrix_Multiclass(mypredicted_labels, myreallabels, saveFig=False, path=w2vmetricPath)

# ROC
roc_auc_dict = roc_auc_score_multiclass(myreallabels, mypredicted_labels)
print(f'\n{modelName} ROC AUC Score\n{roc_auc_dict}\n')





"""
                        ### BLSTM

# Open a strategy scope
with strategy.scope():
    #Build BLSTMModel with parameters 
    myKerasModel =  create_blstm_model(maxlen, vector_size, layers, dropout, optimizer, density_units)

# Fit BLSTM Model with trained data and saved the model for later use
weightpath = os.path.join('model', 'BLSTM_ALL' + optimizer +str(randomSeed))
mymodel = fit_custom_dl_model(myKerasModel, weightpath, dlInputsTrainPath, batch_size, maxlen, vector_size, randomSeed)

# ### Prediction
modelName = 'BLSTM'
mymodel.load_weights(weightpath)
thresholds_dl_labels2, mypredicted_labels2, myreallabels2  = predictMulticlassLabel(mymodel, dlInputsTestPath, maxlen, vector_size, optimizer, modelName, randomSeed, labelEncoder)

# ## Confusion Matrix
metrics = getConfusionMatrix_Multiclass(mypredicted_labels2, myreallabels2, saveFig=True, path='./data/plots/confusionMatrixHeatmapBLSTM.png')

# ## Output Analysis
fileName = "OutputSummary_adamBLSTM1099.xlsx"
DLdata = pd.read_excel(fileName)
print("\nADAM BLSTM Output Summary:\n", DLdata.head(10))


# ## ROC
roc_auc_dict = roc_auc_score_multiclass(myreallabels2, mypredicted_labels2)
print('\nROC AUC Score - BLSTM\n', roc_auc_dict)


#################################################################################
                        ### OPTIMIZATIONS
#################################################################################
                        ### WITHOUT CLASS 0
                        # Preprocessing
print("\n\n=============================== WITHOUT CLASS 0 ===============================\n\n")
#### Split new train/test sets that exclude class 0
# dropClass = 0
# splitTrainTestCategorical('balanced', DLVectors, vectorTrainPath, vectorTestPath, randomSeed, dropClass=dropClass)

# ### Get number of unique classes for density value
X_train = getDataset(vectorTrainPath, getBalanced=True)
original_labels = X_train[-2]
categories = np.unique(original_labels)
mapping, labelEncoder = encode_target(categories)
density_units = categories.shape[0]

# ### Adjust Vector Length
avg = meanLen(vectorTrainPath)
# tranformVectorLen(vectorTrainPath, vectorTestPath, dlInputsTrainPath, dlInputsTestPath, avg, vector_size)
# print(f'New Vector Length (rows x cols): {avg} x {vector_size}\n')

### Replaces true labels with encoded labels in datafile from which our DL will train from
# saveKeyDataMulticlass(dlInputsTrainPath, labelEncoder)
# saveKeyDataMulticlass(dlInputsTestPath, labelEncoder)

                        # BGRU
# Fit BGRU Model with trained data and save the model for later use
optimizer = 'adam' #can be changed to ‘adamax’
maxlen = avg #avg calculated from part 5.6
layers = 2
dropout = 0.2 
batch_size = 32
vector_size = 30
epochs = 10
modelName = 'BGRU0_'

myKerasModel = create_bgru_model(maxlen, vector_size, layers, dropout, optimizer, density_units)
weightpath = os.path.join('model', modelName + optimizer +str(randomSeed))
mymodel = fit_custom_dl_model(myKerasModel, weightpath, dlInputsTrainPath, batch_size, maxlen, vector_size, randomSeed, epochs)

# ### Prediction
mymodel.load_weights(weightpath)
thresholds_dl_labels, mypredicted_labels, myreallabels = predictMulticlassLabel(mymodel, dlInputsTestPath, maxlen, vector_size, optimizer, modelName, randomSeed, labelEncoder)

# ## Confusion Matrix
plotPath = "./data/plots/confusionMatrixHeatmap%s.png" % modelName
metrics = getConfusionMatrix_Multiclass(mypredicted_labels, myreallabels, saveFig=True, path=plotPath)

# ## Output Analysis
fileName = "OutputSummary_adam%s1099.xlsx" % modelName
DLdata = pd.read_excel(fileName)
print(f'\nADAM {modelName} Output Summary:\n {DLdata.head(10)}')

# ## ROC
roc_auc_dict = roc_auc_score_multiclass(myreallabels, mypredicted_labels)
print(f'\n{modelName} ROC AUC Score\n {roc_auc_dict}')


                        ### BLSTM
# Fit BLSTM Model with trained data and saved the model for later use
modelName = 'BLSTM0_'
myKerasModel =  create_blstm_model(maxlen, vector_size, layers, dropout, optimizer, density_units)
weightpath = os.path.join('model', modelName + optimizer +str(randomSeed))
mymodel = fit_custom_dl_model(myKerasModel, weightpath, dlInputsTrainPath, batch_size, maxlen, vector_size, randomSeed)

# ### Prediction
mymodel.load_weights(weightpath)
thresholds_dl_labels2, mypredicted_labels2, myreallabels2  = predictMulticlassLabel(mymodel, dlInputsTestPath, maxlen, vector_size, optimizer, modelName, randomSeed, labelEncoder)

# ## Confusion Matrix
plotPath = "./data/plots/confusionMatrixHeatmap%s.png" % modelName
metrics = getConfusionMatrix_Multiclass(mypredicted_labels2, myreallabels2, saveFig=True, path=plotPath)

# ## Output Analysis
fileName = "OutputSummary_adam%s1099.xlsx" % modelName
DLdata = pd.read_excel(fileName)
print(f'\nADAM {modelName} Output Summary:\n {DLdata.head(10)}')

# ## ROC
roc_auc_dict = roc_auc_score_multiclass(myreallabels2, mypredicted_labels2)
print(f'\n{modelName} ROC AUC Score\n {roc_auc_dict}')




                                        ### RUN ON BEST PARAMS
                                            ### BGRU
print("\n\n=============================== BEST PARAMS - BGRU ===============================\n\n")
optimizer = 'adam' 
layers = 2
dropout = 0.2 
batch_size = 16
vector_size = 30
epochs = 20
activation_fn = 'softmax'
modelName = 'BGRU3_'


# Create MirroredStrategy for Single-host, multi-device (GPU) synchronous training
strategy = tf.distribute.MirroredStrategy()
print('Number of devices: {}'.format(strategy.num_replicas_in_sync))
with strategy.scope():
    myKerasModel = create_bgru_model(avg, vector_size, layers, dropout, optimizer, density_units, activation_fn=activation_fn)

weightpath = os.path.join('model', modelName + optimizer +str(randomSeed))
mymodel = fit_custom_dl_model(myKerasModel, weightpath, dlInputsTrainPath, batch_size, avg, vector_size, randomSeed, epochs)

# ### Prediction
mymodel.load_weights(weightpath)
thresholds_dl_labels, mypredicted_labels, myreallabels, outputs_dict = predictMulticlassLabel(mymodel, dlInputsTestPath, avg, vector_size, optimizer, modelName, randomSeed, labelEncoder)

# ## Confusion Matrix
plotPath = "./data/plots/confusionMatrixHeatmap%s.png" % modelName
metrics = getConfusionMatrix_Multiclass(mypredicted_labels, myreallabels, saveFig=True, path=plotPath)

# ## Output Analysis
fileName = "OutputSummary_adam%s1099.xlsx" % modelName
DLdata = pd.read_excel(fileName)
print(f'\nADAM {modelName} Output Summary:\n {DLdata.head(10)}')

# ## ROC
roc_auc_dict = roc_auc_score_multiclass(myreallabels, mypredicted_labels)
print(f'\n{modelName} ROC AUC Score\n {roc_auc_dict}')



                                            ### BLSTM
print("\n\n=============================== BEST PARAMS - BLSTM ===============================\n\n")
modelName = 'BLSTM3_'
strategy = tf.distribute.MirroredStrategy()
print('Number of devices: {}'.format(strategy.num_replicas_in_sync))

with strategy.scope():
    myKerasModel =  create_blstm_model(avg, vector_size, layers, dropout, optimizer, density_units, activation_fn=activation_fn)

weightpath = os.path.join('model', modelName + optimizer +str(randomSeed))
mymodel = fit_custom_dl_model(myKerasModel, weightpath, dlInputsTrainPath, batch_size, avg, vector_size, randomSeed)

# ### Prediction
mymodel.load_weights(weightpath)
thresholds_dl_labels2, mypredicted_labels2, myreallabels2  = predictMulticlassLabel(mymodel, dlInputsTestPath, avg, vector_size, optimizer, modelName, randomSeed, labelEncoder)

# ## Confusion Matrix
plotPath = "./data/plots/confusionMatrixHeatmap%s.png" % modelName
metrics = getConfusionMatrix_Multiclass(mypredicted_labels2, myreallabels2, saveFig=True, path=plotPath)

# ## Output Analysis
fileName = "OutputSummary_adam%s1099.xlsx" % modelName
DLdata = pd.read_excel(fileName)
print(f'\nADAM {modelName} Output Summary:\n {DLdata.head(10)}')

# ## ROC
roc_auc_dict = roc_auc_score_multiclass(myreallabels2, mypredicted_labels2)
print(f'\n{modelName} ROC AUC Score\n {roc_auc_dict}')

"""


