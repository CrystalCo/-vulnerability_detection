"""
    This file starts from the very beginning by creating the tokenized dataset based on the
    vulnerability type passed in.  It trains a W2V model on the new set, splits into training/test
    set folders named by type, flattens the vectors, then builds and tests a DL model on that data.

    Since the first portion (BGRU) runs all of the above actions, the second portion (BLSTM)
    has lines commented out that would create redundant work.
"""

import gc, os, sys

VUL_PATH = os.environ.get('VUL_PATH')
sys.path.insert(1, VUL_PATH)

from SYSE_2_vulnerabilityType.VulnerabilityClassification import VulnerabilityClassification
from utils.Word2VecModel import Word2VecModel
from utils.DLCustomModels import create_bgru_model, create_blstm_model
from utils.utils import getDataset


def run_all(VUL_TYPE, slicefile, MODEL_TYPE='bgru', model_fn=create_bgru_model):
    RANDOMSEED = 1099
    CLASS_TYPE = '162_%s' % VUL_TYPE
    VECTOR_TRANSFORMER='w2v'
    LAYERS = 2
    DROPOUT = 0.2
    BATCHSIZE = 64
    EPOCHS = 60

    vectorRootPath = os.path.join('data','DLvectors')
    vectorsALLPath = os.path.join(vectorRootPath, f'{VUL_TYPE}_vectors')
    vectorTrainPath = os.path.join(vectorRootPath, f'train_162_{VUL_TYPE}')
    vectorTestPath = os.path.join(vectorRootPath, f'test_162_{VUL_TYPE}')
    inputsRootPath = os.path.join('data', 'DLinputs')
    dlInputsTrainPath = os.path.join(inputsRootPath, f'train_162_{VUL_TYPE}')
    dlInputsTestPath = os.path.join(inputsRootPath, f'test_162_{VUL_TYPE}')

    checkpoint_dir = './ckpt_%s_%s_%s_%s' % (VECTOR_TRANSFORMER, MODEL_TYPE, str(BATCHSIZE), CLASS_TYPE)
    model_name = '%s_%s_batch=%s_seed=%s_epochs=%s_%s' % (MODEL_TYPE.upper(), VECTOR_TRANSFORMER.upper(), BATCHSIZE, RANDOMSEED, EPOCHS, CLASS_TYPE)
    metrics_path = os.path.join(f'metrics', MODEL_TYPE)
    weights_path = os.path.join('model', model_name + 'weights')
    w2vmodelPath = os.path.join('w2vModel','model', f'w2vModel_{VUL_TYPE}')
    tokens_path = os.path.join('data', 'token', VUL_TYPE)

    dvt = VulnerabilityClassification(build_model=model_fn, useGenerator=True, 
        transformer_model=Word2VecModel, transformerPath=w2vmodelPath, 
        metricsPath=metrics_path, randomSeed=RANDOMSEED, window=3, m_epochs=EPOCHS, 
        modelName=model_name, batch_size=BATCHSIZE, mask=True, dropout=DROPOUT, layers=LAYERS,
        tokensPath=tokens_path, vectorsALLPath=vectorsALLPath, vectorRootPath=vectorRootPath, 
        vectorTrainPath=vectorTrainPath, vectorTestPath=vectorTestPath, 
        inputsTrainPath=dlInputsTrainPath, inputsTestPath=dlInputsTestPath,
        checkpoint_dir=checkpoint_dir, weightpath=weights_path)


    print('Starting %s & %s' % (VUL_TYPE, MODEL_TYPE))

    # Reset if need be
    print('Resetting checkpoint and weights...')
    dvt.reset_checkpoint_and_weights(dvt.weightpath)

    # scrape data only from file
    print('\nTokenizing slices...')
    all_tokensPath = os.path.join(dvt.tokensPath, 'ALL_tokens.pkl') # save to data/token/vul_type/
    dvt.tokenizeSlicesPerFile(slicefile, all_tokensPath)

    # train W2V model
    print('\nTraining W2V model...')
    dvt.init_transformer()
    # Fit transformer & transform our data
    dvt.fit_transform(dvt.transformerPath, dvt.tokensPath, dvt.vectorsALLPath, getBalanced=False)

    # Split data into training and test set
    print('\nSplitting train/test...')
    dvt.splitTrainTest(dvt.vectorsALLPath, None)

    # To flatten dataset, we must average out the dimension of the dataset which contains the tokens. AverageS out the row length per sample based on focuspointer.
    print('\nAdjusting Vector Length...')
    dvt.adjustVectorLength() # outputs to data/DLinput

    # Flatten 3D vectors to 2D
    dvt.flatten_vectors(dvt.inputsTrainPath, pcaTransformVulType=None)
    dvt.flatten_vectors(dvt.inputsTestPath, pcaTransformVulType=None)
    # Save the new vector length & avg
    data = getDataset(dvt.inputsTestPath, True)
    dvt.vector_size = len(data[0][0])
    dvt.avg = 1
    print(f'Avg: {dvt.avg}\tVector length: {dvt.vector_size}\n')
    del data
    gc.collect()

    # Build dl model & predict results
    dvt.hotEncodeLabels()
    dvt.saveKeyData(dvt.inputsTrainPath, dvt.inputsTrainPath)
    dvt.saveKeyData(dvt.inputsTestPath, dvt.inputsTestPath)
    print('\nBuilding/fitting DL model...')
    dvt.build_and_fit()
    print('\nPredicting & Scoring...')
    dvt.predict_and_score()
    print('\n\n\n\n\n\n')


VUL_TYPE = 'AE'
slicefile = 'Arithmetic expression.txt'
run_all(VUL_TYPE, slicefile)


"""   AU   """
VUL_TYPE = 'AU'
slicefile = 'Array usage.txt'
run_all(VUL_TYPE, slicefile)


"""   API  """
VUL_TYPE = 'API'
slicefile = 'API function call.txt'
run_all(VUL_TYPE, slicefile)


"""   PTR  """
VUL_TYPE = 'PTR'
slicefile = 'Pointer usage.txt'
run_all(VUL_TYPE, slicefile)


# BLSTM
def run_all(VUL_TYPE, slicefile, MODEL_TYPE='bgru', model_fn=create_bgru_model):
    RANDOMSEED = 1099
    CLASS_TYPE = '162_%s' % VUL_TYPE
    VECTOR_TRANSFORMER='w2v'
    LAYERS = 2
    DROPOUT = 0.2
    BATCHSIZE = 64
    EPOCHS = 60

    vectorRootPath = os.path.join('data','DLvectors')
    vectorsALLPath = os.path.join(vectorRootPath, f'{VUL_TYPE}_vectors')
    vectorTrainPath = os.path.join(vectorRootPath, f'train_162_{VUL_TYPE}')
    vectorTestPath = os.path.join(vectorRootPath, f'test_162_{VUL_TYPE}')
    inputsRootPath = os.path.join('data', 'DLinputs')
    dlInputsTrainPath = os.path.join(inputsRootPath, f'train_162_{VUL_TYPE}')
    dlInputsTestPath = os.path.join(inputsRootPath, f'test_162_{VUL_TYPE}')

    checkpoint_dir = './ckpt_%s_%s_%s_%s' % (VECTOR_TRANSFORMER, MODEL_TYPE, str(BATCHSIZE), CLASS_TYPE)
    model_name = '%s_%s_batch=%s_seed=%s_epochs=%s_%s' % (MODEL_TYPE.upper(), VECTOR_TRANSFORMER.upper(), BATCHSIZE, RANDOMSEED, EPOCHS, CLASS_TYPE)
    metrics_path = os.path.join(f'metrics', MODEL_TYPE)
    weights_path = os.path.join('model', model_name + 'weights')
    w2vmodelPath = os.path.join('w2vModel','model', f'w2vModel_{VUL_TYPE}')
    tokens_path = os.path.join('data', 'token', VUL_TYPE)

    dvt = VulnerabilityClassification(build_model=model_fn, useGenerator=True, 
        transformer_model=Word2VecModel, transformerPath=w2vmodelPath, 
        metricsPath=metrics_path, randomSeed=RANDOMSEED, window=3, m_epochs=EPOCHS, 
        modelName=model_name, batch_size=BATCHSIZE, mask=True, dropout=DROPOUT, layers=LAYERS,
        tokensPath=tokens_path, vectorsALLPath=vectorsALLPath, vectorRootPath=vectorRootPath, 
        vectorTrainPath=vectorTrainPath, vectorTestPath=vectorTestPath, 
        inputsTrainPath=dlInputsTrainPath, inputsTestPath=dlInputsTestPath,
        checkpoint_dir=checkpoint_dir, weightpath=weights_path)


    print('Starting %s & %s' % (VUL_TYPE, MODEL_TYPE))

    # Reset if need be
    # print('Resetting checkpoint and weights...')
    # dvt.reset_checkpoint_and_weights(dvt.weightpath)

    # scrape data only from file
    # print('\nTokenizing slices...')
    # all_tokensPath = os.path.join(dvt.tokensPath, 'ALL_tokens.pkl') # save to data/token/vul_type/
    # dvt.tokenizeSlicesPerFile(slicefile, all_tokensPath)

    # train W2V model
    # print('\nTraining W2V model...')
    # dvt.init_transformer()
    # # Fit transformer & transform our data
    # dvt.fit_transform(dvt.transformerPath, dvt.tokensPath, dvt.vectorsALLPath, getBalanced=False)

    # Split data into training and test set
    # print('\nSplitting train/test...')
    # dvt.splitTrainTest(dvt.vectorsALLPath, None)

    # To flatten dataset, we must average out the dimension of the dataset which contains the tokens. AverageS out the row length per sample based on focuspointer.
    print('\nAdjusting Vector Length...')
    dvt.adjustVectorLength() # outputs to data/DLinput

    # Flatten 3D vectors to 2D
    # dvt.flatten_vectors(dvt.inputsTrainPath, pcaTransformVulType=None)
    # dvt.flatten_vectors(dvt.inputsTestPath, pcaTransformVulType=None)
    # Save the new vector length & avg
    data = getDataset(dvt.inputsTestPath, True)
    dvt.vector_size = len(data[0][0])
    dvt.avg = 1
    print(f'Avg: {dvt.avg}\tVector length: {dvt.vector_size}\n')
    del data
    gc.collect()

    # Build dl model & predict results
    dvt.hotEncodeLabels()
    dvt.saveKeyData(dvt.inputsTrainPath, dvt.inputsTrainPath)
    dvt.saveKeyData(dvt.inputsTestPath, dvt.inputsTestPath)
    print('\nBuilding/fitting DL model...')
    dvt.build_and_fit()
    print('\nPredicting & Scoring...')
    dvt.predict_and_score()
    print('\n\n\n\n\n\n')

VUL_TYPE = 'AE'
slicefile = 'Arithmetic expression.txt'
run_all(VUL_TYPE, slicefile, 'blstm', create_blstm_model)


"""   AU   """
VUL_TYPE = 'AU'
slicefile = 'Array usage.txt'
run_all(VUL_TYPE, slicefile, 'blstm', create_blstm_model)


"""   API  """
VUL_TYPE = 'API'
slicefile = 'API function call.txt'
run_all(VUL_TYPE, slicefile, 'blstm', create_blstm_model)


"""   PTR  """
VUL_TYPE = 'PTR'
slicefile = 'Pointer usage.txt'
run_all(VUL_TYPE, slicefile, 'blstm', create_blstm_model)
