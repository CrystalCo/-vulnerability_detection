import os
import importlib
import pickle

import numpy as np
from sklearn.cluster import KMeans
from scipy.cluster.hierarchy import dendrogram
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, silhouette_score, silhouette_samples
from sklearn.model_selection import GridSearchCV
from matplotlib import cm
from matplotlib import pyplot as plt

def get_SARD(sentence0):
    header = sentence0.split(' ')[1]
    sard = header.split('/')[0]
    return sard

def get_sard_cve_ids(filenames):
    ids = []
    for f in filenames:
        x = get_SARD(f)
        ids.append(x)
    return ids

def cluster_sizes(clusters):
    #clusters is an array of cluster labels for each instance in the data
    
    size = {}
    cluster_labels = np.unique(clusters, return_counts=True) # unique labels

    for c in cluster_labels[0]:
        size[c] = cluster_labels[1][c]
    return size

def cv_silhouette_scorer(estimator, X):
    # Implementing my own scoring method since sklearn does not have a scoring metric for unsupervised learning
    estimator.fit(X)
    cluster_labels = estimator.labels_
    num_labels = len(set(cluster_labels))
    num_samples = len(X)
    if num_labels == 1 or num_labels == num_samples:
        return -1
    else:
        return silhouette_score(X, cluster_labels)

def getDataset(dataset_path, getBalanced=True, RANDOMSEED=None):
    print("Getting dataset...")
    dataset = []
    labels = []
    sard_cve_ids = []
    testcases = []
    vtypes = []
    filetype = "balanced" if getBalanced else "ALL"
    for filename in os.listdir(dataset_path):
        if (not filename.endswith(".DS_Store")) and (filetype in filename):
            print(filename)
            f = open(os.path.join(dataset_path, filename),"rb")
            dataset_file,labels_file,funcs_file,filenames_file,vtype_file, testcases_file = pickle.load(f)
            f.close()
            dataset += dataset_file
            labels += labels_file
            sard_cve_ids = get_sard_cve_ids(filenames_file)
            testcases += testcases_file
            vtypes += vtype_file

    group_labels = [x[0] for x in labels]
    original_labels = [x[1] for x in labels]
    labels = group_labels

    if RANDOMSEED:
        np.random.seed(RANDOMSEED)
        np.random.shuffle(dataset)
        np.random.seed(RANDOMSEED)
        np.random.shuffle(labels)
        np.random.seed(RANDOMSEED)
        np.random.shuffle(original_labels)
        np.random.seed(RANDOMSEED)
        np.random.shuffle(sard_cve_ids)
        np.random.seed(RANDOMSEED)
        np.random.shuffle(testcases)
        np.random.seed(RANDOMSEED)
        np.random.shuffle(vtypes)
   
    return dataset, labels, original_labels, sard_cve_ids, testcases, vtypes

def get_grid(model, parameters, scoring, X, Y=None, vb=1):
    grid = GridSearchCV(estimator=model, param_grid=parameters, scoring=scoring, cv=5, return_train_score=True, verbose=vb) # if we get a warning on the server about iid, add parameter `iid=False`
    grid.fit(X,Y)
    print("Best params: ", grid.best_params_)
    print("Best Score: ", grid.best_score_)
    return grid

def measure_performance(X, y, clf, show_accuracy=True, show_classification_report=True, show_confusion_matrix=True):
    y_pred = clf.predict(X)   
    if show_accuracy:
         print ("Accuracy:{0:.3f}".format(accuracy_score(y, y_pred)),"\n")
    
    if show_classification_report:
        print ("Classification report")
        print (classification_report(y, y_pred),"\n")
      
    if show_confusion_matrix:
        mat = confusion_matrix(y, y_pred)
        print ("Confusion matrix")
        print (mat,"\n")
        target_names = np.unique(y)
        plot_confusion_matrix(mat, target_names)
        
def plot_confusion_matrix(mat, target_names):
    import seaborn as sns; sns.set()

    fig, ax = plt.subplots(figsize=(8,8))
    ax = sns.heatmap(mat.T, square=True, linecolor='grey', linewidths=1, annot=True, 
                fmt='d', cbar=True, cmap='Reds', ax=ax, annot_kws={"fontsize":12, "weight":"bold"},
                xticklabels=target_names,
                yticklabels=target_names)
    bottom, top = ax.get_ylim()
    ax.set_ylim(bottom + 0.5, top - 0.5)
    plt.xlabel('true label')
    plt.ylabel('predicted label');


# source: https://scikit-learn.org/stable/auto_examples/cluster/plot_agglomerative_dendrogram.html#sphx-glr-auto-examples-cluster-plot-agglomerative-dendrogram-py
def plot_dendrogram(model, **kwargs):
    # Create linkage matrix and then plot the dendrogram

    # create the counts of samples under each node
    counts = np.zeros(model.children_.shape[0])
    n_samples = len(model.labels_)
    for i, merge in enumerate(model.children_):
        current_count = 0
        for child_idx in merge:
            if child_idx < n_samples:
                current_count += 1  # leaf node
            else:
                current_count += counts[child_idx - n_samples]
        counts[i] = current_count

    linkage_matrix = np.column_stack([model.children_, model.distances_,
                                      counts]).astype(float)

    # Plot the corresponding dendrogram
    dendrogram(linkage_matrix, **kwargs)

def plot_silhouettes(data, clusters, metric='euclidean'):
    cluster_labels = np.unique(clusters)
    n_clusters = cluster_labels.shape[0]
    silhouette_vals = silhouette_samples(data, clusters, metric='euclidean')
    c_ax_lower, c_ax_upper = 0, 0
    cticks = []
    for i, k in enumerate(cluster_labels):
        c_silhouette_vals = silhouette_vals[clusters == k]
        c_silhouette_vals.sort()
        c_ax_upper += len(c_silhouette_vals)
        color = cm.jet(float(i) / n_clusters)
        plt.barh(range(c_ax_lower, c_ax_upper), c_silhouette_vals, height=1.0, 
                      edgecolor='none', color=color)

        cticks.append((c_ax_lower + c_ax_upper) / 2)
        c_ax_lower += len(c_silhouette_vals)
    
    silhouette_avg = np.mean(silhouette_vals)
    plt.axvline(silhouette_avg, color="red", linestyle="--") 

    plt.yticks(cticks, cluster_labels)
    plt.ylabel('Cluster')
    plt.xlabel('Silhouette coefficient')

    plt.tight_layout()
    #plt.savefig('images/11_04.png', dpi=300)
    plt.show()
    
    return

def plot_params(param_values, param_name, train_scores, test_scores, save_file=''):
    # plot the training and testing scores in a log scale
    plt.plot(param_values, train_scores, label='Train', alpha=0.4, lw=2, c='b')
    plt.plot(param_values, test_scores, label='X-Val', alpha=0.4, lw=2, c='g')
    plt.legend(loc=7)
    plt.xlabel(param_name)
    plt.ylabel("Average Silhoutte Scores")
    if save_file != '':
        plt.title(save_file)
        plt.savefig(save_file)

def print_cluster_sizes(size):
    print("Sizes of predicted clusters:")
    for c in size.keys():
        print("Size of Cluster", c, "= ", size[c])

