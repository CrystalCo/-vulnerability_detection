"""
    Upsampling using Synthetic Minority Oversampling Technique (SMOTE) on our training set.
    Trains our models with vulnerable samples that were upsampled synthetically on classes
    with samples less than class with the highest count.  By default, the other classes will 
    have as many samples as the class with the highest count.
"""

import gc, os, sys

VUL_PATH = os.environ.get('VUL_PATH')
sys.path.insert(1, VUL_PATH)

import numpy as np

from SYSE_2_vulnerabilityType.VulnerabilityClassification import VulnerabilityClassification
from utils.Word2VecModel import Word2VecModel
from utils.DLCustomModels import create_bgru_model, create_blstm_model
from utils.utils import hot_encode_target, getDataset


def run_all(VUL_TYPE, vectorsALLdir='ALL_vectors_granular_vulnerable_only', MODEL_TYPE='bgru', model_fn=create_bgru_model, min_num=None):
    RANDOMSEED = 1099
    CLASS_TYPE = f'{VUL_TYPE}_vulnerable_min_{min_num}_SMOTE'
    VECTOR_TRANSFORMER='w2v'
    LAYERS = 2
    DROPOUT = 0.2
    BATCHSIZE = 64
    EPOCHS = 60

    inputsRootPath = os.path.join('data', 'DLinputs')
    vectorRootPath = os.path.join('data','DLvectors')
    vectorsALLPath = os.path.join(vectorRootPath, vectorsALLdir)
    # Rather than taking up memory by creating separate folders for each type, 
    # we can overwrite each one since the upsampled collection is < the fullset.
    vectorTrainPath = os.path.join(vectorRootPath, f'train')
    vectorTestPath = os.path.join(vectorRootPath, f'test')
    dlInputsTrainPath = os.path.join(inputsRootPath, f'train')
    dlInputsTestPath = os.path.join(inputsRootPath, f'test')

    checkpoint_dir = './ckpt_%s_%s_%s_%s' % (VECTOR_TRANSFORMER, MODEL_TYPE, str(BATCHSIZE), CLASS_TYPE)
    model_name = '%s_%s_batch=%s_seed=%s_epochs=%s_%s' % (MODEL_TYPE.upper(), VECTOR_TRANSFORMER.upper(), BATCHSIZE, RANDOMSEED, EPOCHS, CLASS_TYPE)
    metrics_path = os.path.join(f'metrics', MODEL_TYPE)
    weights_path = os.path.join('model', model_name + 'weights')
    w2vmodelPath = os.path.join('w2vModel','model', f'w2vModel_{VUL_TYPE}')
    tokens_path = os.path.join('data', 'token', VUL_TYPE)

    dvt = VulnerabilityClassification(build_model=model_fn, useGenerator=True, 
        transformer_model=Word2VecModel, transformerPath=w2vmodelPath, 
        metricsPath=metrics_path, randomSeed=RANDOMSEED, window=3, m_epochs=EPOCHS, 
        modelName=model_name, batch_size=BATCHSIZE, mask=True, dropout=DROPOUT, layers=LAYERS,
        tokensPath=tokens_path, vectorsALLPath=vectorsALLPath,
        vectorTrainPath=vectorTrainPath, vectorTestPath=vectorTestPath, 
        inputsTrainPath=dlInputsTrainPath, inputsTestPath=dlInputsTestPath,
        checkpoint_dir=checkpoint_dir, weightpath=weights_path)

    print('Starting %s & %s' % (CLASS_TYPE, MODEL_TYPE))

    # Reset if need be
    print('Resetting checkpoint and weights...')
    dvt.reset_checkpoint_and_weights(dvt.weightpath)

    # Split data into training and test set
    print('\nSplitting train/test...')
    dvt.splitTrainTest(dvt.vectorsALLPath, min_num=min_num, vType=VUL_TYPE, dropOtherVtypes=False)

    # Don't adjust the row length since they're already flattened. Manually set the new vector length & avg.
    data = getDataset(dvt.vectorTestPath, True)
    dvt.vector_size = len(data[0][0])
    dvt.avg = 1
    print(f'Avg: {dvt.avg}\tVector length: {dvt.vector_size}\n')
    del data
    gc.collect()

    # Encode our labels and save encoded results to DLinputs path
    mapping = dvt.encodeLabelsByLabelEncoder()
    print('Mapping:\n', mapping)
    dvt.saveKeyData(dvt.vectorTrainPath, dvt.inputsTrainPath)
    dvt.saveKeyData(dvt.vectorTestPath, dvt.inputsTestPath)
    
    # Apply SMOTE to training class
    print('\nApplying SMOTE..')
    # strategy = {197:  400, 591:  400, 427:  400, 415:  400, 761:  400, 771:  400, 666:  400, 369:  400, 787:  400, 253:  400, 191:  400, 506:  400, 758:  400, 401:  400, 88:   400, 617:  400, 675:  400, 363:  400, 200:  400, 785:  400, 426:  400, 252:  400, 773:  400, 90:   400, 404:  400}
    strategy = {18: 400, 36: 400, 31: 400,  28: 400, 45: 400,  38: 400,  24: 400,  48: 400,  21: 400, 15: 400,  34: 400,  42: 400,  26: 400,  3: 400, 37: 400, 39: 400,23: 400, 19: 400,47: 400, 30: 400, 20: 400,46: 400, 5: 400, 27: 400}
    print('Strategry: ', strategy)
    X, y = dvt.smote(strategy, mapping)

    # Now we can hot encode our labels...?
    categories = np.unique(y)
    mapping2, binarizerEncoder = hot_encode_target(categories)
    print('\nNew hot-encoded mapping:\n', mapping2)
    hot_encoded_y = binarizerEncoder.transform(y)

    # Build DL model
    myKerasModel, checkpointCallback, latest_epoch = dvt.build_estimator()

    # fit with generator
    dvt.fit_estimator_with_generator(X, hot_encoded_y, myKerasModel, checkpointCallback, latest_epoch)

    # Predict & score on our test set (does not have synthetic upsampling)
    y_true, y_pred = dvt.predict_using_encoders(binarizerEncoder)
    dvt.score_predictions(y_true, y_pred)

    print('\n\n\n\n\n\n')


# for testing: 
# vectorsALLdir='Subsample_ALL_vulnerable'
vectorsALLdir='ALL_vectors_granular_vulnerable_only'
min_number = 100

VUL_TYPE = 'ALL'
run_all(VUL_TYPE, vectorsALLdir=vectorsALLdir, min_num=min_number)

### BLSTM ###
run_all(VUL_TYPE, MODEL_TYPE='blstm', model_fn=create_blstm_model, min_num=min_number)

