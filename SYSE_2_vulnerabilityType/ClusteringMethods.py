import os
import importlib
import pickle

import numpy as np
from sklearn.cluster import KMeans
from scipy.cluster.hierarchy import dendrogram
from sklearn.metrics import silhouette_score
from matplotlib import pyplot as plt

# from SYSE_1_isVulnerable.preprocess_dl_Input_version5 import multi_labels_to_two

# copying the code slice since having issues importing
def multi_labels_to_two(label):
    if 1 == label:
        return 1
    else:
        return 0

def _getSardCveIds(filenames):
    ids = []
    for f in filenames:
        x = f.split(' ')[1].split('/')[0]
        ids.append(x)
    return ids

def cluster_sizes(clusters):
    #clusters is an array of cluster labels for each instance in the data
    
    size = {}
    cluster_labels = np.unique(clusters, return_counts=True) # unique labels

    for c in cluster_labels[0]:
        size[c] = cluster_labels[1][c]
    return size

def print_cluster_sizes(size):
    print("Sizes of predicted clusters:")
    for c in size.keys():
        print("Size of Cluster", c, "= ", size[c])

def getTrainingData(traindataSet_path, getBalanced=1, RANDOMSEED=None):
    
    print("Getting Balanced Training dataset...")
    dataset = []
    labels = []
    filenames = []
    testcases = []
    vtypes = []
    filetype = "balanced" if getBalanced else "ALL"
    for filename in os.listdir(traindataSet_path):
        if (not filename.endswith(".DS_Store")) and (filetype in filename):
            print(filename)
            f = open(os.path.join(traindataSet_path, filename),"rb")
            dataset_file,labels_file,funcs_file,filenames_file,vtype_file, testcases_file = pickle.load(f)
            f.close()
            dataset += dataset_file
            labels += labels_file
            filenames = _getSardCveIds(filenames_file)
            testcases += testcases_file
            vtypes += vtype_file

    bin_labels = []
    for label in labels:
        bin_labels.append(multi_labels_to_two(label))
    labels = bin_labels

    # np.random.seed(RANDOMSEED)
    # np.random.shuffle(_dataset)
    # np.random.seed(RANDOMSEED)
    # np.random.shuffle(labels)
    # np.random.seed(RANDOMSEED)
    # np.random.shuffle(testcases)
    # np.random.seed(RANDOMSEED)
    # np.random.shuffle(vtypes)
   
    return dataset, labels, filenames, testcases, vtypes

# source: https://scikit-learn.org/stable/auto_examples/cluster/plot_agglomerative_dendrogram.html#sphx-glr-auto-examples-cluster-plot-agglomerative-dendrogram-py
def plot_dendrogram(model, **kwargs):
    # Create linkage matrix and then plot the dendrogram

    # create the counts of samples under each node
    counts = np.zeros(model.children_.shape[0])
    n_samples = len(model.labels_)
    for i, merge in enumerate(model.children_):
        current_count = 0
        for child_idx in merge:
            if child_idx < n_samples:
                current_count += 1  # leaf node
            else:
                current_count += counts[child_idx - n_samples]
        counts[i] = current_count

    linkage_matrix = np.column_stack([model.children_, model.distances_,
                                      counts]).astype(float)

    # Plot the corresponding dendrogram
    dendrogram(linkage_matrix, **kwargs)

def plot_silhouettes(data, clusters, metric='euclidean'):

    from matplotlib import cm
    from sklearn import metrics
    from sklearn.metrics import silhouette_samples

    cluster_labels = np.unique(clusters)
    n_clusters = cluster_labels.shape[0]
    silhouette_vals = metrics.silhouette_samples(data, clusters, metric='euclidean')
    c_ax_lower, c_ax_upper = 0, 0
    cticks = []
    for i, k in enumerate(cluster_labels):
        c_silhouette_vals = silhouette_vals[clusters == k]
        c_silhouette_vals.sort()
        c_ax_upper += len(c_silhouette_vals)
        color = cm.jet(float(i) / n_clusters)
        plt.barh(range(c_ax_lower, c_ax_upper), c_silhouette_vals, height=1.0, 
                      edgecolor='none', color=color)

        cticks.append((c_ax_lower + c_ax_upper) / 2)
        c_ax_lower += len(c_silhouette_vals)
    
    silhouette_avg = np.mean(silhouette_vals)
    plt.axvline(silhouette_avg, color="red", linestyle="--") 

    plt.yticks(cticks, cluster_labels)
    plt.ylabel('Cluster')
    plt.xlabel('Silhouette coefficient')

    plt.tight_layout()
    #plt.savefig('images/11_04.png', dpi=300)
    plt.show()
    
    return


def cv_silhouette_scorer(estimator, X):
    # Implementing my own scoring method since sklearn does not have a scoring metric for unsupervised learning
    estimator.fit(X)
    cluster_labels = estimator.labels_
    num_labels = len(set(cluster_labels))
    num_samples = len(X)
    if num_labels == 1 or num_labels == num_samples:
        return -1
    else:
        return silhouette_score(X, cluster_labels)

def plot_params(param_values, param_name, train_scores, test_scores):
    # plot the training and testing scores in a log scale
    plt.plot(param_values, train_scores, label='Train', alpha=0.4, lw=2, c='b')
    plt.plot(param_values, test_scores, label='X-Val', alpha=0.4, lw=2, c='g')
    plt.legend(loc=7)
    plt.xlabel(param_name)
    plt.ylabel("Average Silhoutte Scores")
    