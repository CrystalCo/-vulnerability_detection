import os
import importlib
import pickle

import numpy as np
from sklearn.cluster import KMeans
from scipy.cluster.hierarchy import dendrogram
from matplotlib import pyplot as plt

from SYSE_1_isVulnerable.preprocess_dl_Input_version5 import multi_labels_to_two

# copying the code slice since having issues importing
# def multi_labels_to_two(label):
#     if 1 == label:
#         return 1
#     else:
#         return 0

def _getSardCveIds(filenames):
    ids = []
    for f in filenames:
        x = f.split(' ')[1].split('/')[0]
        ids.append(x)
    return ids

def cluster_sizes(clusters, dataset):
    #clusters is an array of cluster labels for each instance in the data
    
    size = {}
    cluster_labels = np.unique(clusters) # unique labels

    for c in cluster_labels:
        size[c] = len(dataset[clusters == c])
    return size

def print_cluster_sizes(size):
    print("Sizes of predicted clusters:")
    for c in size.keys():
        print("Size of Cluster", c, "= ", size[c])

def getTrainingData(traindataSet_path, RANDOMSEED=None):
    
    print("Getting Balanced Training dataset...")
    dataset = []
    labels = []
    filenames = []
    testcases = []
    vtypes = []
    for filename in os.listdir(traindataSet_path):
        if not filename.endswith(".DS_Store"):
            print(filename)
            f = open(os.path.join(traindataSet_path, filename),"rb")
            dataset_file,labels_file,funcs_file,filenames_file,vtype_file, testcases_file = pickle.load(f)
            f.close()
            dataset += dataset_file
            labels += labels_file
            filenames = _getSardCveIds(filenames_file)
            testcases += testcases_file
            vtypes += vtype_file

    bin_labels = []
    for label in labels:
        bin_labels.append(multi_labels_to_two(label))
    labels = bin_labels

    # Flatten the data from 3D to 2D
    dataset = np.array(dataset)
    _dataset = np.zeros(dataset.size).reshape((dataset.shape[0], dataset.shape[1] * dataset.shape[2]))
    for i in range(dataset.shape[0]):
        _dataset[i] = dataset[i].flatten()

    # np.random.seed(RANDOMSEED)
    # np.random.shuffle(_dataset)
    # np.random.seed(RANDOMSEED)
    # np.random.shuffle(labels)
    # np.random.seed(RANDOMSEED)
    # np.random.shuffle(testcases)
    # np.random.seed(RANDOMSEED)
    # np.random.shuffle(vtypes)
   
    return _dataset, labels, filenames, testcases, vtypes

# source: https://scikit-learn.org/stable/auto_examples/cluster/plot_agglomerative_dendrogram.html#sphx-glr-auto-examples-cluster-plot-agglomerative-dendrogram-py
def plot_dendrogram(model, **kwargs):
    # Create linkage matrix and then plot the dendrogram

    # create the counts of samples under each node
    counts = np.zeros(model.children_.shape[0])
    n_samples = len(model.labels_)
    for i, merge in enumerate(model.children_):
        current_count = 0
        for child_idx in merge:
            if child_idx < n_samples:
                current_count += 1  # leaf node
            else:
                current_count += counts[child_idx - n_samples]
        counts[i] = current_count

    linkage_matrix = np.column_stack([model.children_, model.distances_,
                                      counts]).astype(float)

    # Plot the corresponding dendrogram
    dendrogram(linkage_matrix, **kwargs)

def plot_silhouettes(data, clusters, metric='euclidean'):

    from matplotlib import cm
    from sklearn import metrics
    from sklearn.metrics import silhouette_samples

    cluster_labels = np.unique(clusters)
    n_clusters = cluster_labels.shape[0]
    silhouette_vals = metrics.silhouette_samples(data, clusters, metric='euclidean')
    c_ax_lower, c_ax_upper = 0, 0
    cticks = []
    for i, k in enumerate(cluster_labels):
        c_silhouette_vals = silhouette_vals[clusters == k]
        c_silhouette_vals.sort()
        c_ax_upper += len(c_silhouette_vals)
        color = cm.jet(float(i) / n_clusters)
        plt.barh(range(c_ax_lower, c_ax_upper), c_silhouette_vals, height=1.0, 
                      edgecolor='none', color=color)

        cticks.append((c_ax_lower + c_ax_upper) / 2)
        c_ax_lower += len(c_silhouette_vals)
    
    silhouette_avg = np.mean(silhouette_vals)
    plt.axvline(silhouette_avg, color="red", linestyle="--") 

    plt.yticks(cticks, cluster_labels)
    plt.ylabel('Cluster')
    plt.xlabel('Silhouette coefficient')

    plt.tight_layout()
    #plt.savefig('images/11_04.png', dpi=300)
    plt.show()
    
    return

# dlInputsTrainPath = './data/DLinputs/train/'
# randomSeed = 1099

# kmeans = KMeans(n_clusters=4, max_iter=500, verbose=1)
# dataset, labels, filenames, testcases, vtypes = getTrainingData(dlInputsTrainPath, randomSeed)
# print(len(dataset))
