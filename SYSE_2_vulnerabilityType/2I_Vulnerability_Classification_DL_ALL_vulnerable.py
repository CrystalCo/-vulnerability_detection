"""
    Trains our models per vulnerability syntax characteristic with only vulnerable samples (not upsampled). 
    Since it only contains vulnerable samples, the full dataset size is 56,385 which is < the original
    dataset of 413,854.
"""

import gc, os, sys

VUL_PATH = os.environ.get('VUL_PATH')
sys.path.insert(1, VUL_PATH)

from SYSE_2_vulnerabilityType.DetectVulType import DetectVulType
from utils.Word2VecModel import Word2VecModel
from utils.DLCustomModels import create_bgru_model, create_blstm_model
from utils.utils import getDataset


def run_all(VUL_TYPE, vectorsALLdir='ALL_vectors_granular_vulnerable_only', MODEL_TYPE='bgru', model_fn=create_bgru_model):
    RANDOMSEED = 1099
    CLASS_TYPE = '%s' % VUL_TYPE
    VECTOR_TRANSFORMER='w2v'
    LAYERS = 2
    DROPOUT = 0.2
    BATCHSIZE = 64
    EPOCHS = 60

    inputsRootPath = os.path.join('data', 'DLinputs')
    vectorRootPath = os.path.join('data','DLvectors')
    vectorsALLPath = os.path.join(vectorRootPath, vectorsALLdir)
    # Rather than taking up memory by creating separate folders for each type, 
    # we can overwrite each one since the upsampled collection is < the fullset.
    vectorTrainPath = os.path.join(vectorRootPath, f'train_upsampled')
    vectorTestPath = os.path.join(vectorRootPath, f'test_upsampled')
    dlInputsTrainPath = os.path.join(inputsRootPath, f'train_upsampled')
    dlInputsTestPath = os.path.join(inputsRootPath, f'test_upsampled')

    checkpoint_dir = './ckpt_%s_%s_%s_%s' % (VECTOR_TRANSFORMER, MODEL_TYPE, str(BATCHSIZE), CLASS_TYPE)
    model_name = '%s_%s_batch=%s_seed=%s_epochs=%s_%s' % (MODEL_TYPE.upper(), VECTOR_TRANSFORMER.upper(), BATCHSIZE, RANDOMSEED, EPOCHS, CLASS_TYPE)
    metrics_path = os.path.join(f'metrics', MODEL_TYPE)
    weights_path = os.path.join('model', model_name + 'weights')
    w2vmodelPath = os.path.join('w2vModel','model', f'w2vModel_{VUL_TYPE}')
    tokens_path = os.path.join('data', 'token', VUL_TYPE)

    dvt = DetectVulType(build_model=model_fn, useGenerator=True, 
        transformer_model=Word2VecModel, transformerPath=w2vmodelPath, 
        metricsPath=metrics_path, randomSeed=RANDOMSEED, window=3, m_epochs=EPOCHS, 
        modelName=model_name, batch_size=BATCHSIZE, mask=True, dropout=DROPOUT, layers=LAYERS,
        tokensPath=tokens_path, vectorsALLPath=vectorsALLPath, vectorRootPath=vectorRootPath, 
        vectorTrainPath=vectorTrainPath, vectorTestPath=vectorTestPath, 
        inputsTrainPath=dlInputsTrainPath, inputsTestPath=dlInputsTestPath,
        checkpoint_dir=checkpoint_dir, weightpath=weights_path)

    print('Starting %s & %s' % (CLASS_TYPE, MODEL_TYPE))

    # Reset if need be
    print('Resetting checkpoint and weights...')
    dvt.reset_checkpoint_and_weights(dvt.weightpath)

    # Split data into training and test set
    print('\nSplitting train/test...')
    dvt.splitTrainTest(dvt.vectorsALLPath)

    # Don't adjust the row length since they're already flattened. Manually set the new vector length & avg.
    data = getDataset(dvt.vectorTestPath, True)
    dvt.vector_size = len(data[0][0])
    dvt.avg = 1
    print(f'Avg: {dvt.avg}\tVector length: {dvt.vector_size}\n')
    del data
    gc.collect()

    # Build dl model & predict results
    dvt.encodeLabels()
    dvt.saveKeyData(dvt.vectorTrainPath, dvt.inputsTrainPath)
    dvt.saveKeyData(dvt.vectorTestPath, dvt.inputsTestPath)
    print('\nBuilding/fitting DL model...')
    dvt.build_and_fit()
    print('\nPredicting & Scoring...')
    dvt.predict_and_score()
    print('\n\n\n\n\n\n')


VUL_TYPE = 'ALL_vulnerable'
run_all(VUL_TYPE) # for testing: vectorsALLdir='SubSample_ALL_upsampled')

### BLSTM ###
run_all(VUL_TYPE, MODEL_TYPE='blstm', model_fn=create_blstm_model)
