#!/usr/bin/env python
# coding: utf-8

import gc, os, sys

import numpy as np
from sklearn.decomposition import PCA

VUL_PATH = os.environ.get('VUL_PATH')
sys.path.insert(1, VUL_PATH)

from SYSE_1_isVulnerable.preprocess_dl_Input_version5 import process_sequences_shape
from SYSE_2_vulnerabilityType.DetectVulType import DetectVulType
from utils.utils import flatten_categories, getDataset, save_data_to_file
from utils.DLCustomModels import create_bgru_model, create_blstm_model

################################## PCA on full dataset (Granular IDs) #############################
############################################ SET UP ############################################
# ## W2V + BGRU Granular
print('Starting PCA script for W2V & BGRU granular ids for 60 epochs...')
RANDOMSEED = 1099
CLASS_TYPE = 'Granular_PCA'
MODEL_TYPE = 'bgru'
VECTOR_TRANSFORMER='w2v'
OPTIMIZER = 'adam' 
LAYERS = 2
DROPOUT = 0.2 
BATCHSIZE = 64
EPOCHS = 60
TOKENS_PATH = os.path.join('data', 'tokens') # Contains ALL_tokens.pkl
vectorRootPath = os.path.join('data','DLvectors')
vectorsALLPath = os.path.join(vectorRootPath, 'ALL_vectors')
vectorTrainPath = os.path.join(vectorRootPath,'train')
vectorTestPath = os.path.join(vectorRootPath,'test')
dlInputs = os.path.join('data', 'DLinputs')
dlInputsTrainPath = os.path.join(dlInputs,'train')
dlInputsTestPath  = os.path.join(dlInputs,'test')
pca_vectors = os.path.join(vectorRootPath, 'PCA_vectors')
pca_vectors_train = os.path.join(vectorRootPath, 'PCA_train')
pca_vectors_test = os.path.join(vectorRootPath, 'PCA_test')
pca_inputs_train_path = os.path.join(dlInputs,'PCA_train')
pca_inputs_test_path = os.path.join(dlInputs,'PCA_test')

checkpoint_dir = './ckpt_%s_%s_%s_%s' % (VECTOR_TRANSFORMER, MODEL_TYPE, str(BATCHSIZE), CLASS_TYPE)
model_name = '%s_%s_batch=%s_seed=%s_epochs=%s_%s' % (MODEL_TYPE.upper(), VECTOR_TRANSFORMER.upper(), BATCHSIZE, RANDOMSEED, EPOCHS, CLASS_TYPE)
metrics_path = os.path.join(f'{VECTOR_TRANSFORMER}Model', 'metrics', MODEL_TYPE)
weights_path = os.path.join('model', model_name + OPTIMIZER + str(RANDOMSEED))

dvt = DetectVulType(build_model=create_bgru_model, useGenerator=True, layers=LAYERS,
                    metricsPath=metrics_path, randomSeed=RANDOMSEED,
                    m_epochs=EPOCHS, batch_size=BATCHSIZE, modelName=model_name,
                    mask=False, dropout=DROPOUT, optimizer=OPTIMIZER,
                    vectorsALLPath=vectorsALLPath, vectorRootPath=vectorRootPath, 
                    vectorTrainPath=vectorTrainPath, vectorTestPath=vectorTestPath, 
                    inputsTrainPath=dlInputsTrainPath, inputsTestPath=dlInputsTestPath, 
                    checkpoint_dir=checkpoint_dir, weightpath=weights_path)

# Save just group labels in own file
data = getDataset(TOKENS_PATH, False, RANDOMSEED)
data_filenames = data[3][:5]
print('First 5 filenames: ', data_filenames)
print('First 5 labels: ', data[-2][:5])
print('First 5 filename corpus id: ', data[-1][:5])
group_labels = flatten_categories(data[-2], 1)
group_labels_length = len(group_labels)
print('group labels length = ', group_labels_length)
print('unique labels: ', np.unique(group_labels))
del data
gc.collect()

# Overwrite Vector file with group labels
data = getDataset(vectorsALLPath, False)
print(f'VECTOR size in vectorsALLPath: ', len(data[0][0][0]))
data_filenames_vec = data[3][:5]
print('First 5 filenames: ', data_filenames_vec)
print('First 5 labels: ', data[-2][:5])
print('First 5 filename corpus id: ', data[-1][:5])
group_labels_length_vec = len(data[-2])
print('group labels length = ', group_labels_length_vec)
assert group_labels_length == group_labels_length_vec, f'number of classes in tokens group path {group_labels_length} doesnt match the ones in vectors all path {group_labels_length_vec}'
assert data_filenames == data_filenames_vec, f'first 5 filenames dont match.  Need to sort.'
data[-2] = group_labels
print('unique labels: ', np.unique(group_labels))
save_data_to_file(vectorsALLPath, 'ALL_vectors.pkl', data)
del data
gc.collect()


dvt.splitTrainTest(vectorsALLPath, 900)
dvt.adjustVectorLength()


# PCA transformation 
data = getDataset(dvt.inputsTrainPath, True, RANDOMSEED)

avg = dvt.avg
print(f'Avg: ', avg)
input_shape_1 = avg * VECTOR_SIZE
print("New vector size will be: 1x", str(input_shape_1))
# Convert to np arrays so we can reshape using numpy
x_train = process_sequences_shape(data[0], avg, VECTOR_SIZE)
# reshape from 3D to 2D so PCA() will accept
x_train = np.reshape(x_train, (x_train.shape[0], input_shape_1))
# go up to 80% of the feature length
n_stop = int((x_train.shape[1]/5)*4)
step = int(n_stop/10)
n_components = np.arange(100, n_stop, step)
print(f'N components to test: {n_components}')

# Test which n will retain as much of the data's explanation
explained_variance_ratios = []
for n in n_components:
    pca_model = PCA(n_components=n)
    pca_model.fit(x_train)
    sum_expl_var = sum(pca_model.explained_variance_ratio_)
    explained_variance_ratios.append(sum_expl_var)
    print(f' Sum of variance ratios for n={n}: ', sum_expl_var)
    if sum_expl_var >= 0.99:
        break











# ## W2V + BLSTM Granular
print('Starting PCA script for W2V & BLSTM granular ids for 60 epochs...')
RANDOMSEED = 1099
CLASS_TYPE = 'Granular_PCA'
MODEL_TYPE = 'blstm'
VECTOR_TRANSFORMER='w2v'
OPTIMIZER = 'adam' 
LAYERS = 2
DROPOUT = 0.2 
BATCHSIZE = 64
EPOCHS = 60
TOKENS_PATH = os.path.join('data', 'tokens') # Contains ALL_tokens.pkl
vectorRootPath = os.path.join('data','DLvectors')
vectorsALLPath = os.path.join(vectorRootPath, 'ALL_vectors')
vectorTrainPath = os.path.join(vectorRootPath,'train')
vectorTestPath = os.path.join(vectorRootPath,'test')
dlInputs = os.path.join('data', 'DLinputs')
dlInputsTrainPath = os.path.join(dlInputs,'train')
dlInputsTestPath  = os.path.join(dlInputs,'test')
pca_vectors = os.path.join(vectorRootPath, 'PCA_vectors')
pca_vectors_train = os.path.join(vectorRootPath, 'PCA_train')
pca_vectors_test = os.path.join(vectorRootPath, 'PCA_test')
pca_inputs_train_path = os.path.join(dlInputs,'PCA_train')
pca_inputs_test_path = os.path.join(dlInputs,'PCA_test')


data = getDataset(pca_vectors_train, True, RANDOMSEED)
categories = np.unique(data[-2])
_, labelEncoder = encode_target(categories)
DENSITY_UNITS = categories.shape[0]
VECTOR_SIZE = len(data[0][0])
print(f'Density units: {DENSITY_UNITS}. Vector size: {VECTOR_SIZE}')

checkpoint_dir = './ckpt_%s_%s_%s_%s' % (VECTOR_TRANSFORMER, MODEL_TYPE, str(BATCHSIZE), CLASS_TYPE)
model_name = '%s_%s_batch=%s_seed=%s_epochs=%s_%s' % (MODEL_TYPE.upper(), VECTOR_TRANSFORMER.upper(), BATCHSIZE, RANDOMSEED, EPOCHS, CLASS_TYPE)
metrics_path = os.path.join(f'{VECTOR_TRANSFORMER}Model', 'metrics', MODEL_TYPE)
weights_path = os.path.join('model', model_name + OPTIMIZER + str(RANDOMSEED))

dvt = DetectVulType(build_model=create_blstm_model, useGenerator=True, layers=LAYERS,
                    metricsPath=metrics_path, vector_size=VECTOR_SIZE, randomSeed=RANDOMSEED,
                    m_epochs=EPOCHS, batch_size=BATCHSIZE, modelName=model_name,
                    mask=False, dropout=DROPOUT, optimizer=OPTIMIZER,
                    vectorsALLPath=vectorsALLPath, vectorRootPath=vectorRootPath, 
                    vectorTrainPath=pca_vectors_train, vectorTestPath=pca_vectors_test, 
                    inputsTrainPath=pca_inputs_train_path, inputsTestPath=pca_inputs_test_path, 
                    checkpoint_dir=checkpoint_dir, weightpath=weights_path)
dvt.avg = 1
dvt.vector_size = VECTOR_SIZE
dvt.labelEncoder = labelEncoder
dvt.density_units = DENSITY_UNITS
dvt.build_and_fit()
dvt.predict_and_score()

