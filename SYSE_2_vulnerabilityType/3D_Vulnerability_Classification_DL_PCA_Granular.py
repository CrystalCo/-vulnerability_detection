#!/usr/bin/env python
# coding: utf-8

import gc, os, sys

import numpy as np
from sklearn.decomposition import PCA

VUL_PATH = os.environ.get('VUL_PATH')
sys.path.insert(1, VUL_PATH)

from SYSE_1_isVulnerable.preprocess_dl_Input_version5 import process_sequences_shape
from SYSE_2_vulnerabilityType.DetectVulType import DetectVulType
from utils.utils import flatten_categories, getDataset, save_data_to_file
from utils.DLCustomModels import create_bgru_model, create_blstm_model

################################## PCA on full dataset (Granular IDs) #############################
############################################ SET UP ############################################
# ## W2V + BGRU Granular
print('Starting PCA script for W2V & BGRU granular ids for 60 epochs...')
RANDOMSEED = 1099
CLASS_TYPE = 'Granular_PCA'
MODEL_TYPE = 'bgru'
VECTOR_TRANSFORMER='w2v'
OPTIMIZER = 'adam' 
LAYERS = 2
DROPOUT = 0.2 
BATCHSIZE = 64
EPOCHS = 60
TOKENS_PATH = os.path.join('data', 'tokens') # Contains ALL_tokens.pkl
vectorRootPath = os.path.join('data','DLvectors')
vectorsALLPath = os.path.join(vectorRootPath, 'ALL_vectors')
vectorTrainPath = os.path.join(vectorRootPath,'train')
vectorTestPath = os.path.join(vectorRootPath,'test')
dlInputs = os.path.join('data', 'DLinputs')
dlInputsTrainPath = os.path.join(dlInputs,'train')
dlInputsTestPath  = os.path.join(dlInputs,'test')
pca_vectors = os.path.join(vectorRootPath, 'PCA_vectors')
pca_vectors_train = os.path.join(vectorRootPath, 'PCA_train_granular')
pca_vectors_test = os.path.join(vectorRootPath, 'PCA_test_granular')
pca_inputs_train_path = os.path.join(dlInputs,'PCA_train_granular')
pca_inputs_test_path = os.path.join(dlInputs,'PCA_test_granular')

checkpoint_dir = './ckpt_%s_%s_%s_%s' % (VECTOR_TRANSFORMER, MODEL_TYPE, str(BATCHSIZE), CLASS_TYPE)
model_name = '%s_%s_batch=%s_seed=%s_epochs=%s_%s' % (MODEL_TYPE.upper(), VECTOR_TRANSFORMER.upper(), BATCHSIZE, RANDOMSEED, EPOCHS, CLASS_TYPE)
metrics_path = os.path.join(f'{VECTOR_TRANSFORMER}Model', 'metrics', MODEL_TYPE)
weights_path = os.path.join('model', model_name + OPTIMIZER + str(RANDOMSEED))

dvt = DetectVulType(build_model=create_bgru_model, useGenerator=True, layers=LAYERS,
                    metricsPath=metrics_path, randomSeed=RANDOMSEED,
                    m_epochs=EPOCHS, batch_size=BATCHSIZE, modelName=model_name,
                    mask=False, dropout=DROPOUT, optimizer=OPTIMIZER,
                    vectorsALLPath=vectorsALLPath, vectorRootPath=vectorRootPath, 
                    vectorTrainPath=vectorTrainPath, vectorTestPath=vectorTestPath, 
                    inputsTrainPath=dlInputsTrainPath, inputsTestPath=dlInputsTestPath, 
                    checkpoint_dir=checkpoint_dir, weightpath=weights_path)

# Save just group labels in own file
data = getDataset(TOKENS_PATH, False, RANDOMSEED)
data_filenames = data[3][:5]
print('First 5 filenames: ', data_filenames)
print('First 5 labels: ', data[-2][:5])
print('First 5 filename corpus id: ', data[-1][:5])
group_labels = flatten_categories(data[-2], 1)
group_labels_length = len(group_labels)
print('group labels length = ', group_labels_length)
print('unique labels: ', np.unique(group_labels))
del data
gc.collect()

# Overwrite Vector file with group labels
data = getDataset(vectorsALLPath, False)
print(f'VECTOR size in vectorsALLPath: ', len(data[0][0][0]))
data_filenames_vec = data[3][:5]
print('First 5 filenames: ', data_filenames_vec)
print('First 5 labels: ', data[-2][:5])
print('First 5 filename corpus id: ', data[-1][:5])
group_labels_length_vec = len(data[-2])
print('group labels length = ', group_labels_length_vec)
assert group_labels_length == group_labels_length_vec, f'number of classes in tokens group path {group_labels_length} doesnt match the ones in vectors all path {group_labels_length_vec}'
assert data_filenames == data_filenames_vec, f'first 5 filenames dont match.  Need to sort.'
data[-2] = group_labels
print('unique labels: ', np.unique(group_labels))
save_data_to_file(vectorsALLPath, 'ALL_vectors.pkl', data)
del data
gc.collect()


dvt.splitTrainTest(vectorsALLPath, 900) #--> vectortrain, vectortest
dvt.adjustVectorLength() #--> vectortrain, vectortest, inputtrain, inputtest


# Getting PCA transformed feature length from DLinputs/PCA_train since we already did the math for it in 3DGroups.py
data = getDataset(os.path.join(dlInputs,'PCA_test'))
input_shape_1 = len(data[0][0][0])
print("New vector size will be: 1x", str(input_shape_1))

# PCA transformation on train set
data = getDataset(dvt.inputsTrainPath, True, RANDOMSEED)
VECTOR_SIZE = len(data[0][0][0])
try:
    x_train = np.array(data[0]) # should convert to 3d np array shape b/c data in inputsTrainPath should be equal after lengths adjusted
    print(f'Shape of x_train: ', x_train.shape)
    print(f'Avg: {dvt.avg}. Vector size: {VECTOR_SIZE}')
    assert x_train.shape[0] == dvt.avg, f'x_train converted to array, but 2nd dimension avg {x_train.shape[0]} != calculated avg {dvt.avg}'
    assert x_train.shape[1] == VECTOR_SIZE, f'x_train converted to array, but 3rd dimension vector size {x_train.shape[1]} != calculated {VECTOR_SIZE}'
except Exception as e:
    print(e)
    print('x_train to array did not reshape properly. Might mean our data is ragged and should be checked bc at this point the row lengths should be the same (avg). Will try process_sequences_shape next, but lets confirm our shapes are even:')
    print(f'Len data: {len(x_train)} should be # of samples')
    print(f'Len data[0]: {len(x_train[0])} should be # of avg')
    print(f'Len data[0][0]: {len(x_train[0][0])} should be # of features (VECTOR SIZE)')
    print(f'Len data[0][1]: {len(x_train[0][1])} should be # of features (VECTOR SIZE)')
    x_train = process_sequences_shape(data[0], dvt.avg, VECTOR_SIZE)
x_train = np.reshape(x_train, (x_train.shape[0], input_shape_1))
pca_model = PCA(n_components=input_shape_1)
pca_model.fit(x_train)
x_comps = pca_model.transform(x_train)
print(x_comps.shape)
data[0] = x_comps
save_data_to_file(pca_vectors_train, 'DL_Final_balanced_train.pkl', data)


# Fit PCA & Transform our test vectors
data = getDataset(dvt.inputsTestPath, True, RANDOMSEED)
x_test = process_sequences_shape(data[0], dvt.avg, VECTOR_SIZE)
x_test = np.reshape(x_test, (x_test.shape[0], input_shape_1))
pca_model = PCA(n_components=input_shape_1)
pca_model.fit(x_test)
x_comps = pca_model.transform(x_test)
print(x_comps.shape)
data[0] = x_comps
save_data_to_file(pca_vectors_test, 'DL_Final_balanced_test.pkl', data)


dvt.vectorTrainPath = pca_vectors_train
dvt.vectorTestPath = pca_vectors_test
dvt.inputsTrainPath = pca_inputs_train_path
dvt.inputsTestPath = pca_inputs_test_path
dvt.encodeLabels()
dvt.saveKeyData(dvt.vectorTrainPath, dvt.inputsTrainPath)
dvt.saveKeyData(dvt.vectorTestPath, dvt.inputsTestPath)
dvt.vector_size = VECTOR_SIZE
dvt.avg = 1
print(f'Density units: {dvt.density_units}. Vector size: {dvt.vector_size}')
dvt.reset_check_weights(weights_path)
dvt.build_and_fit()
dvt.predict_and_score()







# ## W2V + BLSTM Granular
print('Starting PCA script for W2V & BLSTM granular ids for 60 epochs...')
RANDOMSEED = 1099
CLASS_TYPE = 'Granular_PCA'
MODEL_TYPE = 'blstm'
VECTOR_TRANSFORMER='w2v'
OPTIMIZER = 'adam' 
LAYERS = 2
DROPOUT = 0.2 
BATCHSIZE = 64
EPOCHS = 60
TOKENS_PATH = os.path.join('data', 'tokens') # Contains ALL_tokens.pkl
vectorRootPath = os.path.join('data','DLvectors')
vectorsALLPath = os.path.join(vectorRootPath, 'ALL_vectors')
vectorTrainPath = os.path.join(vectorRootPath,'train')
vectorTestPath = os.path.join(vectorRootPath,'test')
dlInputs = os.path.join('data', 'DLinputs')
dlInputsTrainPath = os.path.join(dlInputs,'train')
dlInputsTestPath  = os.path.join(dlInputs,'test')
pca_vectors = os.path.join(vectorRootPath, 'PCA_vectors')
pca_vectors_train = os.path.join(vectorRootPath, 'PCA_train')
pca_vectors_test = os.path.join(vectorRootPath, 'PCA_test')
pca_inputs_train_path = os.path.join(dlInputs, 'PCA_train_granular')
pca_inputs_test_path = os.path.join(dlInputs, 'PCA_test_granular')

checkpoint_dir = './ckpt_%s_%s_%s_%s' % (VECTOR_TRANSFORMER, MODEL_TYPE, str(BATCHSIZE), CLASS_TYPE)
model_name = '%s_%s_batch=%s_seed=%s_epochs=%s_%s' % (MODEL_TYPE.upper(), VECTOR_TRANSFORMER.upper(), BATCHSIZE, RANDOMSEED, EPOCHS, CLASS_TYPE)
metrics_path = os.path.join(f'{VECTOR_TRANSFORMER}Model', 'metrics', MODEL_TYPE)
weights_path = os.path.join('model', model_name + OPTIMIZER + str(RANDOMSEED))

dvt = DetectVulType(build_model=create_blstm_model, useGenerator=True, layers=LAYERS,
                    metricsPath=metrics_path, randomSeed=RANDOMSEED,
                    m_epochs=EPOCHS, batch_size=BATCHSIZE, modelName=model_name,
                    mask=False, dropout=DROPOUT, optimizer=OPTIMIZER,
                    vectorsALLPath=vectorsALLPath, vectorRootPath=vectorRootPath, 
                    vectorTrainPath=pca_vectors_train, vectorTestPath=pca_vectors_test, 
                    inputsTrainPath=pca_inputs_train_path, inputsTestPath=pca_inputs_test_path, 
                    checkpoint_dir=checkpoint_dir, weightpath=weights_path)
dvt.encodeLabels() # requires vectorTrainPath
dvt.vector_size = VECTOR_SIZE
dvt.avg = 1
print(f'Density units: {dvt.density_units}. Vector size: {dvt.vector_size}')
dvt.reset_check_weights(weights_path)
dvt.build_and_fit() # requires inputsTrainPath
dvt.predict_and_score()

