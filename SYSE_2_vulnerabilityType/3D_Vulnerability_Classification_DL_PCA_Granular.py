#!/usr/bin/env python
# coding: utf-8

import gc, os, sys

import numpy as np
from sklearn.decomposition import PCA

VUL_PATH = os.environ.get('VUL_PATH')
sys.path.insert(1, VUL_PATH)

from SYSE_1_isVulnerable.preprocess_dl_Input_version5 import process_sequences_shape
from SYSE_2_vulnerabilityType.DetectVulType import DetectVulType
from utils.utils import flatten_categories, getDataset, save_data_to_file
from utils.DLCustomModels import create_bgru_model, create_blstm_model

################################## PCA on full dataset (Granular IDs) #############################
############################################ SET UP ############################################
# ## W2V + BGRU Granular
print('Starting PCA script for W2V & BGRU granular ids for 60 epochs...')
RANDOMSEED = 1099
CLASS_TYPE = 'Granular_PCA'
MODEL_TYPE = 'bgru'
VECTOR_TRANSFORMER='w2v'
OPTIMIZER = 'adam' 
LAYERS = 2
DROPOUT = 0.2 
BATCHSIZE = 64
EPOCHS = 60
TOKENS_PATH = os.path.join('data', 'tokens') # Contains ALL_tokens.pkl
vectorRootPath = os.path.join('data','DLvectors')
vectorsALLPath = os.path.join(vectorRootPath, 'ALL_vectors')
vectorTrainPath = os.path.join(vectorRootPath,'train')
vectorTestPath = os.path.join(vectorRootPath,'test')
dlInputs = os.path.join('data', 'DLinputs')
dlInputsTrainPath = os.path.join(dlInputs,'train')
dlInputsTestPath  = os.path.join(dlInputs,'test')
pca_vectors = os.path.join(vectorRootPath, 'PCA_vectors')
pca_vectors_train = os.path.join(vectorRootPath, 'PCA_train_granular')
pca_vectors_test = os.path.join(vectorRootPath, 'PCA_test_granular')
pca_inputs_train_path = os.path.join(dlInputs,'PCA_train_granular')
pca_inputs_test_path = os.path.join(dlInputs,'PCA_test_granular')

checkpoint_dir = './ckpt_%s_%s_%s_%s' % (VECTOR_TRANSFORMER, MODEL_TYPE, str(BATCHSIZE), CLASS_TYPE)
model_name = '%s_%s_batch=%s_seed=%s_epochs=%s_%s' % (MODEL_TYPE.upper(), VECTOR_TRANSFORMER.upper(), BATCHSIZE, RANDOMSEED, EPOCHS, CLASS_TYPE)
metrics_path = os.path.join(f'{VECTOR_TRANSFORMER}Model', 'metrics', MODEL_TYPE)
weights_path = os.path.join('model', model_name + OPTIMIZER + str(RANDOMSEED))

dvt = DetectVulType(build_model=create_bgru_model, useGenerator=True, layers=LAYERS,
                    metricsPath=metrics_path, randomSeed=RANDOMSEED,
                    m_epochs=EPOCHS, batch_size=BATCHSIZE, modelName=model_name,
                    mask=False, dropout=DROPOUT, optimizer=OPTIMIZER,
                    vectorsALLPath=vectorsALLPath, vectorRootPath=vectorRootPath, 
                    vectorTrainPath=vectorTrainPath, vectorTestPath=vectorTestPath, 
                    inputsTrainPath=dlInputsTrainPath, inputsTestPath=dlInputsTestPath, 
                    checkpoint_dir=checkpoint_dir, weightpath=weights_path)

# Save just group labels in own file
# data = getDataset(TOKENS_PATH, False, RANDOMSEED)
# data_filenames = data[3][:5]
# print('First 5 filenames: ', data_filenames)
# print('First 5 labels: ', data[-2][:5])
# print('First 5 filename corpus id: ', data[-1][:5])
# group_labels = flatten_categories(data[-2], 1)
# group_labels_length = len(group_labels)
# print('group labels length = ', group_labels_length)
# print('unique labels: ', np.unique(group_labels))
# del data
# gc.collect()

# # Overwrite Vector file with group labels
# data = getDataset(vectorsALLPath, False)
# print(f'VECTOR size in vectorsALLPath: ', len(data[0][0][0]))
# data_filenames_vec = data[3][:5]
# print('First 5 filenames: ', data_filenames_vec)
# print('First 5 labels: ', data[-2][:5])
# print('First 5 filename corpus id: ', data[-1][:5])
# group_labels_length_vec = len(data[-2])
# print('group labels length = ', group_labels_length_vec)
# assert group_labels_length == group_labels_length_vec, f'number of classes in tokens group path {group_labels_length} doesnt match the ones in vectors all path {group_labels_length_vec}'
# assert data_filenames == data_filenames_vec, f'first 5 filenames dont match.  Need to sort.'
# data[-2] = group_labels
# print('unique labels: ', np.unique(group_labels))
# save_data_to_file(vectorsALLPath, 'ALL_vectors.pkl', data)
# del data
# gc.collect()


dvt.splitTrainTest(vectorsALLPath, 900) #--> vectortrain, vectortest
dvt.adjustVectorLength() #--> vectortrain, vectortest, inputtrain, inputtest


# PCA transformation on train set
data = getDataset(dvt.inputsTrainPath, True, RANDOMSEED)
VECTOR_SIZE = len(data[0][0][0])
input_shape_1 = dvt.avg * VECTOR_SIZE
print(f'Avg: {dvt.avg}. Vector size: {VECTOR_SIZE}')
print("New vector size will be: 1x", str(input_shape_1))
try:
    x_train = np.array(data[0]) # should convert to 3d np array shape b/c data in inputsTrainPath should be equal after lengths adjusted
    print(f'Length of dataset:  {len(data[0])}.  Shape of x_train:  {x_train.shape}')
    print(f'Avg: {dvt.avg}. Vector size: {VECTOR_SIZE}')
    x_train = np.reshape(x_train, (x_train.shape[0], input_shape_1))
except Exception as e:
    print(e)
    print('x_train to array did not reshape properly. Might mean our data is ragged and should be checked bc at this point the row lengths should be the same (avg). Will try process_sequences_shape next, but lets confirm our shapes are even:')
    print('x_train to array did not reshape properly. Might mean our data is ragged and should be checked bc at this point the row lengths should be the same (avg). Will try process_sequences_shape next, but lets confirm our shapes are even:')
    print(f'Len data: {len(x_train)} should be # of samples')
    print(f'Len data[0]: {len(x_train[0])} should be # of vector size')
    x_train = process_sequences_shape(data[0], dvt.avg, VECTOR_SIZE)
    x_train = np.reshape(x_train, (x_train.shape[0], input_shape_1))
n_stop = int((x_train.shape[1]/5)*4)
step = int(n_stop/10)
n_components = np.arange(100, n_stop, step)
print(f'N components to test: {n_components}')

# Test which n will retain as much of the data's explanation
explained_variance_ratios = []
for n in n_components:
    pca_model = PCA(n_components=n)
    pca_model.fit(x_train)
    sum_expl_var = sum(pca_model.explained_variance_ratio_)
    explained_variance_ratios.append(sum_expl_var)
    print(f' Sum of variance ratios for n={n}: ', sum_expl_var)
    if sum_expl_var >= 0.99:
        break

n_index = len(explained_variance_ratios)-1 if len(explained_variance_ratios) > 0 else -1
print(f'Choosing n={n_components[n_index]} since that still leaves us with about {explained_variance_ratios[n_index]}% variance.')

pca_model = PCA(n_components=n_components[n_index])
pca_model.fit(x_train)
x_comps = pca_model.transform(x_train)
print(x_comps.shape)
data[0] = x_comps
save_data_to_file(pca_vectors_train, 'DL_Final_balanced_train.pkl', data)


# Fit PCA & Transform our test vectors
data = getDataset(dvt.inputsTestPath, True, RANDOMSEED)
try:
    x_test = np.array(data[0]) # should convert to 3d np array shape b/c data in inputsTrainPath should be equal after lengths adjusted
    print(f'Length of dataset:  {len(data[0])}.  Shape of x_test:  {x_test.shape}')
    print(f'Avg: {dvt.avg}. Vector size: {VECTOR_SIZE}')
except Exception as e:
    print(e)
    print('x_test to array did not reshape properly. Might mean our data is ragged and should be checked bc at this point the row lengths should be the same (avg). Will try process_sequences_shape next, but lets confirm our shapes are even:')
    print('x_test to array did not reshape properly. Might mean our data is ragged and should be checked bc at this point the row lengths should be the same (avg). Will try process_sequences_shape next, but lets confirm our shapes are even:')
    print(f'Len data: {len(x_test)} should be # of samples')
    print(f'Len data[0]: {len(x_test[0])} should be # of vector size')
    x_test = process_sequences_shape(data[0], dvt.avg, VECTOR_SIZE)
x_test = np.reshape(x_test, (x_test.shape[0], input_shape_1))
pca_model = PCA(n_components=n_components[n_index])
pca_model.fit(x_test)
x_comps = pca_model.transform(x_test)
print(x_comps.shape)
data[0] = x_comps
save_data_to_file(pca_vectors_test, 'DL_Final_balanced_test.pkl', data)


dvt.vectorTrainPath = pca_vectors_train
dvt.vectorTestPath = pca_vectors_test
dvt.inputsTrainPath = pca_inputs_train_path
dvt.inputsTestPath = pca_inputs_test_path
dvt.encodeLabels()
dvt.saveKeyData(dvt.vectorTrainPath, dvt.inputsTrainPath)
dvt.saveKeyData(dvt.vectorTestPath, dvt.inputsTestPath)
VECTOR_SIZE = n_components[n_index]
dvt.vector_size = VECTOR_SIZE
dvt.avg = 1
print(f'Density units: {dvt.density_units}. Vector size: {dvt.vector_size}')
dvt.reset_check_weights(weights_path)
dvt.build_and_fit()
dvt.predict_and_score()







# ## W2V + BLSTM Granular
print('Starting PCA script for W2V & BLSTM granular ids for 60 epochs...')
RANDOMSEED = 1099
CLASS_TYPE = 'Granular_PCA'
MODEL_TYPE = 'blstm'
VECTOR_TRANSFORMER='w2v'
OPTIMIZER = 'adam' 
LAYERS = 2
DROPOUT = 0.2 
BATCHSIZE = 64
EPOCHS = 60
TOKENS_PATH = os.path.join('data', 'tokens') # Contains ALL_tokens.pkl
vectorRootPath = os.path.join('data','DLvectors')
vectorsALLPath = os.path.join(vectorRootPath, 'ALL_vectors')
vectorTrainPath = os.path.join(vectorRootPath,'train')
vectorTestPath = os.path.join(vectorRootPath,'test')
dlInputs = os.path.join('data', 'DLinputs')
dlInputsTrainPath = os.path.join(dlInputs,'train')
dlInputsTestPath  = os.path.join(dlInputs,'test')
pca_vectors = os.path.join(vectorRootPath, 'PCA_vectors')
pca_vectors_train = os.path.join(vectorRootPath, 'PCA_train_granular')
pca_vectors_test = os.path.join(vectorRootPath, 'PCA_test_granular')
pca_inputs_train_path = os.path.join(dlInputs, 'PCA_train_granular')
pca_inputs_test_path = os.path.join(dlInputs, 'PCA_test_granular')

checkpoint_dir = './ckpt_%s_%s_%s_%s' % (VECTOR_TRANSFORMER, MODEL_TYPE, str(BATCHSIZE), CLASS_TYPE)
model_name = '%s_%s_batch=%s_seed=%s_epochs=%s_%s' % (MODEL_TYPE.upper(), VECTOR_TRANSFORMER.upper(), BATCHSIZE, RANDOMSEED, EPOCHS, CLASS_TYPE)
metrics_path = os.path.join(f'{VECTOR_TRANSFORMER}Model', 'metrics', MODEL_TYPE)
weights_path = os.path.join('model', model_name + OPTIMIZER + str(RANDOMSEED))

dvt = DetectVulType(build_model=create_blstm_model, useGenerator=True, layers=LAYERS,
                    metricsPath=metrics_path, randomSeed=RANDOMSEED,
                    m_epochs=EPOCHS, batch_size=BATCHSIZE, modelName=model_name,
                    mask=False, dropout=DROPOUT, optimizer=OPTIMIZER,
                    vectorsALLPath=vectorsALLPath, vectorRootPath=vectorRootPath, 
                    vectorTrainPath=pca_vectors_train, vectorTestPath=pca_vectors_test, 
                    inputsTrainPath=pca_inputs_train_path, inputsTestPath=pca_inputs_test_path, 
                    checkpoint_dir=checkpoint_dir, weightpath=weights_path)
dvt.encodeLabels() # requires vectorTrainPath
dvt.vector_size = VECTOR_SIZE
dvt.avg = 1
print(f'Density units: {dvt.density_units}. Vector size: {dvt.vector_size}')
dvt.reset_check_weights(weights_path)
dvt.build_and_fit() # requires inputsTrainPath
dvt.predict_and_score()


