#!/usr/bin/env python
# coding: utf-8

import os, sys

import numpy as np
from sklearn.decomposition import PCA
import tensorflow as tf
from tensorflow import keras

VUL_PATH = os.environ.get('VUL_PATH')
sys.path.insert(1, VUL_PATH)

from SYSE_1_isVulnerable.preprocess_dl_Input_version5 import process_sequences_shape
from SYSE_1_isVulnerable.splitTrainTest import splitTrainTestCategorical
from SYSE_1_isVulnerable.adjustVectorLen import meanLen
from SYSE_1_isVulnerable.saveKeyData import saveKeyDataMulticlass
from SYSE_1_isVulnerable.ConfusionMatrix import getConfusionMatrix_Multiclass
from SYSE_2_vulnerabilityType.DetectVulType import DetectVulType
from utils.utils import encode_target, flatten_categories, getDataset, save_data_to_file
from utils.DLCustomModels import create_bgru_model, create_blstm_model, fit_custom_dl_model, make_or_restore_model, predictMulticlassLabel

################################## PCA on full dataset (Group IDs) #############################
############################################ SET UP ############################################
RANDOMSEED = 1099
VECTOR_SIZE = 30 #num of vector cols
TOKENS_PATH = os.path.join('data', 'tokens') # Contains ALL_tokens.pkl
vectorRootPath = os.path.join('data','DLvectors')
vectorsALLPath = os.path.join(vectorRootPath, 'ALL_vectors')
vectorDownsampledPath = os.path.join(vectorRootPath, 'Downsampled_vectors')
vectorTrainPath = os.path.join(vectorRootPath,'train')
vectorTestPath = os.path.join(vectorRootPath,'test')
dlInputsTrainPath = os.path.join('data','DLinputs','train')
dlInputsTestPath  = os.path.join('data','DLinputs','test')
 
## W2V + BGRU Granular IDs 
# Reset vector train & test files
print('Starting PCA script...')
data = getDataset(vectorsALLPath, False, RANDOMSEED)
original_labels = data[-2]
categories = np.unique(original_labels)
_, labelEncoder = encode_target(categories)
DENSITY_UNITS = categories.shape[0]

# Flatten W2V matrices from 3D to 2D for PCA transformation
avg = meanLen(vectorTrainPath)
input_shape_1 = avg * VECTOR_SIZE
print("New vector size: ", str(input_shape_1))
x_train = process_sequences_shape(data[0], avg, VECTOR_SIZE)
x_train = np.reshape(x_train, (x_train.shape[0], input_shape_1))
# go up to 75% of the feature length
n_stop = int((x_train.shape[1]/4)*3)
step = int(n_stop/10)
n_components = np.arange(100, n_stop, step)
print(f'N components to test: {n_components}')

# Test which n will retain as much of the data's explanation
explained_variance_ratios = []
for n in n_components:
    pca_model = PCA(n_components=n)
    pca_model.fit(x_train)
    sum_expl_var = sum(pca_model.explained_variance_ratio_)
    explained_variance_ratios.append(sum_expl_var)
    print(f' Sum of variance ratios for n={n}: ', sum_expl_var)
    if sum_expl_var >= 0.99:
        break

n_index = len(explained_variance_ratios)-1 if len(explained_variance_ratios) > 0 else -1
print(f'Choosing n={n_components[n_index]} since that still leaves us with about {explained_variance_ratios[n_index]}% variance.')

# Fit PCA & Transform our vectors
pca_model = PCA(n_components=n_components[n_index])
pca_model.fit(x_train)
x_comps = pca_model.transform(x_train)
print(x_comps.shape)

# Save PCA transformed data
pca_vectors = os.path.join(vectorRootPath, 'PCA_vectors')
pca_vectors_train = os.path.join(vectorRootPath, 'PCA_train')
pca_vectors_test = os.path.join(vectorRootPath, 'PCA_test')
pca_inputs_train_path = os.path.join('data', 'DLinputs', 'PCA_train')
pca_inputs_test_path = os.path.join('data', 'DLinputs', 'PCA_test')
data[0] = x_comps
save_data_to_file(pca_vectors, 'PCA_vectors_balanced.pkl', data)
splitTrainTestCategorical('balanced', pca_vectors, pca_vectors_train,
                          pca_vectors_test, randomSeed=0)
saveKeyDataMulticlass(pca_vectors_train, labelEncoder, pca_inputs_train_path)
saveKeyDataMulticlass(pca_vectors_test, labelEncoder, pca_inputs_test_path)

# Build BGRU model
OPTIMIZER = 'adam' #can be changed to ‘adamax’
LAYERS = 2
DROPOUT = 0.2 
BATCHSIZE = 64
EPOCHS = 20
# Update our dimensions since we have a flattened dataset now
avg = 1
VECTOR_SIZE = x_comps.shape[1]

CLASS_TYPE = 'Granular_PCA'
VECTOR_TRANSFORMER='w2v'
MODEL_TYPE = 'bgru'
checkpoint_dir = './ckpt_%s_%s_%s_%s' % (VECTOR_TRANSFORMER, MODEL_TYPE, str(BATCHSIZE), CLASS_TYPE)
model_name = '%s_%s_batch=%s_seed=%s_%s' % (MODEL_TYPE.upper(), VECTOR_TRANSFORMER.upper(), BATCHSIZE, RANDOMSEED, CLASS_TYPE)
metrics_path = os.path.join(f'{VECTOR_TRANSFORMER}Model','metrics', MODEL_TYPE)
weightpath = os.path.join('model', model_name + OPTIMIZER + str(RANDOMSEED))
dvt = DetectVulType(create_bgru_model, m_epochs=EPOCHS, batch_size=BATCHSIZE, mask=False,
                    dropout=DROPOUT, vectorTrainPath=pca_vectors_train, metricsPath=metrics_path,
                    vectorTestPath=pca_vectors_test, inputsTrainPath=pca_inputs_train_path,
                    inputsTestPath=pca_inputs_test_path, checkpoint_dir=checkpoint_dir)
dvt.avg = avg
dvt.vector_size = VECTOR_SIZE
dvt.density_units = DENSITY_UNITS

if not os.path.exists(checkpoint_dir):
    os.makedirs(checkpoint_dir)

# Reduces memory usage by restricting TF to use only 1 GPU (the one not in use by the server atm)
gpus = tf.config.experimental.list_physical_devices('GPU')
for gpu in gpus:
    tf.config.experimental.set_memory_growth(gpu, True)

# Open a strategy scope
strategy = tf.distribute.MirroredStrategy(['GPU:2'])
print('\nNumber of devices: {}'.format(strategy.num_replicas_in_sync))
with strategy.scope():
    latest_epoch, myKerasModel = make_or_restore_model(checkpoint_dir, dvt.get_compiled_model)

callbacks = [
    keras.callbacks.ModelCheckpoint(
        filepath=os.path.join(checkpoint_dir, 'ckpt-{epoch}'),
        monitor='val_acc',
        verbose=1,
        save_freq='epoch'
    )
]

# Fit
model = fit_custom_dl_model(myKerasModel, weightpath, pca_inputs_train_path, BATCHSIZE, avg, VECTOR_SIZE, RANDOMSEED, EPOCHS, useGenerator=True, callbacks=callbacks, latest_epoch=latest_epoch)

# Predict
model.load_weights(weightpath)
_, mypredicted_labels, myreallabels, _ = predictMulticlassLabel(model, pca_inputs_test_path,
                                                                avg, VECTOR_SIZE, OPTIMIZER,
                                                                model_name, RANDOMSEED, labelEncoder)
# Confusion matrix for each category.
getConfusionMatrix_Multiclass(mypredicted_labels, myreallabels, saveFig=True, path=metrics_path, modelName=model_name)


# ## W2V + BLSTM Granular IDs 
# We dont' need to reset the vector train/test files again since already did it for W2V+BGRU above
CLASS_TYPE = 'Granular_PCA'
MODEL_TYPE = 'blstm'
VECTOR_TRANSFORMER='w2v'
checkpoint_dir = './ckpt_%s_%s_%s_%s' % (VECTOR_TRANSFORMER, MODEL_TYPE, str(BATCHSIZE), CLASS_TYPE)
model_name = '%s_%s_batch=%s_seed=%s_%s' % (MODEL_TYPE.upper(), VECTOR_TRANSFORMER.upper(), BATCHSIZE, RANDOMSEED, CLASS_TYPE)
metrics_path = os.path.join(f'{VECTOR_TRANSFORMER}Model','metrics', MODEL_TYPE)
weightpath = os.path.join('model', model_name + OPTIMIZER + str(RANDOMSEED))
dvt = DetectVulType(create_blstm_model, m_epochs=EPOCHS, batch_size=BATCHSIZE, mask=False,
                    dropout=DROPOUT, vectorTrainPath=pca_vectors_train, metricsPath=metrics_path,
                    vectorTestPath=pca_vectors_test, inputsTrainPath=pca_inputs_train_path,
                    inputsTestPath=pca_inputs_test_path, checkpoint_dir=checkpoint_dir)
dvt.avg = avg
dvt.vector_size = VECTOR_SIZE
dvt.density_units = DENSITY_UNITS

if not os.path.exists(checkpoint_dir):
    os.makedirs(checkpoint_dir)

# Reduces memory usage by restricting TF to use only 1 GPU (the one not in use by the server atm)
gpus = tf.config.experimental.list_physical_devices('GPU')
for gpu in gpus:
    tf.config.experimental.set_memory_growth(gpu, True)

# Open a strategy scope
strategy = tf.distribute.MirroredStrategy(['GPU:2'])
print('\nNumber of devices: {}'.format(strategy.num_replicas_in_sync))
with strategy.scope():
    latest_epoch, myKerasModel = make_or_restore_model(checkpoint_dir, dvt.get_compiled_model)

callbacks = [
    keras.callbacks.ModelCheckpoint(
        filepath=os.path.join(checkpoint_dir, 'ckpt-{epoch}'),
        monitor='val_acc',
        verbose=1,
        save_freq='epoch'
    )
]

# Fit
model = fit_custom_dl_model(myKerasModel, weightpath, pca_inputs_train_path, BATCHSIZE, avg, VECTOR_SIZE, RANDOMSEED, EPOCHS, useGenerator=True, callbacks=callbacks, latest_epoch=latest_epoch)

# Predict
model.load_weights(weightpath)
_, mypredicted_labels, myreallabels, _ = predictMulticlassLabel(model, pca_inputs_test_path,
                                                                avg, VECTOR_SIZE, OPTIMIZER,
                                                                model_name, RANDOMSEED, labelEncoder)
# Confusion matrix for each category.
getConfusionMatrix_Multiclass(mypredicted_labels, myreallabels, saveFig=True, path=metrics_path, modelName=model_name)

# Don't forget to run plot_results.py for plots


# ## D2V + BGRU Granular IDs 
print('Starting PCA script for D2V & BGRU...')
vectorRootPath = os.path.join('data','D2Vectors')
vectorsALLPath = os.path.join(vectorRootPath, 'ALL_vectors')
vectorTrainPath = os.path.join(vectorRootPath,'train')
vectorTestPath = os.path.join(vectorRootPath,'test')
d2InputsTrainPath = os.path.join('data','D2inputs','train')
d2InputsTestPath  = os.path.join('data','D2inputs','test')
pca_vectors = os.path.join(vectorRootPath, 'PCA_vectors')
pca_vectors_train = os.path.join(vectorRootPath, 'PCA_train')
pca_vectors_test = os.path.join(vectorRootPath, 'PCA_test')
pca_inputs_train_path = os.path.join('data', 'D2inputs', 'PCA_train')
pca_inputs_test_path = os.path.join('data', 'D2inputs', 'PCA_test')

# PCA transformation
data = getDataset(vectorsALLPath, False, RANDOMSEED)
x_train = np.array(data[0])
print('D2V vector shape: ', data.shape)
# go up to 75% of the feature length
n_stop = int((x_train.shape[1]/4)*3)
step = int(n_stop/10)
n_components = np.arange(100, n_stop, step)
print(f'N components to test: {n_components}')

# Test which n will retain as much of the data's explanation
explained_variance_ratios = []
for n in n_components:
    pca_model = PCA(n_components=n)
    pca_model.fit(x_train)
    sum_expl_var = sum(pca_model.explained_variance_ratio_)
    explained_variance_ratios.append(sum_expl_var)
    print(f' Sum of variance ratios for n={n}: ', sum_expl_var)
    if sum_expl_var >= 0.99:
        break

# Fit PCA & Transform our vectors
pca_model = PCA(n_components=n_components[n_index])
pca_model.fit(x_train)
x_comps = pca_model.transform(x_train)
print(x_comps.shape)

CLASS_TYPE = 'Granular_PCA'
MODEL_TYPE = 'bgru'
VECTOR_TRANSFORMER = 'd2v'
avg = 1
VECTOR_SIZE = 256
checkpoint_dir = './ckpt_%s_%s_%s_%s' % (VECTOR_TRANSFORMER, MODEL_TYPE, str(BATCHSIZE), CLASS_TYPE)
model_name = '%s_%s_batch=%s_seed=%s_%s' % (MODEL_TYPE.upper(), VECTOR_TRANSFORMER.upper(), BATCHSIZE, RANDOMSEED, CLASS_TYPE)
metrics_path = os.path.join(f'{VECTOR_TRANSFORMER}Model','metrics', MODEL_TYPE)

dvt = DetectVulType(build_model=create_bgru_model, useGenerator=True, layers=LAYERS,
                    metricsPath=metrics_path, vector_size=VECTOR_SIZE, randomSeed=RANDOMSEED, 
                    window=3, m_epochs=EPOCHS, batch_size=BATCHSIZE, modelName=model_name, 
                    mask=False, tokensPath=TOKENS_PATH, dropout=DROPOUT, optimizer=OPTIMIZER,
                    vectorsALLPath=vectorsALLPath, vectorRootPath=vectorRootPath, 
                    vectorTrainPath=pca_vectors_train, vectorTestPath=pca_vectors_test, 
                    inputsTrainPath=pca_inputs_train_path, inputsTestPath=pca_inputs_test_path, 
                    checkpoint_dir=checkpoint_dir)

# Save PCA transformed data
data[0] = x_comps
save_data_to_file(pca_vectors, 'PCA_vectors_balanced.pkl', data)

print(f'BUILDING MODEL {model_name}')
dvt.avg = 1
dvt.vector_size = VECTOR_SIZE
dvt.labelEncoder = labelEncoder
dvt.density_units = DENSITY_UNITS
dvt.splitTrainTest(pca_vectors, minSamples=900)
dvt.saveKeyData(pca_vectors_train, pca_inputs_train_path)
dvt.saveKeyData(pca_vectors_test, pca_inputs_test_path)
dvt.build_and_fit()
dvt.predict_and_score()



# ## D2V + BLSTM Granular IDs 
CLASS_TYPE = 'Granular_PCA'
MODEL_TYPE = 'blstm'
checkpoint_dir = './ckpt_%s_%s_%s_%s' % (VECTOR_TRANSFORMER, MODEL_TYPE, str(BATCHSIZE), CLASS_TYPE)
model_name = '%s_%s_batch=%s_seed=%s_%s' % (MODEL_TYPE.upper(), VECTOR_TRANSFORMER.upper(), BATCHSIZE, RANDOMSEED, CLASS_TYPE)
metrics_path = os.path.join(f'{VECTOR_TRANSFORMER}Model','metrics', MODEL_TYPE)

dvt = DetectVulType(build_model=create_blstm_model, useGenerator=True, layers=LAYERS,
                    metricsPath=metrics_path, vector_size=VECTOR_SIZE, randomSeed=RANDOMSEED, 
                    m_epochs=EPOCHS, batch_size=BATCHSIZE, modelName=model_name, 
                    mask=False, tokensPath=TOKENS_PATH, dropout=DROPOUT, optimizer=OPTIMIZER,
                    vectorsALLPath=vectorsALLPath, vectorRootPath=vectorRootPath, 
                    vectorTrainPath=pca_vectors_train, vectorTestPath=pca_vectors_test, 
                    inputsTrainPath=pca_inputs_train_path, inputsTestPath=pca_inputs_test_path, 
                    checkpoint_dir=checkpoint_dir)

print(f'BUILDING MODEL {model_name}')
dvt.avg = 1
dvt.vector_size = VECTOR_SIZE
dvt.labelEncoder = labelEncoder
dvt.density_units = DENSITY_UNITS
dvt.build_and_fit()
dvt.predict_and_score()


