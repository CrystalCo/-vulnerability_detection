# import gc, os, sys

# import numpy as np

# VUL_PATH = os.environ.get('VUL_PATH')
# sys.path.insert(1, VUL_PATH)

# from SYSE_2_vulnerabilityType.DetectVulType import DetectVulType
# from utils.DLCustomModels import create_bgru_model
# from utils.MLMethods import convert_nested_lists_to_numpy_arrays
# from utils.utils import getDataset, save_data_to_file

# RANDOMSEED = 1099
# CLASS_TYPE = 'Granular_162'
# MODEL_TYPE = 'bgru'
# VECTOR_TRANSFORMER='w2v'
# LAYERS = 2
# DROPOUT = 0.2
# BATCHSIZE = 64
# EPOCHS = 60

# vectorRootPath = os.path.join('data', 'DLvectors')
# vectorTrainPath_flat = os.path.join(vectorRootPath,'train_162classes_flattened')
# vectorTestPath_flat = os.path.join(vectorRootPath,'test_162classes_flattened')
# inputRootPath = os.path.join('data', 'DLinputs')
# dlInputsTrainPath_flat = os.path.join(inputRootPath,'train_162classes_flattened')
# dlInputsTestPath_flat = os.path.join(inputRootPath,'test_162classes_flattened')


# checkpoint_dir = './ckpt_%s_%s_%s_%s' % (VECTOR_TRANSFORMER, MODEL_TYPE, str(BATCHSIZE), CLASS_TYPE)
# model_name = '%s_%s_batch=%s_seed=%s_epochs=%s_%s' % (MODEL_TYPE.upper(), VECTOR_TRANSFORMER.upper(), BATCHSIZE, RANDOMSEED, EPOCHS, CLASS_TYPE)
# metrics_path = os.path.join(f'{VECTOR_TRANSFORMER}Model', 'metrics', MODEL_TYPE)
# weights_path = os.path.join('model', model_name + 'weights')

# dvt = DetectVulType(build_model=create_bgru_model, metricsPath=metrics_path,
#                     randomSeed=RANDOMSEED, layers=LAYERS, weightpath=weights_path,
#                     batch_size=BATCHSIZE, m_epochs=EPOCHS, modelName=model_name, 
#                     vectorTrainPath=vectorTrainPath_flat, vectorTestPath=vectorTestPath_flat,
#                     inputsTrainPath=dlInputsTrainPath_flat, inputsTestPath=dlInputsTestPath_flat,
#                     checkpoint_dir=checkpoint_dir)

# data = getDataset(dvt.inputsTrainPath)
# VECTOR_SIZE = len(data[0][0])
# del data
# gc.collect()

# dvt.vector_size = VECTOR_SIZE
# dvt.avg = 1
# print(f'Avg: {dvt.avg} Vector: {VECTOR_SIZE}')
# dvt.encodeLabels()
# print(f'Density units: {dvt.density_units}. Vector size: {dvt.vector_size}')
# dvt.build_and_fit()
# dvt.predict_and_score()



import os, sys
import numpy as np
VUL_PATH = os.environ.get('VUL_PATH')
sys.path.insert(1, VUL_PATH)
from utils.utils import getDataset
vectorRootPath = os.path.join('data', 'DLvectors')
vectorTestPath_flat = os.path.join(vectorRootPath,'test_162classes_flattened')

FP = """0.000e+00 1.380e+02 0.000e+00 1.840e+02 1.660e+02 0.000e+00 1.242e+03
 8.000e+00 9.000e+00 3.780e+02 6.877e+03 2.000e+00 1.100e+01 0.000e+00
 2.800e+01 4.350e+02 3.820e+02 1.840e+02 3.580e+02 2.000e+00 5.880e+02
 3.800e+01 5.950e+02 2.980e+02 1.680e+02 8.820e+02 1.920e+02 0.000e+00
 0.000e+00 6.300e+01 3.760e+02 1.230e+02 1.000e+00 3.000e+02 3.220e+02
 1.960e+02 9.700e+01 1.450e+02 0.000e+00 0.000e+00 5.000e+00 1.000e+00
 1.000e+00 1.800e+01 1.600e+01 6.000e+00 1.000e+00 8.000e+00 4.000e+00
 1.000e+00 2.000e+00 6.000e+00 0.000e+00 2.300e+01 4.000e+00 4.000e+00
 0.000e+00 9.000e+00 4.000e+00 0.000e+00 0.000e+00 1.000e+00 1.100e+01
 2.460e+02 7.000e+00 3.000e+00 8.000e+01 8.500e+01 0.000e+00 4.000e+00
 0.000e+00 0.000e+00 1.520e+02 9.170e+02 4.900e+01 1.100e+01 4.600e+02
 2.400e+01 1.180e+02 7.800e+01 1.600e+01 1.100e+01 2.900e+01 7.460e+02
 0.000e+00 8.000e+00 1.000e+00 1.000e+00 0.000e+00 1.444e+03 1.000e+00
 3.200e+01 0.000e+00 1.000e+00 1.000e+00 4.000e+00 1.000e+00 0.000e+00
 1.000e+00 3.000e+00 5.300e+01 0.000e+00 0.000e+00 2.000e+00 0.000e+00
 2.000e+00 9.200e+01 3.000e+00 0.000e+00 1.160e+02 1.600e+02 0.000e+00
 3.100e+01 0.000e+00 5.700e+01 1.400e+01 6.000e+00 0.000e+00 0.000e+00
 3.000e+00 8.000e+00 4.000e+00 2.100e+01 0.000e+00 3.410e+02 2.000e+00
 0.000e+00 5.200e+01 2.000e+00 1.000e+00 1.000e+01 2.600e+01 2.950e+02
 2.520e+02 3.950e+02 3.840e+02 2.000e+01 3.240e+02 5.200e+01 7.900e+01
 0.000e+00 2.730e+02 5.500e+01 1.290e+02 4.880e+02 1.450e+02 1.860e+02
 2.000e+02 1.600e+01 4.600e+01 4.000e+00 7.700e+01 1.400e+01 2.210e+02
 4.560e+02 3.200e+02 1.720e+02 2.800e+01 3.000e+00"""
FN = """3.000e+00 8.700e+01 0.000e+00 1.600e+02 1.890e+02 2.000e+00 6.340e+02
 2.900e+01 3.100e+01 3.880e+02 8.660e+02 2.500e+01 1.300e+01 0.000e+00
 1.840e+02 3.620e+02 9.850e+02 2.640e+02 2.470e+02 2.000e+00 5.540e+02
 3.600e+01 5.590e+02 2.980e+02 7.920e+02 8.750e+02 9.360e+02 0.000e+00
 0.000e+00 7.100e+01 1.164e+03 1.240e+02 2.000e+00 2.920e+02 3.120e+02
 2.470e+02 4.400e+01 1.570e+02 0.000e+00 0.000e+00 5.000e+00 0.000e+00
 4.000e+00 1.500e+01 2.100e+01 3.000e+00 6.000e+00 2.500e+01 1.200e+01
 0.000e+00 2.000e+00 3.000e+00 1.100e+01 2.700e+01 2.200e+01 1.000e+00
 1.000e+00 4.000e+00 1.000e+00 1.000e+00 0.000e+00 0.000e+00 1.700e+01
 8.810e+02 3.000e+00 3.000e+00 2.260e+02 8.250e+02 7.000e+00 5.000e+00
 1.000e+00 4.000e+00 1.830e+02 1.612e+03 1.340e+02 4.000e+01 1.820e+02
 2.600e+01 2.040e+02 1.560e+02 1.900e+01 2.500e+01 7.000e+00 4.520e+02
 2.000e+00 0.000e+00 0.000e+00 1.000e+00 0.000e+00 1.147e+03 2.000e+00
 2.500e+01 2.000e+00 2.000e+00 0.000e+00 1.000e+00 3.000e+00 2.000e+00
 0.000e+00 9.000e+00 2.070e+02 0.000e+00 3.000e+00 0.000e+00 0.000e+00
 2.000e+00 1.210e+02 7.000e+00 0.000e+00 3.730e+02 1.870e+02 0.000e+00
 1.400e+01 2.000e+00 6.600e+01 2.800e+01 1.000e+00 3.000e+00 1.000e+00
 4.700e+01 1.100e+01 0.000e+00 3.800e+01 1.000e+00 8.460e+02 0.000e+00
 1.000e+00 3.200e+01 8.000e+00 1.000e+00 6.000e+00 1.010e+02 1.720e+02
 1.650e+02 5.550e+02 3.500e+02 1.900e+01 4.740e+02 7.400e+01 9.900e+01
 2.000e+00 3.730e+02 5.400e+01 2.450e+02 2.910e+02 1.740e+02 4.400e+01
 3.630e+02 3.000e+01 7.520e+02 1.700e+01 1.060e+02 2.000e+00 3.260e+02
 5.370e+02 3.230e+02 7.200e+01 5.200e+01 3.000e+00"""
TP = """2.800e+01 9.050e+02 4.000e+00 1.689e+03 1.554e+03 4.400e+01 2.528e+03
 9.700e+01 2.370e+02 8.900e+01 1.744e+03 1.830e+02 4.680e+02 6.000e+00
 3.510e+02 7.938e+03 8.600e+01 3.090e+03 4.888e+03 1.300e+01 1.517e+03
 2.010e+02 1.170e+03 1.340e+03 2.100e+01 3.345e+03 4.200e+01 1.400e+01
 1.000e+00 7.570e+02 8.040e+02 1.280e+02 7.000e+00 6.600e+02 6.830e+02
 7.700e+01 8.500e+01 5.630e+02 1.000e+00 7.000e+00 4.600e+01 7.000e+00
 7.000e+01 5.600e+01 5.300e+01 1.430e+02 2.400e+01 1.640e+02 3.500e+01
 2.900e+01 1.100e+01 1.500e+01 5.300e+01 3.460e+02 9.400e+01 4.800e+01
 0.000e+00 1.020e+02 7.500e+01 0.000e+00 8.000e+00 4.000e+00 1.270e+02
 1.160e+02 2.900e+01 8.900e+01 4.500e+01 3.250e+02 8.000e+01 2.000e+01
 0.000e+00 9.000e+00 2.282e+03 8.830e+02 1.302e+03 4.500e+01 1.060e+02
 1.600e+01 3.230e+02 8.820e+02 8.800e+01 3.420e+02 5.300e+02 1.480e+02
 2.400e+01 3.500e+01 1.000e+01 4.600e+01 1.400e+01 8.600e+02 1.000e+01
 9.700e+01 0.000e+00 0.000e+00 0.000e+00 4.100e+01 2.100e+01 0.000e+00
 1.000e+00 1.300e+01 4.100e+01 1.000e+00 2.000e+00 0.000e+00 3.000e+00
 2.000e+01 9.240e+02 1.030e+02 2.200e+01 3.570e+02 7.300e+01 1.000e+00
 4.900e+01 3.000e+00 6.700e+01 1.460e+02 1.160e+02 5.000e+00 0.000e+00
 1.200e+01 8.800e+01 7.000e+00 5.460e+02 1.500e+01 2.920e+02 7.000e+00
 8.000e+00 6.800e+02 0.000e+00 4.000e+00 2.330e+02 6.680e+02 1.078e+03
 1.800e+02 3.080e+02 1.600e+02 3.310e+02 1.590e+02 7.000e+01 1.800e+01
 2.100e+01 9.600e+01 2.380e+02 1.262e+03 9.700e+01 3.000e+01 2.500e+01
 4.800e+01 3.000e+00 9.000e+00 5.500e+01 4.300e+01 1.600e+01 8.100e+01
 3.140e+02 6.820e+02 5.600e+01 1.000e+01 5.000e+00"""
TN = """82651. 81552. 82678. 80649. 80773. 82636. 78278. 82548. 82405. 81827.
 73195. 82472. 82190. 82676. 82119. 73947. 81229. 79144. 77189. 82665.
 80023. 82407. 80358. 80746. 81701. 77580. 81512. 82668. 82681. 81791.
 80338. 82307. 82672. 81430. 81365. 82162. 82456. 81817. 82681. 82675.
 82626. 82674. 82607. 82593. 82592. 82530. 82651. 82485. 82631. 82652.
 82667. 82658. 82618. 82286. 82562. 82629. 82681. 82567. 82602. 82681.
 82674. 82677. 82527. 81439. 82643. 82587. 82331. 81447. 82595. 82653.
 82681. 82669. 80065. 79270. 81197. 82586. 81934. 82616. 82037. 81566.
 82559. 82304. 82116. 81336. 82656. 82639. 82671. 82634. 82668. 79231.
 82669. 82528. 82680. 82679. 82681. 82636. 82657. 82680. 82680. 82657.
 82381. 82681. 82677. 82680. 82679. 82658. 81545. 82569. 82660. 81836.
 82262. 82681. 82588. 82677. 82492. 82494. 82559. 82674. 82681. 82620.
 82575. 82671. 82077. 82666. 81203. 82673. 82673. 81918. 82672. 82676.
 82433. 81887. 81137. 82085. 81424. 81788. 82312. 81725. 82486. 82486.
 82659. 81940. 82335. 81046. 81806. 82333. 82427. 82071. 82633. 81875.
 82606. 82456. 82650. 82054. 81375. 81357. 82382. 82592. 82671."""
TN = """82651. 81552. 82678. 80649. 80773. 82636. 78278. 82548. 82405. 81827.
 73195. 82472. 82190. 82676. 82119. 73947. 81229. 79144. 77189. 82665.
 80023. 82407. 80358. 80746. 81701. 77580. 81512. 82668. 82681. 81791.
 80338. 82307. 82672. 81430. 81365. 82162. 82456. 81817. 82681. 82675.
 82626. 82674. 82607. 82593. 82592. 82530. 82651. 82485. 82631. 82652.
 82667. 82658. 82618. 82286. 82562. 82629. 82681. 82567. 82602. 82681.
 82674. 82677. 82527. 81439. 82643. 82587. 82331. 81447. 82595. 82653.
 82681. 82669. 80065. 79270. 81197. 82586. 81934. 82616. 82037. 81566.
 82559. 82304. 82116. 81336. 82656. 82639. 82671. 82634. 82668. 79231.
 82669. 82528. 82680. 82679. 82681. 82636. 82657. 82680. 82680. 82657.
 82381. 82681. 82677. 82680. 82679. 82658. 81545. 82569. 82660. 81836.
 82262. 82681. 82588. 82677. 82492. 82494. 82559. 82674. 82681. 82620.
 82575. 82671. 82077. 82666. 81203. 82673. 82673. 81918. 82672. 82676.
 82433. 81887. 81137. 82085. 81424. 81788. 82312. 81725. 82486. 82486.
 82659. 81940. 82335. 81046. 81806. 82333. 82427. 82071. 82633. 81875.
 82606. 82456. 82650. 82054. 81375. 81357. 82382. 82592. 82671."""


FP = FP.split(' ')
FP = [float(f) for f in FP]
FP = np.array(FP)
print(f'Length of FP: {len(FP)}')

FN = FN.split(' ')
FN = [float(f) for f in FN]
FN = np.array(FN)
print(f'Length of FN: {len(FN)}')

TP = TP.split(' ')
TP = [float(f) for f in TP]
TP = np.array(TP)
print(f'Length of TP: {len(TP)}')

TN = TN.split(' ')
TN = [float(f) for f in TN]
TN = np.array(TN)
print(f'Length of TN: {len(TN)}')

NUM_CATEGORIES = len(FP)
TPR = TP/(TP+FN)
M_TPR = (1/NUM_CATEGORIES) * np.nansum(TPR)
print(f'Mean TPR: {M_TPR}')

FPR = FP/(FP+TN)
M_FPR = (1/NUM_CATEGORIES) * np.nansum(FPR)
print(f'Mean FPR: {M_FPR}')

FNR = FN/(TP+FN)
M_FNR = (1/NUM_CATEGORIES) * np.nansum(FNR)
print(f'Mean FNR: {M_FNR}')

data = getDataset(vectorTestPath_flat)
y_test = data[-2]

labels, counts = np.unique(y_test, return_counts=True)
total = len(y_test)
weights = [c/total for c in counts]

W_FPR = [fpr*w for fpr, w in zip(FPR, weights)]
weighted_fpr = (np.nansum(W_FPR)/total)
print(f'Weighted FNR: {weighted_fpr}')

W_FNR = [fnr*w for fnr, w in zip(FNR, weights)]
weighted_fnr = (np.nansum(W_FNR)/total)
print(f'Weighted FNR: {weighted_fnr}')



