import gc, os, sys
import numpy as np
VUL_PATH = os.environ.get('VUL_PATH')
sys.path.insert(1, VUL_PATH)
from SYSE_2_vulnerabilityType.DetectVulType import DetectVulType
from utils.DLCustomModels import create_blstm_model, create_bgru_model
from utils.MLMethods import convert_nested_lists_to_numpy_arrays, fit_transform_PCA, test_and_get_n_for_PCA
from utils.utils import encode_target, getDataset, save_data_to_file

RANDOMSEED = 1099
CLASS_TYPE = 'Granular_PCA_162'
MODEL_TYPE = 'blstm'
VECTOR_TRANSFORMER='w2v'
LAYERS = 2
DROPOUT = 0.2
BATCHSIZE = 64
EPOCHS = 60

# vectorRootPath = os.path.join('data', 'DLvectors')
# vectorTrainPath = os.path.join(vectorRootPath,'train_162classes_flattened')
# vectorTestPath = os.path.join(vectorRootPath,'test_162classes_flattened')
# pca_vector_train_path = os.path.join(vectorRootPath,'PCA_train_162classes')
# pca_vector_test_path = os.path.join(vectorRootPath,'PCA_test_162classes')
inputRootPath = os.path.join('data', 'DLinputs')
dlInputsTrainPath = os.path.join(inputRootPath,'PCA_train_157classes')
dlInputsTestPath = os.path.join(inputRootPath,'PCA_test_157classes')

checkpoint_dir = './ckpt_%s_%s_%s_%s' % (VECTOR_TRANSFORMER, MODEL_TYPE, str(BATCHSIZE), CLASS_TYPE)
model_name = '%s_%s_batch=%s_seed=%s_epochs=%s_%s' % (MODEL_TYPE.upper(), VECTOR_TRANSFORMER.upper(), BATCHSIZE, RANDOMSEED, EPOCHS, CLASS_TYPE)
metrics_path = os.path.join(f'{VECTOR_TRANSFORMER}Model', 'metrics', MODEL_TYPE)
weights_path = os.path.join('model', model_name + 'weights')

dvt = DetectVulType(build_model=create_blstm_model, metricsPath=metrics_path,
                    randomSeed=RANDOMSEED, layers=LAYERS, weightpath=weights_path,
                    batch_size=BATCHSIZE, m_epochs=EPOCHS, modelName=model_name, 
                    # vectorTrainPath=vectorTrainPath, vectorTestPath=vectorTestPath,
                    inputsTrainPath=dlInputsTrainPath, inputsTestPath=dlInputsTestPath,
                    checkpoint_dir=checkpoint_dir)


# inputRootPath = os.path.join('data', 'DLinputs')
# dlInputsTrainPath = os.path.join(inputRootPath,'PCA_train_157classes')
# dlInputsTestPath = os.path.join(inputRootPath,'PCA_test_157classes')
# data = getDataset(dlInputsTrainPath)
# test_case_ids = data[-1]

# vectorRootPath = os.path.join('data', 'DLvectors')
# vectorALLPath = os.path.join(vectorRootPath, 'PCA_vectors')
# data2 = getDataset(vectorALLPath, False)
# print(f'Length of columns: {len(data2[0][0])}')

# categories = []
# for case_id in test_case_ids:
#     cat_index = data2[-1].index(case_id)
#     assert data2[-1][cat_index] == case_id, f'Case ID from PCA_vectors {data2[-1][cat_index]} != case id from DLinputs/PCA_train_157 {case_id}'
#     category = data2[-2][cat_index]
#     categories.append(category)



categories = """15 20 22 23 36 59 78 79 80 88 89 90 94 99 114 119 120 121
 122 123 124 125 126 127 129 134 170 176 188 189 190 191 193 194 195 196
 197 200 222 223 226 242 244 252 253 254 255 256 259 269 272 284 310 319
 321 325 326 327 328 338 346 347 352 362 363 364 366 367 369 377 390 391
 396 398 399 400 401 404 412 414 415 416 426 427 457 459 464 467 468 469
 475 476 478 479 481 484 489 495 506 510 511 526 535 543 562 563 571 587
 588 590 591 605 606 609 615 617 620 663 665 666 667 672 674 675 676 680
 681 682 685 688 690 704 754 758 761 762 764 765 771 772 773 774 775 780
 785 787 789 805 806 820 821 822 824 828 831 832 833 834 835 839 843 909"""
categories = categories.split(' ')
categories = [int(c) for c in categories]

# Next, encoder logic
dvt.labelEncoder = encode_target(categories)[1]


# # PCA Train
# data = getDataset(dvt.vectorTrainPath, RANDOMSEED=RANDOMSEED)
# x_train = data[0]
# best_n = 4717
# data[0] = fit_transform_PCA(x_train, best_n)
# save_data_to_file(pca_vector_train_path, 'balanced_train.pkl', data)

# # PCA Test
# data = getDataset(dvt.vectorTestPath, RANDOMSEED=RANDOMSEED)
# x_test = data[0]
# best_n = 4717
# data[0] = fit_transform_PCA(x_test, best_n)
# save_data_to_file(pca_vector_test_path, 'balanced_train.pkl', data)

# dvt.vectorTrainPath = pca_vector_train_path
# dvt.vectorTestPath = pca_vector_test_path
# dvt.adjustVectorLength()

# Compare DLinputs/PCA_train_157classes with DLinputs/PCA_train_162classes
# If they match up, then we can keep vectorTrainPath = pca_vector_train_path for our decoder and continue where we left off (epoch 60)
# if they don't match up, try no RANDOMSEED


data = getDataset(dvt.inputsTrainPath)
VECTOR_SIZE = len(data[0][0])
dvt.vector_size = VECTOR_SIZE
dvt.avg = 1
print(f'Avg: {dvt.avg} Vector: {VECTOR_SIZE}')
# dvt.encodeLabels()
print(f'Density units: {dvt.density_units}. Vector size: {dvt.vector_size}')
dvt.build_and_fit()
dvt.predict_and_score()



# import os, sys
# import numpy as np
# VUL_PATH = os.environ.get('VUL_PATH')
# sys.path.insert(1, VUL_PATH)
# from utils.utils import getDataset
# vectorRootPath = os.path.join('data', 'DLvectors')
# vectorTestPath_flat = os.path.join(vectorRootPath,'test_162classes_flattened')

# FP = """0.000e+00 1.380e+02 0.000e+00 1.840e+02 1.660e+02 0.000e+00 1.242e+03
#  8.000e+00 9.000e+00 3.780e+02 6.877e+03 2.000e+00 1.100e+01 0.000e+00
#  2.800e+01 4.350e+02 3.820e+02 1.840e+02 3.580e+02 2.000e+00 5.880e+02
#  3.800e+01 5.950e+02 2.980e+02 1.680e+02 8.820e+02 1.920e+02 0.000e+00
#  0.000e+00 6.300e+01 3.760e+02 1.230e+02 1.000e+00 3.000e+02 3.220e+02
#  1.960e+02 9.700e+01 1.450e+02 0.000e+00 0.000e+00 5.000e+00 1.000e+00
#  1.000e+00 1.800e+01 1.600e+01 6.000e+00 1.000e+00 8.000e+00 4.000e+00
#  1.000e+00 2.000e+00 6.000e+00 0.000e+00 2.300e+01 4.000e+00 4.000e+00
#  0.000e+00 9.000e+00 4.000e+00 0.000e+00 0.000e+00 1.000e+00 1.100e+01
#  2.460e+02 7.000e+00 3.000e+00 8.000e+01 8.500e+01 0.000e+00 4.000e+00
#  0.000e+00 0.000e+00 1.520e+02 9.170e+02 4.900e+01 1.100e+01 4.600e+02
#  2.400e+01 1.180e+02 7.800e+01 1.600e+01 1.100e+01 2.900e+01 7.460e+02
#  0.000e+00 8.000e+00 1.000e+00 1.000e+00 0.000e+00 1.444e+03 1.000e+00
#  3.200e+01 0.000e+00 1.000e+00 1.000e+00 4.000e+00 1.000e+00 0.000e+00
#  1.000e+00 3.000e+00 5.300e+01 0.000e+00 0.000e+00 2.000e+00 0.000e+00
#  2.000e+00 9.200e+01 3.000e+00 0.000e+00 1.160e+02 1.600e+02 0.000e+00
#  3.100e+01 0.000e+00 5.700e+01 1.400e+01 6.000e+00 0.000e+00 0.000e+00
#  3.000e+00 8.000e+00 4.000e+00 2.100e+01 0.000e+00 3.410e+02 2.000e+00
#  0.000e+00 5.200e+01 2.000e+00 1.000e+00 1.000e+01 2.600e+01 2.950e+02
#  2.520e+02 3.950e+02 3.840e+02 2.000e+01 3.240e+02 5.200e+01 7.900e+01
#  0.000e+00 2.730e+02 5.500e+01 1.290e+02 4.880e+02 1.450e+02 1.860e+02
#  2.000e+02 1.600e+01 4.600e+01 4.000e+00 7.700e+01 1.400e+01 2.210e+02
#  4.560e+02 3.200e+02 1.720e+02 2.800e+01 3.000e+00"""
# FN = """3.000e+00 8.700e+01 0.000e+00 1.600e+02 1.890e+02 2.000e+00 6.340e+02
#  2.900e+01 3.100e+01 3.880e+02 8.660e+02 2.500e+01 1.300e+01 0.000e+00
#  1.840e+02 3.620e+02 9.850e+02 2.640e+02 2.470e+02 2.000e+00 5.540e+02
#  3.600e+01 5.590e+02 2.980e+02 7.920e+02 8.750e+02 9.360e+02 0.000e+00
#  0.000e+00 7.100e+01 1.164e+03 1.240e+02 2.000e+00 2.920e+02 3.120e+02
#  2.470e+02 4.400e+01 1.570e+02 0.000e+00 0.000e+00 5.000e+00 0.000e+00
#  4.000e+00 1.500e+01 2.100e+01 3.000e+00 6.000e+00 2.500e+01 1.200e+01
#  0.000e+00 2.000e+00 3.000e+00 1.100e+01 2.700e+01 2.200e+01 1.000e+00
#  1.000e+00 4.000e+00 1.000e+00 1.000e+00 0.000e+00 0.000e+00 1.700e+01
#  8.810e+02 3.000e+00 3.000e+00 2.260e+02 8.250e+02 7.000e+00 5.000e+00
#  1.000e+00 4.000e+00 1.830e+02 1.612e+03 1.340e+02 4.000e+01 1.820e+02
#  2.600e+01 2.040e+02 1.560e+02 1.900e+01 2.500e+01 7.000e+00 4.520e+02
#  2.000e+00 0.000e+00 0.000e+00 1.000e+00 0.000e+00 1.147e+03 2.000e+00
#  2.500e+01 2.000e+00 2.000e+00 0.000e+00 1.000e+00 3.000e+00 2.000e+00
#  0.000e+00 9.000e+00 2.070e+02 0.000e+00 3.000e+00 0.000e+00 0.000e+00
#  2.000e+00 1.210e+02 7.000e+00 0.000e+00 3.730e+02 1.870e+02 0.000e+00
#  1.400e+01 2.000e+00 6.600e+01 2.800e+01 1.000e+00 3.000e+00 1.000e+00
#  4.700e+01 1.100e+01 0.000e+00 3.800e+01 1.000e+00 8.460e+02 0.000e+00
#  1.000e+00 3.200e+01 8.000e+00 1.000e+00 6.000e+00 1.010e+02 1.720e+02
#  1.650e+02 5.550e+02 3.500e+02 1.900e+01 4.740e+02 7.400e+01 9.900e+01
#  2.000e+00 3.730e+02 5.400e+01 2.450e+02 2.910e+02 1.740e+02 4.400e+01
#  3.630e+02 3.000e+01 7.520e+02 1.700e+01 1.060e+02 2.000e+00 3.260e+02
#  5.370e+02 3.230e+02 7.200e+01 5.200e+01 3.000e+00"""
# TP = """2.800e+01 9.050e+02 4.000e+00 1.689e+03 1.554e+03 4.400e+01 2.528e+03
#  9.700e+01 2.370e+02 8.900e+01 1.744e+03 1.830e+02 4.680e+02 6.000e+00
#  3.510e+02 7.938e+03 8.600e+01 3.090e+03 4.888e+03 1.300e+01 1.517e+03
#  2.010e+02 1.170e+03 1.340e+03 2.100e+01 3.345e+03 4.200e+01 1.400e+01
#  1.000e+00 7.570e+02 8.040e+02 1.280e+02 7.000e+00 6.600e+02 6.830e+02
#  7.700e+01 8.500e+01 5.630e+02 1.000e+00 7.000e+00 4.600e+01 7.000e+00
#  7.000e+01 5.600e+01 5.300e+01 1.430e+02 2.400e+01 1.640e+02 3.500e+01
#  2.900e+01 1.100e+01 1.500e+01 5.300e+01 3.460e+02 9.400e+01 4.800e+01
#  0.000e+00 1.020e+02 7.500e+01 0.000e+00 8.000e+00 4.000e+00 1.270e+02
#  1.160e+02 2.900e+01 8.900e+01 4.500e+01 3.250e+02 8.000e+01 2.000e+01
#  0.000e+00 9.000e+00 2.282e+03 8.830e+02 1.302e+03 4.500e+01 1.060e+02
#  1.600e+01 3.230e+02 8.820e+02 8.800e+01 3.420e+02 5.300e+02 1.480e+02
#  2.400e+01 3.500e+01 1.000e+01 4.600e+01 1.400e+01 8.600e+02 1.000e+01
#  9.700e+01 0.000e+00 0.000e+00 0.000e+00 4.100e+01 2.100e+01 0.000e+00
#  1.000e+00 1.300e+01 4.100e+01 1.000e+00 2.000e+00 0.000e+00 3.000e+00
#  2.000e+01 9.240e+02 1.030e+02 2.200e+01 3.570e+02 7.300e+01 1.000e+00
#  4.900e+01 3.000e+00 6.700e+01 1.460e+02 1.160e+02 5.000e+00 0.000e+00
#  1.200e+01 8.800e+01 7.000e+00 5.460e+02 1.500e+01 2.920e+02 7.000e+00
#  8.000e+00 6.800e+02 0.000e+00 4.000e+00 2.330e+02 6.680e+02 1.078e+03
#  1.800e+02 3.080e+02 1.600e+02 3.310e+02 1.590e+02 7.000e+01 1.800e+01
#  2.100e+01 9.600e+01 2.380e+02 1.262e+03 9.700e+01 3.000e+01 2.500e+01
#  4.800e+01 3.000e+00 9.000e+00 5.500e+01 4.300e+01 1.600e+01 8.100e+01
#  3.140e+02 6.820e+02 5.600e+01 1.000e+01 5.000e+00"""
# TN = """82651. 81552. 82678. 80649. 80773. 82636. 78278. 82548. 82405. 81827.
#  73195. 82472. 82190. 82676. 82119. 73947. 81229. 79144. 77189. 82665.
#  80023. 82407. 80358. 80746. 81701. 77580. 81512. 82668. 82681. 81791.
#  80338. 82307. 82672. 81430. 81365. 82162. 82456. 81817. 82681. 82675.
#  82626. 82674. 82607. 82593. 82592. 82530. 82651. 82485. 82631. 82652.
#  82667. 82658. 82618. 82286. 82562. 82629. 82681. 82567. 82602. 82681.
#  82674. 82677. 82527. 81439. 82643. 82587. 82331. 81447. 82595. 82653.
#  82681. 82669. 80065. 79270. 81197. 82586. 81934. 82616. 82037. 81566.
#  82559. 82304. 82116. 81336. 82656. 82639. 82671. 82634. 82668. 79231.
#  82669. 82528. 82680. 82679. 82681. 82636. 82657. 82680. 82680. 82657.
#  82381. 82681. 82677. 82680. 82679. 82658. 81545. 82569. 82660. 81836.
#  82262. 82681. 82588. 82677. 82492. 82494. 82559. 82674. 82681. 82620.
#  82575. 82671. 82077. 82666. 81203. 82673. 82673. 81918. 82672. 82676.
#  82433. 81887. 81137. 82085. 81424. 81788. 82312. 81725. 82486. 82486.
#  82659. 81940. 82335. 81046. 81806. 82333. 82427. 82071. 82633. 81875.
#  82606. 82456. 82650. 82054. 81375. 81357. 82382. 82592. 82671."""
# TN = """82651. 81552. 82678. 80649. 80773. 82636. 78278. 82548. 82405. 81827.
#  73195. 82472. 82190. 82676. 82119. 73947. 81229. 79144. 77189. 82665.
#  80023. 82407. 80358. 80746. 81701. 77580. 81512. 82668. 82681. 81791.
#  80338. 82307. 82672. 81430. 81365. 82162. 82456. 81817. 82681. 82675.
#  82626. 82674. 82607. 82593. 82592. 82530. 82651. 82485. 82631. 82652.
#  82667. 82658. 82618. 82286. 82562. 82629. 82681. 82567. 82602. 82681.
#  82674. 82677. 82527. 81439. 82643. 82587. 82331. 81447. 82595. 82653.
#  82681. 82669. 80065. 79270. 81197. 82586. 81934. 82616. 82037. 81566.
#  82559. 82304. 82116. 81336. 82656. 82639. 82671. 82634. 82668. 79231.
#  82669. 82528. 82680. 82679. 82681. 82636. 82657. 82680. 82680. 82657.
#  82381. 82681. 82677. 82680. 82679. 82658. 81545. 82569. 82660. 81836.
#  82262. 82681. 82588. 82677. 82492. 82494. 82559. 82674. 82681. 82620.
#  82575. 82671. 82077. 82666. 81203. 82673. 82673. 81918. 82672. 82676.
#  82433. 81887. 81137. 82085. 81424. 81788. 82312. 81725. 82486. 82486.
#  82659. 81940. 82335. 81046. 81806. 82333. 82427. 82071. 82633. 81875.
#  82606. 82456. 82650. 82054. 81375. 81357. 82382. 82592. 82671."""

# FP = FP.split(' ')
# FP = [float(f) for f in FP]
# FP = np.array(FP)
# print(f'Length of FP: {len(FP)}')

# FN = FN.split(' ')
# FN = [float(f) for f in FN]
# FN = np.array(FN)
# print(f'Length of FN: {len(FN)}')

# TP = TP.split(' ')
# TP = [float(f) for f in TP]
# TP = np.array(TP)
# print(f'Length of TP: {len(TP)}')

# TN = TN.split(' ')
# TN = [float(f) for f in TN]
# TN = np.array(TN)
# print(f'Length of TN: {len(TN)}')

# NUM_CATEGORIES = len(FP)
# FPR = FP/(FP+TN)
# M_FPR = (1/NUM_CATEGORIES) * np.nansum(FPR)
# print(f'Mean FPR: {M_FPR}')

# FNR = FN/(TP+FN)
# M_FNR = (1/NUM_CATEGORIES) * np.nansum(FNR)
# print(f'Mean FNR: {M_FNR}')

# data = getDataset(vectorTestPath_flat)
# y_test = data[-2]

# labels, counts = np.unique(y_test, return_counts=True)
# total = len(y_test)

# FPR_sum = 0
# i = 0 # index for FPR
# for label, count in zip(labels, counts):
#     result = FPR[i] * count
#     FPR_sum += result if np.isnan(result) != True else 0
#     i += 1

# W_FPR = FPR_sum/total
# print(f'Weighted FNR: {W_FPR}')


# FNR_sum = 0
# i = 0
# for label, count in zip(labels, counts):
#     result = FNR[i] * count
#     FNR_sum += result if np.isnan(result) != True else 0
#     i +=1 

# W_FNR = FNR_sum/total
# print(f'Weighted FNR: {W_FNR}')




