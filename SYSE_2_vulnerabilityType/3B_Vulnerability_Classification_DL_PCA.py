#!/usr/bin/env python
# coding: utf-8

import os, sys

import numpy as np
from sklearn.decomposition import PCA
import tensorflow as tf
from tensorflow import keras

VUL_PATH = os.environ.get('VUL_PATH')
sys.path.insert(1, VUL_PATH)

from SYSE_1_isVulnerable.preprocess_dl_Input_version5 import process_sequences_shape
from SYSE_1_isVulnerable.splitTrainTest import splitTrainTestCategorical
from SYSE_1_isVulnerable.adjustVectorLen import meanLen, tranformVectorLen
from SYSE_1_isVulnerable.saveKeyData import saveKeyDataMulticlass
from SYSE_1_isVulnerable.ConfusionMatrix import getConfusionMatrix_Multiclass
from SYSE_2_vulnerabilityType.DetectVulType import DetectVulType
from utils.utils import encode_target, getDataset, save_data_to_file
from utils.DLCustomModels import create_bgru_model, create_blstm_model, fit_custom_dl_model, make_or_restore_model, predictMulticlassLabel


randomSeed = 1099
vectorDim = 30 #num of vector cols
vectorRootPath = os.path.join('data','DLvectors')
vectorALLPath = os.path.join(vectorRootPath, 'ALL_vectors')
vectorDownsampledPath = os.path.join(vectorRootPath, 'Downsampled_vectors')
vectorTrainPath = os.path.join(vectorRootPath,'train')
vectorTestPath = os.path.join(vectorRootPath,'test')
dlInputsTrainPath = os.path.join('data','DLinputs','train')
dlInputsTestPath  = os.path.join('data','DLinputs','test')

# Phase 3: PCA on full dataset (Group IDs)
## W2V + BGRU Granular IDs 
data = getDataset(vectorALLPath, False, randomSeed)
original_labels = data[-2]
categories = np.unique(original_labels)
mapping, labelEncoder = encode_target(categories)
density_units = categories.shape[0]

avg = meanLen(vectorTrainPath)
input_shape_1 = avg * vectorDim
print("New vector size: ", str(input_shape_1))

x_train = process_sequences_shape(data[0], avg, vectorDim)
x_train = np.reshape(x_train, (x_train.shape[0], input_shape_1))
n_stop = int(len(x_train[0])/2)
step = int(n_stop/10)
n_components = np.arange(100, n_stop, step)
print(f'N components to test: {n_components}')

explained_variance_ratios = []
for n in n_components:
    pca_model = PCA(n_components=n)
    pca_model.fit(x_train)
    sum_expl_var = sum(pca_model.explained_variance_ratio_)
    explained_variance_ratios.append(sum_expl_var)
    print(f' Sum of variance ratios for n={n}: ', sum_expl_var)
    if sum_expl_var >= 99:
        break

n_index = len(explained_variance_ratios)
print(f'Choosing n={n_components[n_index]} since that still leaves us with about {explained_variance_ratios[n_index]}% variance.')

pca_model = PCA(n_components=n_components[n_index])
pca_model.fit(x_train)
x_comps = pca_model.transform(x_train)
print(x_comps.shape)

# Save PCA transformed data
pca_vectors = os.path.join(vectorRootPath, 'PCA_vectors')
pca_vectors_train = os.path.join(vectorRootPath, 'PCA_train')
pca_vectors_test = os.path.join(vectorRootPath, 'PCA_test')
pca_inputs_train_path = os.path.join('data', 'DLinputs', 'PCA_train')
pca_inputs_test_path = os.path.join('data', 'DLinputs', 'PCA_test')

data[0] = x_comps
save_data_to_file(pca_vectors, 'PCA_vectors_balanced.pkl', data)
splitTrainTestCategorical('balanced', pca_vectors, pca_vectors_train,
                          pca_vectors_test, randomSeed=0)
saveKeyDataMulticlass(pca_vectors_train, labelEncoder, pca_inputs_train_path)
saveKeyDataMulticlass(pca_vectors_test, labelEncoder, pca_inputs_test_path)


myoptimizer = 'adam' #can be changed to ‘adamax’
layers = 2
dropout = 0.2 
BATCHSIZE = 64
epochs = 20
# Update our dimensions since we have a flattened dataset now
maxlen = avg = 1
vectorDim = x_comps.shape[1]

CLASS_TYPE = 'Granular_PCA'
VECTOR_TRANSFORMER='w2v'
MODEL_TYPE = 'bgru'
checkpoint_dir = './ckpt_%s_%s_%s_%s' % (VECTOR_TRANSFORMER, MODEL_TYPE, str(BATCHSIZE), CLASS_TYPE)
model_name = '%s_%s_batch=%s_seed=%s_%s' % (MODEL_TYPE.upper(), VECTOR_TRANSFORMER.upper(), BATCHSIZE, randomSeed, CLASS_TYPE)
w2vmetricPath = os.path.join('w2vModel','metrics', MODEL_TYPE)
weightpath = os.path.join('model', model_name + myoptimizer + str(randomSeed))
dvt = DetectVulType(create_bgru_model, m_epochs=epochs, batch_size=BATCHSIZE, mask=False,
                    dropout=dropout, vectorTrainPath=pca_vectors_train,
                    vectorTestPath=pca_vectors_test, inputsTrainPath=pca_inputs_train_path,
                    inputsTestPath=pca_inputs_test_path, checkpoint_dir=checkpoint_dir)
dvt.avg = avg
dvt.vector_size = vectorDim
dvt.density_units = density_units

if not os.path.exists(checkpoint_dir):
    os.makedirs(checkpoint_dir)

# Reduces memory usage by restricting TF to use only 1 GPU (the one not in use by the server atm)
gpus = tf.config.experimental.list_physical_devices('GPU')
for gpu in gpus:
    tf.config.experimental.set_memory_growth(gpu, True)

# Open a strategy scope
strategy = tf.distribute.MirroredStrategy(['GPU:2'])
print('\nNumber of devices: {}'.format(strategy.num_replicas_in_sync))
with strategy.scope():
    latest_epoch, myKerasModel = make_or_restore_model(checkpoint_dir, dvt.get_compiled_model)

callbacks = [
    keras.callbacks.ModelCheckpoint(
        filepath=os.path.join(checkpoint_dir, 'ckpt-{epoch}'),
        monitor='val_acc',
        verbose=1,
        save_freq='epoch'
    )
]

# Fit
model = fit_custom_dl_model(myKerasModel, weightpath, pca_inputs_train_path, BATCHSIZE, avg, vectorDim, randomSeed, epochs, useGenerator=True, callbacks=callbacks, latest_epoch=latest_epoch)

# Predict
model.load_weights(weightpath)
_, mypredicted_labels, myreallabels, _ = predictMulticlassLabel(model, pca_inputs_test_path,
                                                                maxlen, vectorDim, myoptimizer,
                                                                model_name, randomSeed, labelEncoder)
# Confusion matrix for each category.
getConfusionMatrix_Multiclass(mypredicted_labels, myreallabels, saveFig=True, path=w2vmetricPath, modelName=model_name)



# ## W2V + BLSTM Granular IDs 
# ## D2V + BGRU Granular IDs 
# ## D2V + BLSTM Granular IDs 

# # PCA on full dataset (Granular IDs)
# ## W2V + BGRU Group 
# ## W2V + BLSTM Group 
# ## D2V + BGRU Group 
# ## D2V + BLSTM Group 


