#!/usr/bin/env python
# coding: utf-8

# # Vulnerability Type Classification
# The following models will be used to see which one performs best when attempting to classify a vulnerability type in a program:
# - Decision Tree
# - K-Nearest Neighbors
# - Naive Bayes

# ### Setting variables
import os, sys
import numpy as np
import pandas as pd

randomSeed = 1099
# Training & testing split, where training is balanced & testing is not
vectorTrainPath = '../data/DLvectors/train/'
vectorTestPath = '../data/DLvectors/test/'
# Balanced Train & Test sets with same length vectors for DL models
dlInputsTrainPath = '../data/DLinputs/train/'
dlInputsTestPath  = '../data/DLinputs/test/'


# ### Updating path
# Must insert path to directory above in order to access files in main
VUL_PATH = os.environ.get('VUL_PATH')
sys.path.insert(1, VUL_PATH)


# ### Import data
# #### Get D2V vector datasets 
from MLMethods import getDataset, get_grid, set_group_id
train_dataset, labels, sard_cve_ids, testcases, vtypes = getDataset(vectorTrainPath, 1, randomSeed)
train_df = pd.DataFrame({'testcase': testcases, 'sard_cve_id': sard_cve_ids, 'vtype': vtypes, 'label': labels})

test_dataset, labels, sard_cve_ids, testcases, vtypes = getDataset(vectorTestPath, 0, randomSeed)
test_df = pd.DataFrame({'testcase': testcases, 'sard_cve_id': sard_cve_ids, 'vtype': vtypes, 'label': labels})

# #### Get group ids per program
groups_df = pd.read_csv('../data/CVE/SARD_CVE_to_groups.csv', index_col=0)

# #### Append the group id associated to each case
set_group_id(train_df, groups_df)
set_group_id(test_df, groups_df)


# #### Drops rows with any missing group ids
null_indices1 = pd.isnull(train_df).any(1).to_numpy().nonzero()[0]
print('Indices with null values (looking for null group ids in particular): ', null_indices1)
print('Dropping any rows in training data that are missing group ids...')
print(train_df.shape)
train_df.dropna(inplace=True)
print(train_df.shape)

null_indices2 = pd.isnull(test_df).any(1).to_numpy().nonzero()[0]
print('\nIndices with null values (looking for null group ids in particular)', null_indices2)
print('Dropping any rows in testing data that are missing group ids...')
print(test_df.shape)
test_df.dropna(inplace=True)
print(test_df.shape)


# #### Set training, testing, and target variables
# Set training data
train_data = pd.DataFrame(train_dataset)
train_data.drop(index=null_indices1, inplace=True)
X_train = train_data.values

# Set testing data
test_data = pd.DataFrame(test_dataset)
test_data.drop(index=null_indices2, inplace=True)
X_test = test_data.values

# Set target attribute 
Y_train = train_df['group_id'].values
Y_train = Y_train.astype('int')

Y_test = test_df['group_id'].values
Y_test = Y_test.astype('int')


# ## Decision Tree
from sklearn import neighbors, tree, naive_bayes

print('\nGrid search on DT')
model = tree.DecisionTreeClassifier()
sample_leaf_fractions = np.linspace(0.0005, 0.02, 15)*13.06
sample_split_fractions = np.linspace(0.0005, 0.015, 10)*13.06
scoring = 'accuracy'
parameters = {
    'criterion': ['entropy','gini'],
    'max_depth': np.linspace(1, 20, 10),
    'min_samples_leaf': sample_leaf_fractions,
    'min_samples_split': sample_split_fractions
}
grid = get_grid(model, parameters, scoring, X_train, Y_train)
treeclf = tree.DecisionTreeClassifier(criterion='entropy', max_depth=13, min_samples_leaf=0.00653, min_samples_split=0.027571)
treeclf.fit(X_train, Y_train)

from MLMethods import measure_performance
print('Measuring performance')
print('Score on training: ', treeclf.score(X_train, Y_train))
print('Score on testing: ', treeclf.score(X_test, Y_test))
measure_performance(X_test, Y_test, treeclf)


# ## K-Nearest Neighbors
# Consider normalizing the data for better score
print('\nGrid search on KNN')
model = neighbors.KNeighborsClassifier()
scoring = 'accuracy'
parameters = {
    'n_neighbors': np.arange(1, 9, 1),
    'weights': ['uniform', 'distance'],
    'p': [1, 2], # 1 = manhattan distance, 2 = euclidean distance
}
grid = get_grid(model, parameters, scoring, X_train, Y_train)
knnclf =  neighbors.KNeighborsClassifier(n_neighbors=4, p=2, weights='distance')
knnclf.fit(X_train, Y_train)
print('Measuring performance')
print('Score on training: ', knnclf.score(X_train, Y_train))
print('Score on testing: ', knnclf.score(X_test, Y_test))
measure_performance(X_test, Y_test, knnclf)


# ## Gaussian Naive Bayes
# I don't expect this method to perform well since Bayes' theorem assumes features are strongly independent, when in fact ours is tightly coupled. https://scikit-learn.org/stable/modules/classes.html?highlight=naive%20bayes#module-sklearn.naive_bayes 
print('\n Gaussian Naive Bayes')
nbclf = naive_bayes.GaussianNB()
nbclf = nbclf.fit(X_train, Y_train)
print('Measuring performance')
print ("Score on Training: ", nbclf.score(X_train, Y_train))
print ("Score on Testing: ", nbclf.score(X_test, Y_test))
measure_performance(X_test, Y_test, nbclf)




