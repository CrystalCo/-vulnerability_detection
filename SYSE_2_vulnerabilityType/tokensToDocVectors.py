import gc, os, pickle

from gensim.models.doc2vec import Doc2Vec, TaggedDocument

class DirofCorpus(object):
    def __init__(self, dirname):
        self.dirname = dirname
    
    def __iter__(self):
        d = self.dirname
        for fn in os.listdir(d): 
            if not fn.endswith('DS_Store'):
                fnn = fn
                for filename in os.listdir(os.path.join(d, fnn)):
                    if not filename.endswith('DS_Store'):
                        pklname = filename
                        with open(os.path.join(d, fnn, pklname), 'rb') as f:
                            data = pickle.load(f)
                            tokens = data[0][0] # array of tokens in unicode string
                            tag1 = data[4][0] # vtype (API, AE, ARR, or PTR), so we know which source file contains the test case ID
                            tag2 = fn # unique doc id, which is the test case ID of the slice
                            tags = [tag1, int(tag2)]
                            document = TaggedDocument(tokens, tags)
                            yield document
                            del data, tokens, tag1, tags, document
                            gc.collect()


def createD2VModel(d2vmodelPath, tokenPath, vdim):
    print('Fitting D2V model from corpus...')
    """
        Doc2Vec parameters
            dm=1  :=  default; uses PV-DM (distributed memory) instead of PV-DBOW (distributed bag of words)
            vector_size :=
            window (int, optional)  :=  The maximum distance between the current and predicted word within a sentence.
            alpha (float, optional)  :=  The initial learning rate.
            min_alpha (float, optional)  :=  Learning rate will linearly drop to min_alpha as training progresses.
            seed (int, optional)  :=  Seed for the random number generator. Initial vectors for each word are seeded with a hash of the concatenation of word + str(seed). Note that for a fully deterministically-reproducible run, you must also limit the model to a single worker thread (workers=1), to eliminate ordering jitter from OS thread scheduling. In Python 3, reproducibility between interpreter launches also requires use of the PYTHONHASHSEED environment variable to control hash randomization. *
            min_count (int, optional)  :=  Ignores all words with total frequency lower than this.  **
            max_vocab_size (int, optional)  :=  Limits the RAM during vocabulary building; if there are more unique words than this, then prune the infrequent ones. Every 10 million word types need about 1GB of RAM. Set to None for no limit.
            sample (float, optional)  :=  The threshold for configuring which higher-frequency words are randomly downsampled, useful range is (0, 1e-5).
            workers (int, optional)  :=  Use these many worker threads to train the model (=faster training with multicore machines).
            epochs (int, optional)  :=  Number of iterations (epochs) over the corpus. 'iter' in W2V. *
            hs ({1,0}, optional)  :=  If 1, hierarchical softmax will be used for model training. If set to 0, and negative is non-zero, negative sampling will be used. **
            negative (int, optional)  :=  If > 0, negative sampling will be used, the int for negative specifies how many “noise words” should be drawn (usually between 5-20). If set to 0, no negative sampling is used. **
            ns_exponent (float, optional)  :=  The exponent used to shape the negative sampling distribution. A value of 1.0 samples exactly in proportion to the frequencies, 0.0 samples all words equally, while a negative value samples low-frequency words more than high-frequency words. The popular default value of 0.75 was chosen by the original Word2Vec paper. More recently, in https://arxiv.org/abs/1804.04212, Caselles-Dupré, Lesaint, & Royo-Letelier suggest that other values may perform better for recommendation applications.
            dm_mean=None - default; ({1,0}, optional)  :=  If 0 , use the sum of the context word vectors. If 1, use the mean. Only applies when dm is used in non-concatenative mode.
            dm_concat=0 - default; ({1,0}, optional)  :=  If 1, use concatenation of context vectors rather than sum/average; Note concatenation results in a much-larger model, as the input is no longer the size of one (sampled or arithmetically combined) word vector, but the size of the tag(s) and all words in the context strung together.
                - i.e. wv's are flattened. TODO: change to 1 & compare results
            dm_tag_count=1 (int, optional)  :=  Expected constant number of document tags per document, when using dm_concat mode.
            dbow_words=0 - default; ({1,0}, optional) If set to 1 trains word-vectors (in skip-gram fashion) simultaneous with DBOW doc-vector training; If 0, only trains doc-vectors (faster).
            trim_rule=None - default; (function, optional) Vocabulary trimming rule, specifies whether certain words should remain in the vocabulary, be trimmed away, or handled using the default (discard if word count < min_count). Can be None (min_count will be used, look to keep_vocab_item()), or a callable that accepts parameters (word, count, min_count) and returns either gensim.utils.RULE_DISCARD, gensim.utils.RULE_KEEP or gensim.utils.RULE_DEFAULT. The rule, if given, is only used to prune vocabulary during current method call and is not stored as part of the model.


            *TO DO: should we use same as what our W2V Model used?
            **Used same as our W2V's
    """
    mymodel = Doc2Vec(documents=DirofCorpus(tokenPath), alpha=0.01,  min_count=0, max_vocab_size=None, hs=0, negative=10, workers=1, vector_size=vdim)
    mymodel.save(d2vmodelPath)
    print("Model created and saved in: " + d2vmodelPath)
    words = sorted(mymodel.wv.vocab.keys())
    print("Number of words in model:", len(words))
    fp = open("wordsD2Vmodel.txt", "w", encoding="utf-8")
    for word in words:
        fp.write(word + '\n')
    fp.close()
    return mymodel


def fitD2VModel(d2vmodelPath, tokenPath, vectorPath):
    mymodel = Doc2Vec.load(d2vmodelPath)
    for corpusfiles in os.listdir(tokenPath):
        #print(corpusfiles)
        if not corpusfiles.endswith('DS_Store'):
            cfs = corpusfiles
            if cfs not in os.listdir(vectorPath): 
                folder_path = os.path.join(vectorPath, cfs)
                os.mkdir(folder_path)
            for corpusfile in os.listdir(tokenPath + cfs):
                if not corpusfile.endswith('DS_Store'):
                    cf = corpusfile
                    corpus_path = os.path.join(tokenPath, corpusfiles, cf)
                    f_corpus = open(corpus_path, 'rb')
                    data = pickle.load(f_corpus)
                    f_corpus.close()
                    data[0] = [mymodel[int(cfs)]]
                    vector_path = os.path.join(vectorPath, corpusfiles, corpusfile)
                    f_vector = open(vector_path, 'wb')
                    pickle.dump(data, f_vector, protocol=pickle.HIGHEST_PROTOCOL)
                    f_vector.close()
    print("D2V Completed: The vector file is in vector folder")

"""
import pandas as pd
import numpy as np
from sklearn.base import BaseEstimator
from sklearn.model_selection import GridSearchCV
from sklearn.cluster import AgglomerativeClustering
from sklearn.metrics import silhouette_score
from sklearn.pipeline import Pipeline

from ClusteringMethods import getTrainingData

class Doc2VecModel(BaseEstimator):
    
    def __init__(self, epochs=1, vector_size=100, window=1):
        # Grid search to find best params for vector_sizes, epochs, window
        # Once we have best params for this, test on dm_mean, dm_concat, & dbow_words
        self.d2v_model = None
        self.token_path = './data/token/SARD/'
        self.d2vmodelPath = './d2vModel/model/d2vModel_ALL'
        self.epochs = epochs
        self.vector_size = vector_size
        self.window = window

    def fit(self, raw_documents, y=None):
        # Initialize, build, & train model
        self.d2v_model = Doc2Vec(documents=DirofCorpus(self.token_path), alpha=0.01, epochs=self.epochs,  min_count=0, max_vocab_size=None, sg=1, hs=0, negative=10, workers=1, window=self.window, vector_size=self.vector_size)

        # TODO: fit the model

        return self

    def transform(self, raw_documents):
        X = []
        for index, row in raw_documents.iteritems():
            X.append(self.d2v_model.infer_vector(row))
        X = pd.DataFrame(X, index=raw_documents.index)
        return X

    def fit_transform(self, raw_documents, y=None):
        self.fit(raw_documents)
        return self.transform(raw_documents)

def cv_silhouette_scorer(estimator, X):
    estimator.fit(X)
    cluster_labels = estimator.labels_
    num_labels = len(set(cluster_labels))
    num_samples = len(X)
    if num_labels == 1 or num_labels == num_samples:
        return -1
    else:
        return silhouette_score(X, cluster_labels)

param_grid = {'doc2vec__window': [1, 2, 3, 4, 5],
              'doc2vec__epochs': [1, 5],
              'doc2vec__vector_size': [100, 250],
              'agg__n_clusters': np.arange(4, 12),
}

pipe_log = Pipeline([('doc2vec', Doc2VecModel()), ('agg', AgglomerativeClustering(affinity='euclidean', linkage='ward'))])

log_grid = GridSearchCV(pipe_log, 
                        param_grid=param_grid,
                        scoring=cv_silhouette_scorer,
                        return_train_score=True,
                        verbose=1)

fitted = log_grid.fit(dataset["posts"], dataset["type"])

# Best parameters
print("Best Parameters: {}\n".format(log_grid.best_params_))
print("Best accuracy: {}\n".format(log_grid.best_score_))
print("Finished.")
"""