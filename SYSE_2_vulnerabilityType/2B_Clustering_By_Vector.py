#!/usr/bin/env python
# coding: utf-8

# # Method 2:  Clustering by Vector
# The goal remains to group weaknesses/vulnerabilities by similarity.  In method 1, group definitions were extracted from the [CWE website's](https://cwe.mitre.org/index.html), which contained a predefined hierarchical graph for CWE IDs.  Then, the tree was traversed to return a subset that contained the CWE IDs found in the original dataset (from the source code slices) and their overarching primary group listing.  This produced 15 groups that varied in size from 52 to 274,281.  For example, the source code contained 52 slices of code containing a potential CWE-398 weakness.  ['Code Quality' (CWE-398)](https://cwe.mitre.org/data/definitions/700.html) is a top-level category according to the CWE website.  No other CWE-IDs were obtained from our slices that fell under the CWE-398's branch, and therefore Group #14 is named 'Code Quality' and has a size of 52.  On the other hand, 'Improper Control of a Resource Through its Lifetime' (CWE-664) contained a majority of CWE-IDs (75 out of the 166 unique CWE-IDs in the source code slices) and sample counts, totaling 274,281 samples, which is approximately 66.3% of all the samples/groups.
# 
# In this method, an attempt is made to group source code slices by converting them into vectors & clustering the vectors with a similarity distance.
# 
# First, a preliminary cluster analysis will be made using the original source code & files to vectorize them.
# 
# Second, a cluster analysis will be made using a slightly modified source code which will include slices that have a groupid assigned to them.  This is because out of the 420,627 slices, 1,369 slices contained deprecated or obsolete CWE-IDs and 5,286 slices obtained by CVEs did not have a CWE-ID associated with them.  Thus, a maximum of 413,972 slices can be used in the training & testing samples in order to compare the accuracy results to the preliminary cluster analysis.
# 

# # 1. Preliminary Cluster Analysis
# Using the original source code & files to vectorize the source code for the clustering models.
# 
# ### Setting variables
import os
vType = "ALL"
randomSeed = 1099
numSamples = 250 #Max Num of slice samples from each file
vectorDim = 100 #num of vector cols
slicePath = './data/slicesSource/'
tokenPath = './data/token/SARD/'
d2vmodelPath = './d2vModel/model/d2vModel_ALL'
w2vmodelPath = './w2vModel/model/w2vModel_ALL'
vectorPath =  './data/vector/'
vectorTrainPath = './data/DLvectors/train/'
vectorTestPath = './data/DLvectors/test/'
dlInputsTrainPath = './data/DLinputs/train/'
dlInputsTestPath  = './data/DLinputs/test/'


# ### Updating path
# Must insert path to directory above in order to access files in main
import sys
sys.path.insert(1, '/Users/crystalcontreras/Desktop/DePaul/VulnerabilitiesResearch/vulnerability_detection')


# ### A. slicesToTokens.py
# Tokens are all the words in a slice mapped to predefined tokens.  Length of tokens varies by program (aka slice).
# Each test case ID will get its own pkl file (in the SARD directory) that contains the tokenized words in index 0, its label in index 2, etc.
# from SYSE_1_isVulnerable.slicesToTokens import tokenizeSlices
# mycase_ID = tokenizeSlices(slicePath, tokenPath, numSamples)


# ### B. isDuplicatedID.py
# from SYSE_1_isVulnerable.isDuplicatedID import isDuplicatedID
# print("The dataset has duplicated ID: ", isDuplicatedID(mycase_ID))


# ### C. tokensToVectors.py
# **Aim:** Transform tokens to vectors using the W2V model
# 
# Word2Vec model is a form of feature reduction. "New features are combinations of the original features." (*slide 25 of WQ2021_Week4_TextCategorization.pdf*)
# 
# - Represent each word with a low-dimensional vector
# - Word similarity = vector similarity
# 
# So, it takes all our tokens and reduces the ones that are similar, thus creating a vector per token.
# 
# Example: Program 1 contains 100 tokens (found in /SARD/testcase_id.pkl, index 0). Each token is then turned into a vector representation with 30 columns.  This would give us a 100 x 30 matrix, where each row is a token's vector representation.

from tokensToDocVectors import createD2VModel, fitD2VModel
myD2Vmodel = createD2VModel(d2vmodelPath, tokenPath, vectorDim)
fitD2VModel(d2vmodelPath, tokenPath, vectorPath)
print(myD2Vmodel)
"""
STOP HERE
"""

# **Input:** Tokens in index 0 in each .pkl file in './data/token/SARD'
# 
# **Output:**
# 1. W2V Model created and saved in './w2vModel/w2vModel_ALL'
# 2. Vocabs in W2V Model saved in wordsW2Vmodel.txt
# 3. Index 0 is transformed tokens-to-vectors saved in './data/vector/' 
#     1. For 1 program, vector array has 30 columns(Vdim), row = (#of tokens)
#     2. .pkl files of each program containing 5 items in array:
#     3. one token is transformed to 30 vectors as shown below.
# 
# ![image.png](attachment:image.png)
# 

# Can use Tensorflow to visualize all the vectors or points created from word2vec
# ![Screen%20Shot%202021-08-18%20at%2011.03.28%20AM.png](attachment:Screen%20Shot%202021-08-18%20at%2011.03.28%20AM.png)

# ### D. splitTrainTest.py
# Randomly shuffles vector pkl files to choose from. Combines all the [token vectors, labels, functions, filenames, vul' types, test case ids of slices] (one array w/6 nested arrays of those entries ^), & creates 2 files with this (training/testing).
# 
# Because of this, not sure if we can use it for our clustering purposes.  Thus, I will try the sklearn library's splitTrainTest function.
# 
# ```
# from SYSE_1_isVulnerable.splitTrainTest import *
# splitTrainTest(vType, vectorPath, vectorTrainPath, vectorTestPath,randomSeed, split = 0.8 )
# ```
# **Input:**
# - Vector files in './data/vector/'
# 
# **Output:**
# - 1. ALL_Train.pkl in './data/DLvectors/train/'
# - 1. ALL_Test.pkl in './data/DLvectors/test/'


from SYSE_1_isVulnerable.splitTrainTest import *
splitTrainTest(vType, vectorPath, vectorTrainPath, vectorTestPath,randomSeed, split = 0.8 )


# ### E. downSampling.py
# 
# **Input:**
# - Train set in train.pkl
# 
# **Output:**
# - Balanced Train set in balancedClassTrain.pkl, Saved in ./data/DLvectors/train/
# 
# <br />
# 
# **appendCaseIDLabel0**:  Takes a single pkl file, divides up the test case ids based on whether they have or don't have a vulnerability, and returns:
# - the case ids with vulnerabilities
# - the case ids without vulnerabilities
# - the count of case ids with vulnerabilities (for use on how much we should down size aka downSampleNum)
# 
# **downsampling**:  Reduces the amount of non vulnerable samples to match the amount of vulnerable samples in our train.pkl.  Outputs an array of arrays to ./data/DLvectors/train/balancedClassTrain.pkl.
# Combines all the [token vectors, labels, functions, filenames, vul' types, test case ids of slices] (one array w/6 nested arrays of these entries)


from SYSE_1_isVulnerable.downSampling import *
caseID_one,caseID_zero,downsampleNum = appendCaseIDLabel0(vectorTrainPath)
downsampling (caseID_one,caseID_zero, downsampleNum , randomSeed, vectorPath, vectorTrainPath)
#Optional used to check if the class label are balanced 
print("Class Label is balanced: " , isClassBalanced(vectorTrainPath))


# ### F. adjustVectorLen.py
# Why do we need this?  That way each program (aka slice) contains the same amount of rows/tokens in the data matrix. a program with 199 rows will reduce to maxlen/avg 9e.g. 64 rows0.  a program with < avg rows will fill the remaining rows with arrays of 0s.
# 
# **aim:** 
# - calculate Mean Vector Length
# - transform each sampleâ€™s "Vector Length" to Mean Length.
# (reduces row count (aka the # of tokens) in each sample)
# 
# **Input:**
# - Balanced Train & Test sets in ./data/DLvectors/
# 
# **Output:**
# - Balanced Train & Test sets with same length vectors in ./data/DLInputs/DL_Final_balanced*.pkl


from SYSE_1_isVulnerable.adjustVectorLen import *
avg = meanLen(vectorTrainPath)
tranformVectorLen(vectorTrainPath, vectorTestPath, dlInputsTrainPath, dlInputsTestPath, avg, vectorDim, vType)
print("New Vector Length (rows x cols): " ,avg, " x " ,vectorDim)


# ### G. saveKeyData.py
# Find out if I need this b/c was used to fit BGRU model in original project.
# 
# **Input:**
# - Balanced Final Train & Test Final sets in ./data/DLvectors/
# 
# **Output:**
# - ../data/DLinputs/train/KeyData_DL_Final_balancedClassTrain.pkl.txt, ../data/DLinputs/test/KeyData_DL_Final_ALL_test.pkl.txt
# 
# Format:
# 
# caseID, realLabel, vType
# 
# 135,1,PTR
# 
# 8322,0,API
# 
# ...

from SYSE_1_isVulnerable.saveKeyData import *
saveKeyData(dlInputsTrainPath)
saveKeyData(dlInputsTestPath)




# ### Part A. Clustering using sklearn's models
# Testing the accuracy of kmeans.  If we just cluster all the 


from sklearn.cluster import KMeans
kmeans = KMeans(n_clusters=4, max_iter=500, verbose=1)

from KMeansModel import getTrainingData
dataset, labels, testcases, vtypes = getTrainingData(dlInputsTrainPath, randomSeed)
kmeans.fit(dataset)
