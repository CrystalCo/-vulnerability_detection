#!/usr/bin/env python
# coding: utf-8

# # Method 2:  Clustering by Vector
# In this method, an attempt is made to group source code slices by converting them into vectors & clustering the vectors with a similarity distance.

# ### Setting variables
import os
vType = "ALL"
randomSeed = 1099
numSamples = 2500 #Max Num of slice samples from each file
vectorDim = 100 #num of vector cols
slicePath = './data/slicesSource/'
tokenPath = './data/token/SARD/'
d2vmodelPath = './d2vModel/model/d2vModel_ALL'
w2vmodelPath = './w2vModel/model/w2vModel_ALL'
vectorPath =  './data/vector/'
vectorTrainPath = './data/DLvectors/train/'
vectorTestPath = './data/DLvectors/test/'
dlInputsTrainPath = './data/DLinputs/train/'
dlInputsTestPath  = './data/DLinputs/test/'


# ### Updating path
# Must insert path to directory above in order to access files in main
import sys
VUL_PATH = os.environ.get('VUL_PATH')
sys.path.insert(1, VUL_PATH)


# ### A. slicesToTokens.py
# Tokens are all the words in a slice mapped to predefined tokens.  Length of tokens varies by program (aka slice).
# Each test case ID will get its own pkl file (in the SARD directory) that contains the tokenized words in index 0, its label in index 2, etc.
from SYSE_1_isVulnerable.slicesToTokens import tokenizeSlices
mycase_ID = tokenizeSlices(slicePath, tokenPath, numSamples)


# ### B. isDuplicatedID.py
from SYSE_1_isVulnerable.isDuplicatedID import isDuplicatedID
print("The dataset has duplicated ID: ", isDuplicatedID(mycase_ID))


# ### C. tokensToDocVectors.py
from tokensToDocVectors import createD2VModel, fitD2VModel
myD2Vmodel = createD2VModel(d2vmodelPath, tokenPath, vectorDim)
fitD2VModel(d2vmodelPath, tokenPath, vectorPath)


# ### D. splitTrainTest.py
# **Input:**
# - Vector files in './data/vector/'
# 
# **Output:**
# - 1. ALL_Train.pkl in './data/DLvectors/train/'
# - 1. ALL_Test.pkl in './data/DLvectors/test/'


from SYSE_1_isVulnerable.splitTrainTest import splitTrainTest
splitTrainTest(vType, vectorPath, vectorTrainPath, vectorTestPath,randomSeed, split = 0.8 )


# ### E. downSampling.py
# 
# **Input:**
# - Train set in train.pkl
# 
# **Output:**
# - Balanced Train set in balancedClassTrain.pkl, Saved in ./data/DLvectors/train/

from SYSE_1_isVulnerable.downSampling import appendCaseIDLabel0, downsampling, isClassBalanced
caseID_one,caseID_zero,downsampleNum = appendCaseIDLabel0(vectorTrainPath)
downsampling (caseID_one,caseID_zero, downsampleNum , randomSeed, vectorPath, vectorTrainPath)
#Optional used to check if the class label are balanced 
print("Class Label is balanced: " , isClassBalanced(vectorTrainPath))



# ### Part A. Grid Search on Clustering models with balanced dataset
from ClusteringMethods import getTrainingData
dataset, labels, filenames, testcases, vtypes = getTrainingData(vectorTrainPath, 1, randomSeed)

import numpy as np
import pandas as pd
from sklearn import metrics
from sklearn.model_selection import GridSearchCV
from sklearn.cluster import KMeans, AgglomerativeClustering
from matplotlib import pyplot as plt

from ClusteringMethods import cluster_sizes, print_cluster_sizes, plot_silhouettes, cv_silhouette_scorer, plot_params

# AgglomerativeClustering, where linkage=ward
n_clusters = np.arange(4,17)
affinity=['euclidean'] # Ward can only work on euclidean distance
linkage=['ward']
params = {'n_clusters': n_clusters, 'affinity': affinity, 'linkage': linkage}
model = AgglomerativeClustering()

grid = GridSearchCV(estimator=model, param_grid=params, scoring=cv_silhouette_scorer, return_train_score=True, verbose=1)
grid.fit(dataset)
print("best params for AgglomerativeClustering, where linkage=ward: ", grid.best_params_)
grid_cv_results = pd.DataFrame(grid.cv_results_)
fig=plt.figure(figsize=(5,3), dpi= 100, facecolor='w', edgecolor='k')
print("Hierarchical Clustering on Ward Linkage")
plot_params(n_clusters, 'K clusters', grid_cv_results['mean_train_score'], grid_cv_results['mean_test_score'])
plt.savefig('HierWard_GS_plot.png')

# AgglomerativeClustering, where linkage=avg
linkage=['average']
params = {'n_clusters': n_clusters, 'affinity': affinity, 'linkage': linkage}
model = AgglomerativeClustering()
grid = GridSearchCV(estimator=model, param_grid=params, scoring=cv_silhouette_scorer, return_train_score=True, verbose=1)
grid.fit(dataset)
print("best params for AgglomerativeClustering, where linkage=avg: ", grid.best_params_)
grid_cv_results = pd.DataFrame(grid.cv_results_)
fig=plt.figure(figsize=(5,3), dpi= 100, facecolor='w', edgecolor='k')
print("Hierarchical Clustering on Avg Linkage")
plot_params(n_clusters, 'K clusters', grid_cv_results['mean_train_score'], grid_cv_results['mean_test_score'])
plt.savefig('HierAvg_GS_plot.png')

# Kmeans Clustering grid search
params = {'n_clusters': n_clusters, 'max_iter': [500]}
model = KMeans()
grid = GridSearchCV(estimator=model, param_grid=params, scoring=cv_silhouette_scorer, return_train_score=True, verbose=1)
grid.fit(dataset)
print("best params for kmeans: ", grid.best_params_)
fig=plt.figure(figsize=(5,3), dpi= 100, facecolor='w', edgecolor='k')
print('KMeans Clustering')
plot_params(n_clusters, 'K clusters', grid.cv_results_['mean_train_score'], grid.cv_results_['mean_test_score'])
plt.savefig('KMeans_GS_plot.png')

