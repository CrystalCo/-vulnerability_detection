#!/usr/bin/env python
# coding: utf-8

# # Method 2:  Clustering by Vector
# In this method, an attempt is made to group source code slices by converting them into vectors & clustering the vectors with a similarity distance.

# ### Setting variables
import os
vType = "ALL"
randomSeed = 1099
numSamples = 250 #Max Num of slice samples from each file
vectorDim = 100 #num of vector cols
slicePath = './data/slicesSource/'
tokenPath = './data/token/SARD/'
d2vmodelPath = './d2vModel/model/d2vModel_ALL'
w2vmodelPath = './w2vModel/model/w2vModel_ALL'
vectorPath =  './data/vector/'
vectorTrainPath = './data/DLvectors/train/'
vectorTestPath = './data/DLvectors/test/'
dlInputsTrainPath = './data/DLinputs/train/'
dlInputsTestPath  = './data/DLinputs/test/'


# ### Updating path
# Must insert path to directory above in order to access files in main
import sys
VUL_PATH = os.environ.get('VUL_PATH')
sys.path.insert(1, VUL_PATH)


# ### A. slicesToTokens.py
# Tokens are all the words in a slice mapped to predefined tokens.  Length of tokens varies by program (aka slice).
# Each test case ID will get its own pkl file (in the SARD directory) that contains the tokenized words in index 0, its label in index 2, etc.
# from SYSE_1_isVulnerable.slicesToTokens import tokenizeSlices
# mycase_ID = tokenizeSlices(slicePath, tokenPath, numSamples)


# ### B. isDuplicatedID.py
# from SYSE_1_isVulnerable.isDuplicatedID import isDuplicatedID
# print("The dataset has duplicated ID: ", isDuplicatedID(mycase_ID))


# ### C. tokensToDocVectors.py
# from tokensToDocVectors import createD2VModel, fitD2VModel
# myD2Vmodel = createD2VModel(d2vmodelPath, tokenPath, vectorDim)
# fitD2VModel(d2vmodelPath, tokenPath, vectorPath)
# print("STOP HERE after fitting doc2vec model")


# ### D. splitTrainTest.py
# **Input:**
# - Vector files in './data/vector/'
# 
# **Output:**
# - 1. ALL_Train.pkl in './data/DLvectors/train/'
# - 1. ALL_Test.pkl in './data/DLvectors/test/'


from SYSE_1_isVulnerable.splitTrainTest import splitTrainTest
# splitTrainTest(vType, vectorPath, vectorTrainPath, vectorTestPath,randomSeed, split = 0.8 )


# ### E. downSampling.py
# 
# **Input:**
# - Train set in train.pkl
# 
# **Output:**
# - Balanced Train set in balancedClassTrain.pkl, Saved in ./data/DLvectors/train/

from SYSE_1_isVulnerable.downSampling import appendCaseIDLabel0, downsampling, isClassBalanced
# caseID_one,caseID_zero,downsampleNum = appendCaseIDLabel0(vectorTrainPath)
# downsampling (caseID_one,caseID_zero, downsampleNum , randomSeed, vectorPath, vectorTrainPath)
#Optional used to check if the class label are balanced 
# print("Class Label is balanced: " , isClassBalanced(vectorTrainPath))



# ### Part A. Clustering using sklearn's models
# Testing the accuracy of kmeans.  If we just cluster all the 


from sklearn.cluster import KMeans
kmeans = KMeans(n_clusters=4, max_iter=500, verbose=1)

from ClusteringMethods import getTrainingData
dataset, labels, filenames, testcases, vtypes = getTrainingData(vectorTrainPath, randomSeed)
kmeans.fit(dataset)
