import os, sys, gc

import numpy as np
from sklearn.model_selection import StratifiedKFold

VUL_PATH = os.environ.get('VUL_PATH')
sys.path.insert(1, VUL_PATH)

from SYSE_1_isVulnerable.adjustVectorLen import meanLen
from SYSE_2_vulnerabilityType.VulnerabilityClassification import VulnerabilityClassification
from utils.transformDataDimensions import getAvgLength, setTrimmedDataset
from utils.DLCustomModels import create_bgru_model, predictMulticlassLabelEncoderToHotEncoded
from utils.utils import getDataset, getDatasetSectionByIndex, getDatasetXY, hot_encode_target, save_data_to_file, preprocess_data_filter_by_count


class CrossValidation():
    
    def __init__(self, build_model, k, VUL_TYPE, MODEL_TYPE='bgru', vectorsALLdir='ALL_vectors_granular_vulnerable_only') -> None:
        self.model = build_model
        self.k = k

        self.MODEL_TYPE = MODEL_TYPE
        self.metric_names = ['Mean Recall', 'Mean FPR', 'Mean FNR', 'Mean F1-score', 'Weighted Recall', 'Weighted FPR', 'Weighted FNR', 'Weighted F1-score']
        self.metrics = [[], [], [], [], [], [], [], []]
        self.RANDOMSEED = 1099
        self.CLASS_TYPE = f'{VUL_TYPE}_muVDP_noMin'
        self.BATCHSIZE = 64
        self.EPOCHS = 60

        self.rootPath = os.path.join('data','DLvectors')
        self.vectorsALLPath = os.path.join(self.rootPath, vectorsALLdir)
        self.vectorTrainPath = os.path.join(self.rootPath, f'train')
        self.vectorTestPath = os.path.join(self.rootPath, f'test')
        self.rootPath = os.path.join('data', 'DLinputs')
        self.dlInputsTrainPath = os.path.join(self.rootPath, f'train')
        self.dlInputsTestPath = os.path.join(self.rootPath, f'test')

        self.checkpoint_dir = os.path.join('ckpt', f'ckpt_{str(self.BATCHSIZE)}_{self.CLASS_TYPE}')
        self.model_name = '%s_batch_%s_epochs_%s_%s' % (self.MODEL_TYPE.upper(), self.BATCHSIZE, self.EPOCHS, self.CLASS_TYPE)
        self.metrics_path = os.path.join(f'metrics', self.MODEL_TYPE)
        self.weights_path = os.path.join('model', self.model_name + '_weights')

        self.dvt = VulnerabilityClassification(build_model=self.model, metricsPath=self.metrics_path, 
            m_epochs=self.EPOCHS, modelName=self.model_name, vectorsALLPath=self.vectorsALLPath,
            vectorTrainPath=self.vectorTrainPath, vectorTestPath=self.vectorTestPath,
            inputsTrainPath=self.dlInputsTrainPath, inputsTestPath=self.dlInputsTestPath,
            checkpoint_dir=self.checkpoint_dir, weightpath=self.weights_path)

        print('Starting %s & %s' % (self.CLASS_TYPE, MODEL_TYPE))

    def set_data_to_minNum(self, inputpath, min_num):
        ## Preprocess the data
        # remove classes that don't have at least min_num samples
        data = getDataset(inputpath, False)
        data = preprocess_data_filter_by_count(min_num, data)
        # output path will be the vector all path
        save_data_to_file(self.dvt.vectorsALLPath, 'ALL_vectors.pkl', data)

    def flatten_3D_data(self):
        ## update vector length and avg
        X = getDatasetSectionByIndex(self.dvt.vectorsALLPath, False, index=0)
        # find out if dataset is already flattened
        self.dvt.avg = meanLen(self.dvt.vectorsALLPath, fName='ALL')
        if (self.dvt.avg != len(X[0]) or self.dvt.avg != len(X[1]) or self.dvt.avg != len(X[-1])):
        # trim by avg, then flatten 
            setTrimmedDataset(self.dvt.vectorsALLPath, self.dvt.vectorsALLPath, self.dvt.avg, self.dvt.vector_size, vType='ALL', outputFilename='ALL_vectors.pkl')
            # Flatten 3D vectors to 2D
            self.dvt.flatten_vectors(self.dvt.vectorsALLPath, getBalanced=False)
            # update vector length to new length and avg to 1
            self.update_vector_length_and_avg()

    def update_vector_length_and_avg(self):
        data = getDataset(self.dvt.vectorsALLPath, False)
        self.dvt.vector_size = len(data[0][0])
        self.dvt.avg = 1
        print(f'Avg: {self.dvt.avg}\tVector length: {self.dvt.vector_size}\n')
        del data
        gc.collect()

    def run_all(self):
        self.flatten_3D_data()
        ## update vector length and avg if need be
        if (self.dvt.avg != 1):
            self.update_vector_length_and_avg()

        # Split data into training and test set
        print('\nSplitting train/test...')
        X, y = getDatasetXY(self.dvt.vectorsALLPath, False, self.RANDOMSEED)
        y = np.array(y)
        skf = StratifiedKFold(n_splits=self.k, shuffle=True, random_state=self.RANDOMSEED)

        for train_index, test_index in skf.split(X, y):
            # Reset checkpoints
            print('Resetting checkpoint and weights...')
            self.dvt.reset_checkpoint_and_weights(self.dvt.weightpath)

            # Stratified k-fold
            X_train, X_test = X[train_index], X[test_index]
            y_train, y_test = y[train_index], y[test_index]

            # hot encode labels
            categories = np.unique(y_train)
            self.dvt.labelEncoder = hot_encode_target(categories)[1]
            self.dvt.density_units = categories.shape[0]
            encoded_y_train = self.dvt.labelEncoder.transform(y_train)

            # Build DL model
            myKerasModel, checkpointCallback, latest_epoch = self.dvt.build_estimator()

            # fit with generator
            self.dvt.fit_estimator_with_generator(X_train, encoded_y_train, myKerasModel, checkpointCallback, latest_epoch)

            # Predict & score on our test set (does not have synthetic upsampling)
            encoded_y_test = self.dvt.labelEncoder.transform(y_test)
            encoded_y_test, y_pred, _, _ = predictMulticlassLabelEncoderToHotEncoded(myKerasModel, X_test, encoded_y_test, self.dvt.avg, self.dvt.vector_size)

            metrics = self.dvt.score_predictions(y_test, y_pred)
            metric_names = [ 'Mean Precision', 'Mean Recall', 'Mean FPR', 'Mean FNR', 'Mean F1-score', 'Weighted    Precision', 'Weighted Recall', 'Weighted FPR', 'Weighted FNR', 'Weighted F1-score', 'Accuracy']
            for m, name in zip(metrics, metric_names):
                print('{}: {:.4f}'.format(name, m))

            self.metrics[0].append(metrics[1])
            self.metrics[1].append(metrics[2])
            self.metrics[2].append(metrics[3])
            self.metrics[3].append(metrics[4])
            self.metrics[4].append(metrics[6])
            self.metrics[5].append(metrics[7])
            self.metrics[6].append(metrics[8])
            self.metrics[7].append(metrics[9])

            print('\n\n\n\n\n\n')

        # Average out the scores
        print(f'Number of k-folds = {self.k}')
        for i in range(len(self.metrics)):
            avg = sum(self.metrics[i])/self.k
            print(f'Average score for {self.metric_names[i]}: {avg}')
        print('Length of metrics: %d ' % len(self.metrics[0]))




VUL_TYPE = 'API'
inputALLdir= os.path.join('data', 'DLvectors', 'ALL_vectors_muVDP')
vectorsALLdir='ALL_vectors'
min_number = 10
CV = CrossValidation(build_model=create_bgru_model, k=3, VUL_TYPE=VUL_TYPE, MODEL_TYPE='bgru', vectorsALLdir=vectorsALLdir)
CV.set_data_to_minNum(inputALLdir, min_number)
CV.run_all()

