import os, sys, gc

import numpy as np
from sklearn.model_selection import StratifiedKFold

VUL_PATH = os.environ.get('VUL_PATH')
sys.path.insert(1, VUL_PATH)

from SYSE_1_isVulnerable.adjustVectorLen import meanLen
from SYSE_2_vulnerabilityType.DetectVulType import DetectVulType
from utils.transformDataDimensions import setTrimmedDataset
from utils.DLCustomModels import create_bgru_model, predictMulticlassLabelEncoderToHotEncoded
from utils.SplitTrainTestMulticlass import preprocess_data_filter_by_count
from utils.utils import getDataset, getDatasetXY, hot_encode_target, save_data_to_file


class CrossValidation():
    
    def __init__(self, build_model, k) -> None:
        self.model = build_model
        self.k = k
        self.metric_names = ['Mean Recall', 'Mean FPR', 'Mean FNR', 'Mean F1-score', 'Weighted Recall', 'Weighted FPR', 'Weighted FNR', 'Weighted F1-score']
        self.metrics = [[], [], [], [], [], [], [], []]

    def run_all(self, VUL_TYPE, vectorsALLdir='ALL_vectors_granular_vulnerable_only', MODEL_TYPE='bgru', min_num=None):
        RANDOMSEED = 1099
        CLASS_TYPE = f'{VUL_TYPE}_muVDP_min_{min_num}'
        BATCHSIZE = 64
        EPOCHS = 60

        rootPath = os.path.join('data','DLvectors')
        vectorsALLPath = os.path.join(rootPath, vectorsALLdir)
        vectorTrainPath = os.path.join(rootPath, f'train')
        vectorTestPath = os.path.join(rootPath, f'test')
        rootPath = os.path.join('data', 'DLinputs')
        dlInputsTrainPath = os.path.join(rootPath, f'train')
        dlInputsTestPath = os.path.join(rootPath, f'test')

        checkpoint_dir = os.path.join('ckpt', f'ckpt_{str(BATCHSIZE)}_{CLASS_TYPE}')
        model_name = '%s_batch_%s_epochs_%s_%s' % (MODEL_TYPE.upper(), BATCHSIZE, EPOCHS, CLASS_TYPE)
        metrics_path = os.path.join(f'metrics', MODEL_TYPE)
        weights_path = os.path.join('model', model_name + '_weights')

        dvt = DetectVulType(build_model=self.model, metricsPath=metrics_path, 
            m_epochs=EPOCHS, modelName=model_name, vectorsALLPath=vectorsALLPath,
            vectorTrainPath=vectorTrainPath, vectorTestPath=vectorTestPath,
            inputsTrainPath=dlInputsTrainPath, inputsTestPath=dlInputsTestPath,
            checkpoint_dir=checkpoint_dir, weightpath=weights_path)

        print('Starting %s & %s' % (CLASS_TYPE, MODEL_TYPE))

        # Reset if need be
        print('Resetting checkpoint and weights...')
        dvt.reset_checkpoint_and_weights(dvt.weightpath)

        # Preprocess the data
        data = getDataset(dvt.vectorsALLPath, False)
        data = preprocess_data_filter_by_count(min_num, data)
        outputpath = os.path.join('data', 'DLvectors', 'ALL_vectors')
        save_data_to_file(outputpath, 'ALL_vectors.pkl', data)
        dvt.vectorsALLPath = outputpath

        # trim by avg, then flatten 
        dvt.avg = meanLen(dvt.vectorsALLPath, fName='ALL')
        setTrimmedDataset(dvt.vectorsALLPath, dvt.vectorsALLPath, dvt.avg, dvt.vector_size, vType='ALL', outputFilename='ALL_vectors.pkl')
         # Flatten 3D vectors to 2D
        dvt.flatten_vectors(dvt.vectorsALLPath, getBalanced=False)
        # update vector length and avg
        data = getDataset(dvt.vectorsALLPath, False)
        dvt.vector_size = len(data[0][0])
        dvt.avg = 1
        print(f'Avg: {dvt.avg}\tVector length: {dvt.vector_size}\n')
        del data
        gc.collect()

        # Split data into training and test set
        print('\nSplitting train/test...')
        X, y = getDatasetXY(dvt.vectorsALLPath, False, RANDOMSEED)
        y = np.array(y)
        skf = StratifiedKFold(n_splits=self.k, shuffle=True, random_state=RANDOMSEED)

        for train_index, test_index in skf.split(X, y):
            # Stratified k-fold
            print("TRAIN:", train_index, "TEST:", test_index)
            X_train, X_test = X[train_index], X[test_index]
            y_train, y_test = y[train_index], y[test_index]

            # hot encode labels
            categories = np.unique(y_train)
            dvt.labelEncoder = hot_encode_target(categories)[1]
            dvt.density_units = categories.shape[0]
            encoded_y_train = dvt.labelEncoder.transform(y_train)

            # Build DL model
            myKerasModel, checkpointCallback, latest_epoch = dvt.build_estimator()

            # fit with generator
            dvt.fit_estimator_with_generator(X_train, encoded_y_train, myKerasModel, checkpointCallback, latest_epoch)

            # Predict & score on our test set (does not have synthetic upsampling)
            encoded_y_test = dvt.labelEncoder.transform(y_test)
            y_true, y_pred, _, _ = predictMulticlassLabelEncoderToHotEncoded(self.model, X_test, encoded_y_test, dvt.avg, dvt.vector_size)

            metrics = dvt.score_predictions(y_true, y_pred)
            metric_names = [ 'Mean Precision', 'Mean Recall', 'Mean FPR', 'Mean FNR', 'Mean F1-score', 'Weighted    Precision', 'Weighted Recall', 'Weighted FPR', 'Weighted FNR', 'Weighted F1-score', 'Accuracy']
            for m, name in zip(metrics, metric_names):
                print('{}: {:.4f}'.format(name, m))

            self.metrics[0].append(metrics[1])
            self.metrics[1].append(metrics[2])
            self.metrics[2].append(metrics[3])
            self.metrics[3].append(metrics[4])
            self.metrics[4].append(metrics[6])
            self.metrics[5].append(metrics[7])
            self.metrics[6].append(metrics[8])
            self.metrics[7].append(metrics[9])

            print('\n\n\n\n\n\n')

        # Average out the scores
        print(f'Number of k-folds = {self.k}')
        for i in range(len(self.metrics)):
            avg = sum(self.metrics[i])/self.k
            print(f'Average score for {self.metric_names[i]}: {avg}')
        assert(len(self.metrics[0]) == self.k, 'length of k and metrics collected do not match')




VUL_TYPE = 'API'
vectorsALLdir='ALL_vulnerable_vectors_muVDP'
min_number = 100
CV = CrossValidation(build_model=create_bgru_model, k=3)
CV.run_all(VUL_TYPE=VUL_TYPE, vectorsALLdir=vectorsALLdir, MODEL_TYPE='bgru', min_num=min_number)

