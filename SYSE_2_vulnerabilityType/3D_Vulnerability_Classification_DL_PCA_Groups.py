import os, sys

import numpy as np
from sklearn.decomposition import PCA

VUL_PATH = os.environ.get('VUL_PATH')
sys.path.insert(1, VUL_PATH)

from SYSE_1_isVulnerable.preprocess_dl_Input_version5 import process_sequences_shape
from SYSE_1_isVulnerable.splitTrainTest import splitTrainTestCategorical
from SYSE_1_isVulnerable.adjustVectorLen import meanLen
from SYSE_1_isVulnerable.saveKeyData import saveKeyDataMulticlass
from SYSE_1_isVulnerable.ConfusionMatrix import getConfusionMatrix_Multiclass
from SYSE_2_vulnerabilityType.DetectVulType import DetectVulType
from utils.Doc2VecModel import Doc2VecModel
from utils.Word2VecModel import Word2VecModel
from utils.utils import encode_target, flatten_categories, getDataset, save_data_to_file
from utils.DLCustomModels import create_bgru_model, create_blstm_model, fit_custom_dl_model, make_or_restore_model, predictMulticlassLabel

################################## PCA on full dataset (Group IDs) #############################
############################################ SET UP ############################################
# ## W2V + BGRU Group 
print('Starting PCA script for W2V & BGRU group...')
RANDOMSEED = 1099
# VECTOR_SIZE = 30 #num of vector cols
TOKENS_PATH = os.path.join('data', 'tokens') # Contains ALL_tokens.pkl
TOKENS_GROUP_PATH = os.path.join('data', 'tokens_groups')
vectorRootPath = os.path.join('data','DLvectors')
vectorsALLPath = os.path.join(vectorRootPath, 'ALL_vectors_groups')
vectorTrainPath = os.path.join(vectorRootPath,'train')
vectorTestPath = os.path.join(vectorRootPath,'test')
dlInputsTrainPath = os.path.join('data','DLinputs','train')
dlInputsTestPath  = os.path.join('data','DLinputs','test')
pca_vectors = os.path.join(vectorRootPath, 'PCA_vectors')
pca_vectors_train = os.path.join(vectorRootPath, 'PCA_train')
pca_vectors_test = os.path.join(vectorRootPath, 'PCA_test')
pca_inputs_train_path = os.path.join('data', 'D2inputs', 'PCA_train')
pca_inputs_test_path = os.path.join('data', 'D2inputs', 'PCA_test')

CLASS_TYPE = 'Group_PCA'
MODEL_TYPE = 'bgru'
VECTOR_TRANSFORMER='w2v'
OPTIMIZER = 'adam' #can be changed to ‘adamax’
LAYERS = 2
DROPOUT = 0.2 
BATCHSIZE = 64
EPOCHS = 20

checkpoint_dir = './ckpt_%s_%s_%s_%s' % (VECTOR_TRANSFORMER, MODEL_TYPE, str(BATCHSIZE), CLASS_TYPE)
model_name = '%s_%s_batch=%s_seed=%s_%s' % (MODEL_TYPE.upper(), VECTOR_TRANSFORMER.upper(), BATCHSIZE, RANDOMSEED, CLASS_TYPE)
metrics_path = os.path.join(f'{VECTOR_TRANSFORMER}Model','metrics', MODEL_TYPE)
transformer_model_path = os.path.join(f'{VECTOR_TRANSFORMER}Model','model', MODEL_TYPE)

dvt = DetectVulType(build_model=create_bgru_model, useGenerator=True, layers=LAYERS,
                    transformer_model=Word2VecModel, transformerPath=transformer_model_path,
                    metricsPath=metrics_path, 
                    alpha=0.05, t_epochs=15, sample=0.001, workers=-1, negative=10, 
                    window=3, randomSeed=RANDOMSEED, 
                    m_epochs=EPOCHS, batch_size=BATCHSIZE, modelName=model_name, 
                    mask=False, tokensPath=TOKENS_PATH, dropout=DROPOUT, optimizer=OPTIMIZER,
                    vectorsALLPath=vectorsALLPath, vectorRootPath=vectorRootPath, 
                    vectorTrainPath=pca_vectors_train, vectorTestPath=pca_vectors_test, 
                    inputsTrainPath=pca_inputs_train_path, inputsTestPath=pca_inputs_test_path, 
                    checkpoint_dir=checkpoint_dir)
"""
dvt.init_transformer()
dvt.fit_transform(dvt.transformerPath, dvt.tokensPath, dvt.vectorsALLPath, False, index=0)
# PCA transformation
data = getDataset(dvt.vectorsALLPath, False, RANDOMSEED)
original_labels = data[-2]
categories = np.unique(original_labels)
_, labelEncoder = encode_target(categories)
DENSITY_UNITS = categories.shape[0]

# Flatten W2V matrices from 3D to 2D for PCA transformation
avg = meanLen(vectorTrainPath)
input_shape_1 = avg * VECTOR_SIZE
print("New vector size: ", str(input_shape_1))
x_train = process_sequences_shape(data[0], avg, VECTOR_SIZE)
x_train = np.reshape(x_train, (x_train.shape[0], input_shape_1))
# go up to 75% of the feature length
n_stop = int((x_train.shape[1]/4)*3)
step = int(n_stop/10)
n_components = np.arange(100, n_stop, step)
print(f'N components to test: {n_components}')

# Test which n will retain as much of the data's explanation
explained_variance_ratios = []
for n in n_components:
    pca_model = PCA(n_components=n)
    pca_model.fit(x_train)
    sum_expl_var = sum(pca_model.explained_variance_ratio_)
    explained_variance_ratios.append(sum_expl_var)
    print(f' Sum of variance ratios for n={n}: ', sum_expl_var)
    if sum_expl_var >= 0.99:
        break

n_index = len(explained_variance_ratios)-1 if len(explained_variance_ratios) > 0 else -1
print(f'Choosing n={n_components[n_index]} since that still leaves us with about {explained_variance_ratios[n_index]}% variance.')

# Fit PCA & Transform our vectors
pca_model = PCA(n_components=n_components[n_index])
pca_model.fit(x_train)
x_comps = pca_model.transform(x_train)
print(x_comps.shape)

data[0] = x_comps
save_data_to_file(pca_vectors, 'PCA_vectors_balanced.pkl', data)
"""
data = getDataset(pca_vectors, True, RANDOMSEED)
original_labels = data[-2]
categories = np.unique(original_labels)
_, labelEncoder = encode_target(categories)
DENSITY_UNITS = categories.shape[0]
VECTOR_SIZE = len(data[0][0])

print(f'BUILDING MODEL {model_name}')
dvt.avg = 1
dvt.vector_size = VECTOR_SIZE
dvt.labelEncoder = labelEncoder
dvt.density_units = DENSITY_UNITS
dvt.splitTrainTest(pca_vectors, minSamples=None)
dvt.saveKeyData(pca_vectors_train, pca_inputs_train_path)
dvt.saveKeyData(pca_vectors_test, pca_inputs_test_path)
dvt.build_and_fit()
dvt.predict_and_score()





# ## W2V + BLSTM Group 
CLASS_TYPE = 'Group_PCA'
MODEL_TYPE = 'blstm'
VECTOR_TRANSFORMER='w2v'

checkpoint_dir = './ckpt_%s_%s_%s_%s' % (VECTOR_TRANSFORMER, MODEL_TYPE, str(BATCHSIZE), CLASS_TYPE)
model_name = '%s_%s_batch=%s_seed=%s_%s' % (MODEL_TYPE.upper(), VECTOR_TRANSFORMER.upper(), BATCHSIZE, RANDOMSEED, CLASS_TYPE)
metrics_path = os.path.join(f'{VECTOR_TRANSFORMER}Model','metrics', MODEL_TYPE)

dvt = DetectVulType(build_model=create_blstm_model, useGenerator=True, layers=LAYERS,
                    metricsPath=metrics_path, vector_size=VECTOR_SIZE, randomSeed=RANDOMSEED, 
                    m_epochs=EPOCHS, batch_size=BATCHSIZE, modelName=model_name, 
                    mask=False, tokensPath=TOKENS_PATH, dropout=DROPOUT, optimizer=OPTIMIZER,
                    vectorsALLPath=vectorsALLPath, vectorRootPath=vectorRootPath, 
                    vectorTrainPath=pca_vectors_train, vectorTestPath=pca_vectors_test, 
                    inputsTrainPath=pca_inputs_train_path, inputsTestPath=pca_inputs_test_path, 
                    checkpoint_dir=checkpoint_dir)

print(f'BUILDING MODEL {model_name}')
dvt.avg = 1
dvt.vector_size = VECTOR_SIZE
dvt.labelEncoder = labelEncoder
dvt.density_units = DENSITY_UNITS
dvt.build_and_fit()
dvt.predict_and_score()

"""




################################## PCA on full dataset (Group IDs) 60 epochs #############################
############################################ SET UP ############################################
# ## W2V + BGRU Group 
print('Starting PCA script for W2V & BGRU group ids for 60 epochs...')
RANDOMSEED = 1099
CLASS_TYPE = 'Group_PCA'
MODEL_TYPE = 'bgru'
VECTOR_TRANSFORMER='w2v'
OPTIMIZER = 'adam' #can be changed to ‘adamax’
LAYERS = 2
DROPOUT = 0.2 
BATCHSIZE = 64
EPOCHS = 60
TOKENS_PATH = os.path.join('data', 'tokens') # Contains ALL_tokens.pkl
TOKENS_GROUP_PATH = os.path.join('data', 'tokens_groups')
vectorRootPath = os.path.join('data','DLvectors')
vectorsALLPath = os.path.join(vectorRootPath, 'ALL_vectors_groups')
vectorTrainPath = os.path.join(vectorRootPath,'train')
vectorTestPath = os.path.join(vectorRootPath,'test')
dlInputsTrainPath = os.path.join('data','DLinputs','train')
dlInputsTestPath  = os.path.join('data','DLinputs','test')
pca_vectors = os.path.join(vectorRootPath, 'PCA_vectors')
pca_vectors_train = os.path.join(vectorRootPath, 'PCA_train')
pca_vectors_test = os.path.join(vectorRootPath, 'PCA_test')
pca_inputs_train_path = os.path.join('data', 'D2inputs', 'PCA_train')
pca_inputs_test_path = os.path.join('data', 'D2inputs', 'PCA_test')


data = getDataset(pca_vectors_train, True, RANDOMSEED)
VECTOR_SIZE = len(data[0][0])

checkpoint_dir = './ckpt_%s_%s_%s_%s' % (VECTOR_TRANSFORMER, MODEL_TYPE, str(BATCHSIZE), CLASS_TYPE)
model_name = '%s_%s_batch=%s_seed=%s_%s' % (MODEL_TYPE.upper(), VECTOR_TRANSFORMER.upper(), BATCHSIZE, RANDOMSEED, CLASS_TYPE)
metrics_path = os.path.join(f'{VECTOR_TRANSFORMER}Model','metrics', MODEL_TYPE)

dvt = DetectVulType(build_model=create_bgru_model, useGenerator=True, layers=LAYERS,
                    metricsPath=metrics_path, vector_size=VECTOR_SIZE, randomSeed=RANDOMSEED,
                    m_epochs=EPOCHS, batch_size=BATCHSIZE, modelName=model_name,
                    mask=False, dropout=DROPOUT, optimizer=OPTIMIZER,
                    vectorsALLPath=vectorsALLPath, vectorRootPath=vectorRootPath, 
                    vectorTrainPath=pca_vectors_train, vectorTestPath=pca_vectors_test, 
                    inputsTrainPath=pca_inputs_train_path, inputsTestPath=pca_inputs_test_path, 
                    checkpoint_dir=checkpoint_dir)


data[0] = x_comps
save_data_to_file(pca_vectors, 'PCA_vectors_balanced.pkl', data)

print(f'BUILDING MODEL {model_name}')
dvt.avg = 1
dvt.vector_size = VECTOR_SIZE
dvt.labelEncoder = labelEncoder
dvt.density_units = DENSITY_UNITS
dvt.splitTrainTest(pca_vectors, minSamples=900)
dvt.saveKeyData(pca_vectors_train, pca_inputs_train_path)
dvt.saveKeyData(pca_vectors_test, pca_inputs_test_path)
dvt.build_and_fit()
dvt.predict_and_score()
"""




