import gc, os, sys

import numpy as np
from sklearn.decomposition import PCA

VUL_PATH = os.environ.get('VUL_PATH')
sys.path.insert(1, VUL_PATH)

from SYSE_1_isVulnerable.preprocess_dl_Input_version5 import process_sequences_shape
from SYSE_1_isVulnerable.adjustVectorLen import meanLen
from SYSE_1_isVulnerable.ConfusionMatrix import getConfusionMatrix_Multiclass
from SYSE_2_vulnerabilityType.DetectVulType import DetectVulType
from utils.Doc2VecModel import Doc2VecModel
from utils.Word2VecModel import Word2VecModel
from utils.utils import encode_target, flatten_categories, getDataset, save_data_to_file
from utils.DLCustomModels import create_bgru_model, create_blstm_model

################################## PCA on full dataset (Group IDs) #############################
############################################ SET UP ############################################
# ## W2V + BGRU Group 
print('Starting PCA script for W2V & BGRU group...')
RANDOMSEED = 1099
# VECTOR_SIZE = 30 #num of vector cols
TOKENS_PATH = os.path.join('data', 'tokens') # Contains ALL_tokens.pkl
TOKENS_GROUP_PATH = os.path.join('data', 'tokens_groups')
vectorRootPath = os.path.join('data','DLvectors')
vectorsALLPath = os.path.join(vectorRootPath, 'ALL_vectors')
vectorsALLGroupPath = os.path.join(vectorRootPath, 'ALL_vectors_groups')
vectorTrainPath = os.path.join(vectorRootPath,'train_group')
vectorTestPath = os.path.join(vectorRootPath,'test_group')
dlInputs = os.path.join('data', 'DLinputs')
dlInputsTrainPath = os.path.join(dlInputs,'train_group')
dlInputsTestPath  = os.path.join(dlInputs,'test_group')
pca_vectors = os.path.join(vectorRootPath, 'PCA_vectors')
pca_vectors_train = os.path.join(vectorRootPath, 'PCA_train')
pca_vectors_test = os.path.join(vectorRootPath, 'PCA_test')
pca_inputs_train_path = os.path.join(dlInputs,'PCA_train')
pca_inputs_test_path = os.path.join(dlInputs,'PCA_test')

CLASS_TYPE = 'Group_PCA'
MODEL_TYPE = 'bgru'
VECTOR_TRANSFORMER='w2v'
OPTIMIZER = 'adam'
LAYERS = 2
DROPOUT = 0.2 
BATCHSIZE = 64
EPOCHS = 60

checkpoint_dir = './ckpt_%s_%s_%s_%s' % (VECTOR_TRANSFORMER, MODEL_TYPE, str(BATCHSIZE), CLASS_TYPE)
model_name = '%s_%s_batch=%s_seed=%s_epochs=%s_%s' % (MODEL_TYPE.upper(), VECTOR_TRANSFORMER.upper(), BATCHSIZE, RANDOMSEED, EPOCHS, CLASS_TYPE)
metrics_path = os.path.join(f'{VECTOR_TRANSFORMER}Model','metrics', MODEL_TYPE)
transformer_model_path = os.path.join(f'{VECTOR_TRANSFORMER}Model','model', MODEL_TYPE)
weights_path = os.path.join('model', model_name + OPTIMIZER + str(RANDOMSEED))

dvt = DetectVulType(build_model=create_bgru_model, useGenerator=True, layers=LAYERS,
                    transformer_model=Word2VecModel, transformerPath=transformer_model_path,
                    metricsPath=metrics_path, 
                    alpha=0.05, t_epochs=15, sample=0.001, workers=-1, negative=10, 
                    window=3, randomSeed=RANDOMSEED, 
                    m_epochs=EPOCHS, batch_size=BATCHSIZE, modelName=model_name, 
                    mask=False, tokensPath=TOKENS_PATH, dropout=DROPOUT, optimizer=OPTIMIZER,
                    vectorsALLPath=vectorsALLGroupPath, vectorRootPath=vectorRootPath,
                    vectorTrainPath=vectorTrainPath, vectorTestPath=vectorTestPath,
                    inputsTrainPath=pca_vectors_train, inputsTestPath=pca_vectors_test, 
                    checkpoint_dir=checkpoint_dir, weightpath=weights_path)
# Save just group labels in own file
data = getDataset(TOKENS_PATH, False, RANDOMSEED)
data_filenames = data[3][:5]
print('First 5 filenames: ', data_filenames)
print('First 5 labels: ', data[-2][:5])
print('First 5 filename corpus id: ', data[-1][:5])
group_labels = flatten_categories(data[-2], 0)
group_labels_length = len(group_labels)
print('group labels length = ', group_labels_length)
data[-2] = group_labels
print('unique labels: ', np.unique(group_labels))
save_data_to_file(TOKENS_GROUP_PATH, 'ALL_group_tokens.pkl', data)
del data
gc.collect()

# Overwrite Vector file with group labels
data = getDataset(vectorsALLPath, False)
VECTOR_SIZE = len(data[0][0][0])
print(f'VECTOR SIZE: ', VECTOR_SIZE)
data_filenames_vec = data[3][:5]
print('First 5 filenames: ', data_filenames_vec)
print('First 5 labels: ', data[-2][:5])
print('First 5 filename corpus id: ', data[-1][:5])
group_labels_length_vec = len(data[-2])
print('group labels length = ', group_labels_length_vec)
assert group_labels_length == group_labels_length_vec, f'number of classes in tokens group path {group_labels_length} doesnt match the ones in vectors all group path {group_labels_length_vec}'
assert data_filenames == data_filenames_vec, f'first 5 filenames dont match.  Need to sort.'
data[-2] = group_labels
print('unique labels: ', np.unique(group_labels))
save_data_to_file(vectorsALLGroupPath, 'ALL_vectors_groups.pkl', data)
del data
gc.collect()

dvt.splitTrainTest(vectorsALLGroupPath)
# This will save our truncated matrices to pca_vector_train and pca_vector_test
dvt.adjustVectorLength()

# PCA transformation 
# Get best n on training, then apply to train and test sets. inputsTrainPath is set to pca_vectors_train.
data = getDataset(dvt.inputsTrainPath, True, RANDOMSEED)

# Flatten W2V matrices from 3D to 2D for PCA transformation
avg = dvt.avg
print(f'Avg: ', avg)
input_shape_1 = avg * VECTOR_SIZE
print("New vector size will be: 1x", str(input_shape_1))
# Convert to np arrays so we can reshape using numpy
x_train = process_sequences_shape(data[0], avg, VECTOR_SIZE)
# reshape from 3D to 2D so PCA() will accept
x_train = np.reshape(x_train, (x_train.shape[0], input_shape_1))
# go up to 80% of the feature length
n_stop = int((x_train.shape[1]/5)*4)
step = int(n_stop/10)
n_components = np.arange(100, n_stop, step)
print(f'N components to test: {n_components}')

# Test which n will retain as much of the data's explanation
explained_variance_ratios = []
for n in n_components:
    pca_model = PCA(n_components=n)
    pca_model.fit(x_train)
    sum_expl_var = sum(pca_model.explained_variance_ratio_)
    explained_variance_ratios.append(sum_expl_var)
    print(f' Sum of variance ratios for n={n}: ', sum_expl_var)
    if sum_expl_var >= 0.99:
        break

n_index = len(explained_variance_ratios)-1 if len(explained_variance_ratios) > 0 else -1
print(f'Choosing n={n_components[n_index]} since that still leaves us with about {explained_variance_ratios[n_index]}% variance.')
# Fit PCA & Transform our training vectors
pca_model = PCA(n_components=n_components[n_index])
pca_model.fit(x_train)
x_comps = pca_model.transform(x_train)
print(x_comps.shape)
data[0] = x_comps
save_data_to_file(dvt.inputsTrainPath, 'balanced_train.pkl', data)



# Fit PCA & Transform our test vectors
data = getDataset(dvt.inputsTestPath, True, RANDOMSEED)
# Flatten W2V matrices from 3D to 2D for PCA transformation
x_train = process_sequences_shape(data[0], avg, VECTOR_SIZE)
x_train = np.reshape(x_train, (x_train.shape[0], input_shape_1))
pca_model = PCA(n_components=n_components[n_index])
pca_model.fit(x_train)
x_comps = pca_model.transform(x_train)
print(x_comps.shape)
data[0] = x_comps
save_data_to_file(dvt.inputsTestPath, 'balanced_test.pkl', data)

dvt.vectorTrainPath = pca_vectors_train
dvt.vectorTestPath = pca_vectors_test
dvt.inputsTrainPath = pca_inputs_train_path
dvt.inputsTestPath = pca_inputs_test_path
dvt.encodeLabels()
dvt.saveKeyData(dvt.vectorTrainPath, dvt.inputsTrainPath)
dvt.saveKeyData(dvt.vectorTestPath, dvt.inputsTestPath)
VECTOR_SIZE = n_components[n_index]
dvt.vector_size = VECTOR_SIZE
dvt.avg = 1
print(f'Density units: {dvt.density_units}. Vector size: {dvt.vector_size}')
dvt.reset_check_weights(weights_path)
dvt.build_and_fit()
dvt.predict_and_score()
















"""


# ## W2V + BLSTM Group 
CLASS_TYPE = 'Group_PCA'
MODEL_TYPE = 'blstm'
VECTOR_TRANSFORMER='w2v'

checkpoint_dir = './ckpt_%s_%s_%s_%s' % (VECTOR_TRANSFORMER, MODEL_TYPE, str(BATCHSIZE), CLASS_TYPE)
model_name = '%s_%s_batch=%s_seed=%s_%s' % (MODEL_TYPE.upper(), VECTOR_TRANSFORMER.upper(), BATCHSIZE, RANDOMSEED, CLASS_TYPE)
metrics_path = os.path.join(f'{VECTOR_TRANSFORMER}Model','metrics', MODEL_TYPE)

dvt = DetectVulType(build_model=create_blstm_model, useGenerator=True, layers=LAYERS,
                    metricsPath=metrics_path, vector_size=VECTOR_SIZE, randomSeed=RANDOMSEED, 
                    m_epochs=EPOCHS, batch_size=BATCHSIZE, modelName=model_name, 
                    mask=False, tokensPath=TOKENS_PATH, dropout=DROPOUT, optimizer=OPTIMIZER,
                    vectorsALLPath=vectorsALLPath, vectorRootPath=vectorRootPath, 
                    vectorTrainPath=pca_vectors_train, vectorTestPath=pca_vectors_test, 
                    inputsTrainPath=pca_inputs_train_path, inputsTestPath=pca_inputs_test_path, 
                    checkpoint_dir=checkpoint_dir)

print(f'BUILDING MODEL {model_name}')
dvt.avg = 1
dvt.vector_size = VECTOR_SIZE
dvt.labelEncoder = labelEncoder
dvt.density_units = DENSITY_UNITS
dvt.build_and_fit()
dvt.predict_and_score()





################################## PCA on full dataset (Group IDs) 60 epochs #############################
############################################ SET UP ############################################
# ## W2V + BGRU Group 
print('Starting PCA script for W2V & BGRU group ids for 60 epochs...')
RANDOMSEED = 1099
CLASS_TYPE = 'Group_PCA'
MODEL_TYPE = 'bgru'
VECTOR_TRANSFORMER='w2v'
OPTIMIZER = 'adam' 
LAYERS = 2
DROPOUT = 0.2 
BATCHSIZE = 64
EPOCHS = 60
TOKENS_PATH = os.path.join('data', 'tokens') # Contains ALL_tokens.pkl
TOKENS_GROUP_PATH = os.path.join('data', 'tokens_groups')
vectorRootPath = os.path.join('data','DLvectors')
vectorsALLPath = os.path.join(vectorRootPath, 'ALL_vectors_groups')
vectorTrainPath = os.path.join(vectorRootPath,'train')
vectorTestPath = os.path.join(vectorRootPath,'test')
dlInputs = os.path.join('data', 'DLinputs')
dlInputsTrainPath = os.path.join(dlInputs,'train')
dlInputsTestPath  = os.path.join(dlInputs,'test')
pca_vectors = os.path.join(vectorRootPath, 'PCA_vectors')
pca_vectors_train = os.path.join(vectorRootPath, 'PCA_train')
pca_vectors_test = os.path.join(vectorRootPath, 'PCA_test')
pca_inputs_train_path = os.path.join(dlInputs,'PCA_train')
pca_inputs_test_path = os.path.join(dlInputs,'PCA_test')

checkpoint_dir = './ckpt_%s_%s_%s_%s' % (VECTOR_TRANSFORMER, MODEL_TYPE, str(BATCHSIZE), CLASS_TYPE)
model_name = '%s_%s_batch=%s_seed=%s_epochs=%s_%s' % (MODEL_TYPE.upper(), VECTOR_TRANSFORMER.upper(), BATCHSIZE, RANDOMSEED, EPOCHS, CLASS_TYPE)
metrics_path = os.path.join(f'{VECTOR_TRANSFORMER}Model', 'metrics', MODEL_TYPE)
weights_path = os.path.join('model', model_name + OPTIMIZER + str(RANDOMSEED))

dvt = DetectVulType(build_model=create_bgru_model, useGenerator=True, layers=LAYERS,
                    metricsPath=metrics_path, vector_size=VECTOR_SIZE, randomSeed=RANDOMSEED,
                    m_epochs=EPOCHS, batch_size=BATCHSIZE, modelName=model_name,
                    mask=False, dropout=DROPOUT, optimizer=OPTIMIZER,
                    vectorsALLPath=vectorsALLPath, vectorRootPath=vectorRootPath, 
                    vectorTrainPath=pca_vectors_train, vectorTestPath=pca_vectors_test, 
                    inputsTrainPath=pca_inputs_train_path, inputsTestPath=pca_inputs_test_path, 
                    checkpoint_dir=checkpoint_dir, weightpath=weights_path)
dvt.avg = 1
dvt.vector_size = VECTOR_SIZE
dvt.labelEncoder = labelEncoder
dvt.density_units = DENSITY_UNITS
dvt.build_and_fit()
dvt.predict_and_score()

data = getDataset(pca_vectors_train, True, RANDOMSEED)
categories = np.unique(data[-2])
_, labelEncoder = encode_target(categories)
DENSITY_UNITS = categories.shape[0]
VECTOR_SIZE = len(data[0][0])
print(f'Density units: {DENSITY_UNITS}. Vector size: {VECTOR_SIZE}')

# ## W2V + BLSTM Group 
print('Starting PCA script for W2V & BLSTM group ids for 60 epochs...')
RANDOMSEED = 1099
CLASS_TYPE = 'Group_PCA'
MODEL_TYPE = 'blstm'
VECTOR_TRANSFORMER='w2v'
OPTIMIZER = 'adam' 
LAYERS = 2
DROPOUT = 0.2 
BATCHSIZE = 64
EPOCHS = 60
TOKENS_PATH = os.path.join('data', 'tokens') # Contains ALL_tokens.pkl
TOKENS_GROUP_PATH = os.path.join('data', 'tokens_groups')
vectorRootPath = os.path.join('data','DLvectors')
vectorsALLPath = os.path.join(vectorRootPath, 'ALL_vectors_groups')
vectorTrainPath = os.path.join(vectorRootPath,'train')
vectorTestPath = os.path.join(vectorRootPath,'test')
dlInputsTrainPath = os.path.join(dlInputs,'train')
dlInputsTestPath  = os.path.join(dlInputs,'test')
pca_vectors = os.path.join(vectorRootPath, 'PCA_vectors')
pca_vectors_train = os.path.join(vectorRootPath, 'PCA_train')
pca_vectors_test = os.path.join(vectorRootPath, 'PCA_test')
pca_inputs_train_path = os.path.join(dlInputs,'PCA_train')
pca_inputs_test_path = os.path.join(dlInputs,'PCA_test')

data = getDataset(pca_vectors_train, True, RANDOMSEED)
categories = np.unique(data[-2])
_, labelEncoder = encode_target(categories)
DENSITY_UNITS = categories.shape[0]
VECTOR_SIZE = len(data[0][0])

checkpoint_dir = './ckpt_%s_%s_%s_%s' % (VECTOR_TRANSFORMER, MODEL_TYPE, str(BATCHSIZE), CLASS_TYPE)
model_name = '%s_%s_batch=%s_seed=%s_epochs=%s_%s' % (MODEL_TYPE.upper(), VECTOR_TRANSFORMER.upper(), BATCHSIZE, RANDOMSEED, EPOCHS, CLASS_TYPE)
metrics_path = os.path.join(f'{VECTOR_TRANSFORMER}Model', 'metrics', MODEL_TYPE)
weights_path = os.path.join('model', model_name + OPTIMIZER + str(RANDOMSEED))

dvt = DetectVulType(build_model=create_blstm_model, useGenerator=True, layers=LAYERS,
                    metricsPath=metrics_path, vector_size=VECTOR_SIZE, randomSeed=RANDOMSEED,
                    m_epochs=EPOCHS, batch_size=BATCHSIZE, modelName=model_name,
                    mask=False, dropout=DROPOUT, optimizer=OPTIMIZER,
                    vectorsALLPath=vectorsALLPath, vectorRootPath=vectorRootPath, 
                    vectorTrainPath=pca_vectors_train, vectorTestPath=pca_vectors_test, 
                    inputsTrainPath=pca_inputs_train_path, inputsTestPath=pca_inputs_test_path, 
                    checkpoint_dir=checkpoint_dir, weightpath=weights_path)

dvt.avg = 1
dvt.vector_size = VECTOR_SIZE
dvt.labelEncoder = labelEncoder
dvt.density_units = DENSITY_UNITS
dvt.build_and_fit()
dvt.predict_and_score()


"""