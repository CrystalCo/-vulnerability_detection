"""
    This file starts from the very beginning by creating the tokenized dataset based on the
    vulnerability type passed in.  It trains a W2V model on the new set, splits into training/test
    set folders named by type, flattens the vectors, then builds and tests a DL model on that data.

    Since the first portion (BGRU) runs all of the above actions, the second portion (BLSTM)
    has lines commented out that would create redundant work.
"""

import gc, os, sys

VUL_PATH = os.environ.get('VUL_PATH')
sys.path.insert(1, VUL_PATH)

from SYSE_2_vulnerabilityType.VulnerabilityClassification import VulnerabilityClassification
from utils.Word2VecModel import Word2VecModel
from utils.DLCustomModels import create_bgru_model, create_blstm_model
from utils.utils import GetData, GetDataByIndex, getDataset


def run_all(VUL_TYPE, slicefile, vectorsALLdir, MODEL_TYPE='bgru', model_fn=create_bgru_model, min_num=None):
    RANDOMSEED = 1099
    BATCHSIZE = 64
    LAYERS = 2
    DROPOUT = 0.2
    EPOCHS = 60
    UNITS = 256
    VECTOR_TRANSFORMER='w2v'
    CLASS_TYPE = f'{VUL_TYPE}_min_{min_num}'
    OPTIMIZER = 'ADAM'
    ACTIVATION_FN = 'sigmoid'
    RECURRENT_ACTIVATION = 'hard_sigmoid'
    DENSE_ACT_FN = 'softmax'
    METRICS = ['CategoricalAccuracy', 'Recall']

    all_tokens_path = os.path.join('data', 'tokens')
    inputsRootPath = os.path.join('data', 'DLinputs')
    vectorRootPath = os.path.join('data', 'DLvectors')
    vectorsALLPath = os.path.join(vectorRootPath, vectorsALLdir)
    vectorTrainPath = os.path.join(vectorRootPath, 'train')
    vectorTestPath = os.path.join(vectorRootPath, 'test')
    dlInputsTrainPath = os.path.join(inputsRootPath, 'train')
    dlInputsTestPath = os.path.join(inputsRootPath, 'test')

    checkpoint_dir = './ckpt_%s_%s_%s_%s' % (VECTOR_TRANSFORMER, MODEL_TYPE, str(BATCHSIZE), CLASS_TYPE)
    model_name = '%s_%s_epochs_%s_opt_%s_%s_%s_%sunits_%s' % (MODEL_TYPE.upper(), VECTOR_TRANSFORMER.upper(), EPOCHS, OPTIMIZER, ACTIVATION_FN, DENSE_ACT_FN, UNITS, CLASS_TYPE)
    metrics_path = os.path.join(f'metrics', MODEL_TYPE)
    weights_path = os.path.join('model', model_name + '_weights')
    w2vmodelPath = os.path.join('w2vModel','model', f'w2vModel_{VUL_TYPE}')

    VC = VulnerabilityClassification(build_model=model_fn,
        transformer_model=Word2VecModel, transformerPath=w2vmodelPath,
        metricsPath=metrics_path, randomSeed=RANDOMSEED, window=3, m_epochs=EPOCHS,
        modelName=model_name, batch_size=BATCHSIZE, mask=True, dropout=DROPOUT, layers=LAYERS,
        tokensPath=all_tokens_path, vectorsALLPath=vectorsALLPath,
        vectorTrainPath=vectorTrainPath, vectorTestPath=vectorTestPath,
        inputsTrainPath=dlInputsTrainPath, inputsTestPath=dlInputsTestPath,
        checkpoint_dir=checkpoint_dir, weightpath=weights_path, optimizer=OPTIMIZER,
        activation_fn=ACTIVATION_FN, recurrent_activation=RECURRENT_ACTIVATION, dense_activation_fn=DENSE_ACT_FN,
        units=UNITS, metrics=METRICS)


    print('Starting %s & %s' % (VUL_TYPE, MODEL_TYPE))

    # Reset if need be
    print('Resetting checkpoint and weights...')
    VC.reset_checkpoint_and_weights(VC.weightpath)

    # scrape data only from file
    print('\nTokenizing slices...')
    all_tokensPath = os.path.join(VC.tokensPath, 'ALL_tokens.pkl') # save to data/token/vul_type/
    VC.tokenizeSlicesPerFile(slicefile, all_tokensPath)

    # train W2V model
    print('\nTraining W2V model...')
    VC.init_transformer()
    # Fit transformer & transform our data
    VC.fit_transform(VC.transformerPath, VC.tokensPath, VC.vectorsALLPath, getBalanced=False)

    # Split data into training and test set
    print('\nSplitting train/test...')
    VC.splitTrainTest(VC.vectorsALLPath, min_num=min_num, vType=VUL_TYPE, dropOtherVtypes=True)

    # To flatten dataset, we must average out the dimension of the dataset which contains the tokens. AverageS out the row length per sample based on focuspointer.
    print('\nAdjusting Vector Length...')
    input_path = os.path.join(VC.vectorTrainPath, 'balanced_train.pkl')
    output_path = os.path.join(VC.inputsTrainPath, 'DL_Final_balanced_train.pkl')
    VC.SetVLength(input_path, output_path)
    # # Flatten 3D vectors to 2D
    # FlattenVectors(output_path, output_path, VC.avg, VC.vector_size)
    input_path = os.path.join(VC.vectorTestPath, 'balanced_test.pkl')
    output_path = os.path.join(VC.inputsTestPath, 'DL_Final_balanced_test.pkl')
    VC.SetVLength(input_path, output_path)
    # FlattenVectors(output_path, output_path, VC.avg, VC.vector_size)

    VC.vector_size = len(GetDataByIndex(output_path, index=0)[0][0])
    VC.avg = 1
    print(f'Avg: {VC.avg}\tVector length: {VC.vector_size}\n')

    # Build dl model & predict results
    VC.hotEncodeLabels()
    input_path = os.path.join(VC.inputsTrainPath, 'DL_Final_balanced_train.pkl')
    VC.saveKeyData(input_path)
    VC.saveKeyData(output_path)
    print('\nBuilding/fitting DL model...')
    VC.build_and_fit()
    print('\nPredicting & Scoring...')
    VC.predict_and_score()
    print('\n\n\n\n\n\n')

vectorsALLdir='ALL_vulnerable_vectors_sysevr'

VUL_TYPE = 'AE'
slicefile = 'Arithmetic expression.txt'
run_all(VUL_TYPE, slicefile, vectorsALLdir, min_num=100)


"""   AU   """
VUL_TYPE = 'AU'
slicefile = 'Array usage.txt'
run_all(VUL_TYPE, slicefile, vectorsALLdir, min_num=100)


"""   API  """
VUL_TYPE = 'API'
slicefile = 'API function call.txt'
run_all(VUL_TYPE, slicefile, vectorsALLdir, min_num=100)


"""   PTR  """
VUL_TYPE = 'PTR'
slicefile = 'Pointer usage.txt'
run_all(VUL_TYPE, slicefile, vectorsALLdir, min_num=100)


# BLSTM

VUL_TYPE = 'AE'
slicefile = 'Arithmetic expression.txt'
run_all(VUL_TYPE, slicefile, vectorsALLdir, 'blstm', create_blstm_model)


"""   AU   """
VUL_TYPE = 'AU'
slicefile = 'Array usage.txt'
run_all(VUL_TYPE, slicefile, 'blstm', create_blstm_model)


"""   API  """
VUL_TYPE = 'API'
slicefile = 'API function call.txt'
run_all(VUL_TYPE, slicefile, 'blstm', create_blstm_model)


"""   PTR  """
VUL_TYPE = 'PTR'
slicefile = 'Pointer usage.txt'
run_all(VUL_TYPE, slicefile, 'blstm', create_blstm_model)
