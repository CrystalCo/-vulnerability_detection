{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9524ad9",
   "metadata": {},
   "source": [
    "# Vulnerability Type Classification using DL\n",
    "The following models will be used to see which one performs best when attempting to classify a vulnerability type in a program:\n",
    "- Bidirectional Gated Recurrent Unit (BGRU)\n",
    "- Bidirectional Long Short-Term Memory (BLSTM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec64ad79",
   "metadata": {},
   "source": [
    "## Setting Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "884b6f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573b59a4",
   "metadata": {},
   "source": [
    "## Setting Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49c4c3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "randomSeed = 1099\n",
    "vectorDim = 30 #num of vector cols\n",
    "\n",
    "vectorRootPath = os.path.join('data','DLvectors')\n",
    "vectorDownsampledPath = os.path.join(vectorRootPath, 'Downsampled_vectors')\n",
    "vectorTrainPath = os.path.join(vectorRootPath,'train')\n",
    "vectorTestPath = os.path.join(vectorRootPath,'test')\n",
    "dlInputsTrainPath = os.path.join('data','DLinputs','train')\n",
    "dlInputsTestPath  = os.path.join('data','DLinputs','test')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a58f2819",
   "metadata": {},
   "source": [
    "### Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "afe27daa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Commencing split for training and testing sets...\n",
      "Samples in Training set:  4614\n",
      "Train set saved in data\\DLvectors\\train\\balanced_train.pkl\n",
      "Samples in Test set:  1143\n",
      "Test set saved in data\\DLvectors\\test\\balanced_test.pkl\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from SYSE_1_isVulnerable.splitTrainTest import splitTrainTestCategorical\n",
    "splitTrainTestCategorical('balanced', vectorDownsampledPath, vectorTrainPath, \n",
    "                          vectorTestPath, randomSeed=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5fec6c",
   "metadata": {},
   "source": [
    "### Set variables needed for our models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d52f6d6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting dataset... data\\DLvectors\\Downsampled_vectors\\ALL_vectors.pkl\n",
      "\n",
      "Original categories: [284 664 682 691 693 697 703 707 710]\n",
      "Total number of classes: 9\n",
      "Encoded classes:\n",
      "[[1 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0 0]\n",
      " [0 0 0 1 0 0 0 0 0]\n",
      " [0 0 0 0 1 0 0 0 0]\n",
      " [0 0 0 0 0 1 0 0 0]\n",
      " [0 0 0 0 0 0 1 0 0]\n",
      " [0 0 0 0 0 0 0 1 0]\n",
      " [0 0 0 0 0 0 0 0 1]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from utils.utils import encode_target, getDataset\n",
    "\n",
    "data = getDataset(vectorDownsampledPath, False, randomSeed)\n",
    "X = data[0]\n",
    "original_labels = data[-2]\n",
    "categories = np.unique(original_labels)\n",
    "mapping, labelEncoder = encode_target(categories)\n",
    "density_units = categories.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b009e788",
   "metadata": {},
   "source": [
    "### F. adjustVectorLen.py \n",
    "Ensure each program (aka slice) contains the same amount of rows (i.e. tokens) in the data matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ed7e175",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "data\\DLvectors\\train\\balanced_train.pkl\n",
      "Mean Vector Length 142\n",
      "Loading data...\n",
      "Train set\n",
      "data\\DLvectors\\train\\balanced_train.pkl\n",
      "threshold:  4260\n",
      "New Vector Length:  142\n",
      "\n",
      "Test set\n",
      "data\\DLvectors\\test\\balanced_test.pkl\n",
      "threshold:  4260\n",
      "New Vector Length:  142\n",
      "New Vector Length (rows x cols): 142 x 30\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from SYSE_1_isVulnerable.adjustVectorLen import meanLen, tranformVectorLen\n",
    "avg = meanLen(vectorTrainPath)\n",
    "tranformVectorLen(vectorTrainPath, vectorTestPath, dlInputsTrainPath, dlInputsTestPath, avg, vectorDim)\n",
    "print(f'New Vector Length (rows x cols): {avg} x {vectorDim}\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b229620",
   "metadata": {},
   "source": [
    "### G. saveKeyData.py\n",
    "Used to fit DL models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b3f1db9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Save metadata from filename: data\\DLinputs\\train\\DL_Final_balanced_train.pkl\n",
      "Original category labels:  [284 664 682 691 693 697 703 707 710]\n",
      "Confirm all data indices contain the same amount of data: \n",
      "4614\n",
      "4614\n",
      "4614\n",
      "4614\n",
      "4614\n",
      "4614\n",
      "4614\n",
      "Saved in  data\\DLinputs\\train\\DL_Final_balanced_train.pkl\n",
      "\n",
      "Save metadata from filename: data\\DLinputs\\test\\DL_Final_balanced_test.pkl\n",
      "Original category labels:  [284 664 682 691 693 697 703 707 710]\n",
      "Confirm all data indices contain the same amount of data: \n",
      "1143\n",
      "1143\n",
      "1143\n",
      "1143\n",
      "1143\n",
      "1143\n",
      "1143\n",
      "Saved in  data\\DLinputs\\test\\DL_Final_balanced_test.pkl\n"
     ]
    }
   ],
   "source": [
    "from SYSE_1_isVulnerable.saveKeyData import saveKeyDataMulticlass\n",
    "saveKeyDataMulticlass(dlInputsTrainPath, labelEncoder)\n",
    "saveKeyDataMulticlass(dlInputsTestPath, labelEncoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a8e3d9a",
   "metadata": {},
   "source": [
    "## Phase 1: Base Classifiers\n",
    "### Deep Learning models\n",
    "#### BGRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e300893c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from utils.DLCustomModels import create_bgru_model, fit_custom_dl_model, make_or_restore_model, predictMulticlassLabel\n",
    "from SYSE_2_vulnerabilityType.DetectVulType import DetectVulType\n",
    "from SYSE_1_isVulnerable.ConfusionMatrix import getConfusionMatrix_Multiclass\n",
    "\n",
    "myoptimizer = 'adam' #can be changed to ‘adamax’\n",
    "maxlen = avg #avg calculated\n",
    "layers = 2\n",
    "dropout = 0.2 \n",
    "BATCHSIZE = 64\n",
    "vectorDim = 30\n",
    "epochs = 20\n",
    "\n",
    "CLASS_TYPE = 'Group'\n",
    "VECTOR_TRANSFORMER='w2v'\n",
    "MODEL_TYPE = 'bgru'\n",
    "checkpoint_dir = './ckpt_%s_%s_%s_%s' % (VECTOR_TRANSFORMER, MODEL_TYPE, str(BATCHSIZE), CLASS_TYPE)\n",
    "model_name = '%s_%s_batch=%s_seed=%s_%s' % (MODEL_TYPE.upper(), VECTOR_TRANSFORMER.upper(), BATCHSIZE, randomSeed, CLASS_TYPE)\n",
    "w2vmetricPath = os.path.join('w2vModel','metrics', MODEL_TYPE)\n",
    "weightpath = os.path.join('model', model_name + myoptimizer + str(randomSeed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9d78245a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dvt = DetectVulType(create_bgru_model, m_epochs=epochs, batch_size=BATCHSIZE, mask=True, \n",
    "                    dropout=dropout, vectorTrainPath=vectorTrainPath, \n",
    "                    vectorTestPath=vectorTestPath, inputsTrainPath=dlInputsTrainPath, \n",
    "                    inputsTestPath=dlInputsTestPath, checkpoint_dir=checkpoint_dir)\n",
    "dvt.avg = avg\n",
    "dvt.density_units = density_units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3c87e95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare a directory to store all the checkpoints.\n",
    "if not os.path.exists(checkpoint_dir):\n",
    "    os.makedirs(checkpoint_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2bc04cbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n",
      "\n",
      "Number of devices: 1\n",
      "Creating a new model\n",
      "\n",
      "Build BGRU Model with maxlen 142 and vector size 30\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "masking_1 (Masking)          (None, 142, 30)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 142, 512)          440832    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 142, 512)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_3 (Bidirection (None, 512)               1181184   \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 9)                 4617      \n",
      "=================================================================\n",
      "Total params: 1,626,633\n",
      "Trainable params: 1,626,633\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Open a strategy scope\n",
    "strategy = tf.distribute.MirroredStrategy()\n",
    "print('\\nNumber of devices: {}'.format(strategy.num_replicas_in_sync))\n",
    "with strategy.scope():\n",
    "    latest_epoch, myKerasModel = make_or_restore_model(checkpoint_dir, dvt.get_compiled_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "827fce8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fitting model with training set...\n",
      "filename:  DL_Final_balanced_train.pkl\n",
      "4614 4614\n",
      "[[0 0 0 ... 0 0 1]\n",
      " [0 1 0 ... 0 0 0]\n",
      " [1 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 1 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "Fitting model start: 2021-12-06 15:20:10.158529\n",
      "Epoch 1/20\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "72/72 [==============================] - 105s 1s/step - loss: 2.0742 - categorical_accuracy: 0.1730 - recall: 6.5104e-04\n",
      "\n",
      "Epoch 00001: saving model to ./ckpt_w2v_bgru_64_Group\\ckpt-1\n",
      "INFO:tensorflow:Assets written to: ./ckpt_w2v_bgru_64_Group\\ckpt-1\\assets\n",
      "Epoch 2/20\n",
      "72/72 [==============================] - 98s 1s/step - loss: 2.0214 - categorical_accuracy: 0.1730 - recall: 0.0000e+00\n",
      "\n",
      "Epoch 00002: saving model to ./ckpt_w2v_bgru_64_Group\\ckpt-2\n",
      "INFO:tensorflow:Assets written to: ./ckpt_w2v_bgru_64_Group\\ckpt-2\\assets\n",
      "Epoch 3/20\n",
      "72/72 [==============================] - 99s 1s/step - loss: 2.0050 - categorical_accuracy: 0.1730 - recall: 0.0000e+00\n",
      "\n",
      "Epoch 00003: saving model to ./ckpt_w2v_bgru_64_Group\\ckpt-3\n",
      "INFO:tensorflow:Assets written to: ./ckpt_w2v_bgru_64_Group\\ckpt-3\\assets\n",
      "Epoch 4/20\n",
      "72/72 [==============================] - 101s 1s/step - loss: 1.9996 - categorical_accuracy: 0.1790 - recall: 0.0000e+00\n",
      "\n",
      "Epoch 00004: saving model to ./ckpt_w2v_bgru_64_Group\\ckpt-4\n",
      "INFO:tensorflow:Assets written to: ./ckpt_w2v_bgru_64_Group\\ckpt-4\\assets\n",
      "Epoch 5/20\n",
      "72/72 [==============================] - 103s 1s/step - loss: 1.9957 - categorical_accuracy: 0.1780 - recall: 0.0000e+00\n",
      "\n",
      "Epoch 00005: saving model to ./ckpt_w2v_bgru_64_Group\\ckpt-5\n",
      "INFO:tensorflow:Assets written to: ./ckpt_w2v_bgru_64_Group\\ckpt-5\\assets\n",
      "Epoch 6/20\n",
      "72/72 [==============================] - 99s 1s/step - loss: 1.9930 - categorical_accuracy: 0.1710 - recall: 0.0000e+00\n",
      "\n",
      "Epoch 00006: saving model to ./ckpt_w2v_bgru_64_Group\\ckpt-6\n",
      "INFO:tensorflow:Assets written to: ./ckpt_w2v_bgru_64_Group\\ckpt-6\\assets\n",
      "Epoch 7/20\n",
      "72/72 [==============================] - 98s 1s/step - loss: 1.9914 - categorical_accuracy: 0.1736 - recall: 0.0000e+00\n",
      "\n",
      "Epoch 00007: saving model to ./ckpt_w2v_bgru_64_Group\\ckpt-7\n",
      "INFO:tensorflow:Assets written to: ./ckpt_w2v_bgru_64_Group\\ckpt-7\\assets\n",
      "Epoch 8/20\n",
      "72/72 [==============================] - 103s 1s/step - loss: 1.9896 - categorical_accuracy: 0.1747 - recall: 0.0000e+00\n",
      "\n",
      "Epoch 00008: saving model to ./ckpt_w2v_bgru_64_Group\\ckpt-8\n",
      "INFO:tensorflow:Assets written to: ./ckpt_w2v_bgru_64_Group\\ckpt-8\\assets\n",
      "Epoch 9/20\n",
      "72/72 [==============================] - 104s 1s/step - loss: 1.9853 - categorical_accuracy: 0.1799 - recall: 0.0000e+00\n",
      "\n",
      "Epoch 00009: saving model to ./ckpt_w2v_bgru_64_Group\\ckpt-9\n",
      "INFO:tensorflow:Assets written to: ./ckpt_w2v_bgru_64_Group\\ckpt-9\\assets\n",
      "Epoch 10/20\n",
      "72/72 [==============================] - 105s 1s/step - loss: 1.9818 - categorical_accuracy: 0.1816 - recall: 0.0000e+00\n",
      "\n",
      "Epoch 00010: saving model to ./ckpt_w2v_bgru_64_Group\\ckpt-10\n",
      "INFO:tensorflow:Assets written to: ./ckpt_w2v_bgru_64_Group\\ckpt-10\\assets\n",
      "Epoch 11/20\n",
      "72/72 [==============================] - 106s 1s/step - loss: 1.9810 - categorical_accuracy: 0.1875 - recall: 0.0000e+00\n",
      "\n",
      "Epoch 00011: saving model to ./ckpt_w2v_bgru_64_Group\\ckpt-11\n",
      "INFO:tensorflow:Assets written to: ./ckpt_w2v_bgru_64_Group\\ckpt-11\\assets\n",
      "Epoch 12/20\n",
      "72/72 [==============================] - 98s 1s/step - loss: 1.9711 - categorical_accuracy: 0.2023 - recall: 0.0000e+00\n",
      "\n",
      "Epoch 00012: saving model to ./ckpt_w2v_bgru_64_Group\\ckpt-12\n",
      "INFO:tensorflow:Assets written to: ./ckpt_w2v_bgru_64_Group\\ckpt-12\\assets\n",
      "Epoch 13/20\n",
      "72/72 [==============================] - 97s 1s/step - loss: 1.9463 - categorical_accuracy: 0.2257 - recall: 0.0000e+00\n",
      "\n",
      "Epoch 00013: saving model to ./ckpt_w2v_bgru_64_Group\\ckpt-13\n",
      "INFO:tensorflow:Assets written to: ./ckpt_w2v_bgru_64_Group\\ckpt-13\\assets\n",
      "Epoch 14/20\n",
      "72/72 [==============================] - 102s 1s/step - loss: 1.9036 - categorical_accuracy: 0.2467 - recall: 0.0000e+00\n",
      "\n",
      "Epoch 00014: saving model to ./ckpt_w2v_bgru_64_Group\\ckpt-14\n",
      "INFO:tensorflow:Assets written to: ./ckpt_w2v_bgru_64_Group\\ckpt-14\\assets\n",
      "Epoch 15/20\n",
      "72/72 [==============================] - 96s 1s/step - loss: 1.8737 - categorical_accuracy: 0.2639 - recall: 0.0000e+00\n",
      "\n",
      "Epoch 00015: saving model to ./ckpt_w2v_bgru_64_Group\\ckpt-15\n",
      "INFO:tensorflow:Assets written to: ./ckpt_w2v_bgru_64_Group\\ckpt-15\\assets\n",
      "Epoch 16/20\n",
      "72/72 [==============================] - 98s 1s/step - loss: 1.8686 - categorical_accuracy: 0.2622 - recall: 0.0000e+00\n",
      "\n",
      "Epoch 00016: saving model to ./ckpt_w2v_bgru_64_Group\\ckpt-16\n",
      "INFO:tensorflow:Assets written to: ./ckpt_w2v_bgru_64_Group\\ckpt-16\\assets\n",
      "Epoch 17/20\n",
      "72/72 [==============================] - 97s 1s/step - loss: 1.8575 - categorical_accuracy: 0.2730 - recall: 0.0000e+00\n",
      "\n",
      "Epoch 00017: saving model to ./ckpt_w2v_bgru_64_Group\\ckpt-17\n",
      "INFO:tensorflow:Assets written to: ./ckpt_w2v_bgru_64_Group\\ckpt-17\\assets\n",
      "Epoch 18/20\n",
      "72/72 [==============================] - 102s 1s/step - loss: 1.8501 - categorical_accuracy: 0.2732 - recall: 0.0000e+00\n",
      "\n",
      "Epoch 00018: saving model to ./ckpt_w2v_bgru_64_Group\\ckpt-18\n",
      "INFO:tensorflow:Assets written to: ./ckpt_w2v_bgru_64_Group\\ckpt-18\\assets\n",
      "Epoch 19/20\n",
      "72/72 [==============================] - 98s 1s/step - loss: 1.8524 - categorical_accuracy: 0.2669 - recall: 0.0000e+00\n",
      "\n",
      "Epoch 00019: saving model to ./ckpt_w2v_bgru_64_Group\\ckpt-19\n",
      "INFO:tensorflow:Assets written to: ./ckpt_w2v_bgru_64_Group\\ckpt-19\\assets\n",
      "Epoch 20/20\n",
      "72/72 [==============================] - 98s 1s/step - loss: 1.8519 - categorical_accuracy: 0.2747 - recall: 0.0000e+00\n",
      "\n",
      "Epoch 00020: saving model to ./ckpt_w2v_bgru_64_Group\\ckpt-20\n",
      "INFO:tensorflow:Assets written to: ./ckpt_w2v_bgru_64_Group\\ckpt-20\\assets\n",
      "Fitting model time total: 0:40:12.662572\n",
      "\n"
     ]
    }
   ],
   "source": [
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        filepath=os.path.join(checkpoint_dir, 'ckpt-{epoch}'),\n",
    "        monitor='val_acc',\n",
    "        verbose=1,\n",
    "        save_freq='epoch'\n",
    "    )\n",
    "]\n",
    "\n",
    "model = fit_custom_dl_model(myKerasModel, weightpath, dlInputsTrainPath, BATCHSIZE, avg, vectorDim, randomSeed, epochs, useGenerator=True, callbacks=callbacks, latest_epoch=latest_epoch)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0026140c",
   "metadata": {},
   "source": [
    "##### Measure Performance - DLPrediction.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "35822505",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of dataset: 1143\n",
      "Avg: 142\n",
      "36/36 [==============================] - 19s 465ms/step\n",
      "36/36 [==============================] - 18s 465ms/step - loss: 2.0099 - categorical_accuracy: 0.1750 - recall: 0.0000e+00\n",
      "loss on predicted test dataset: 2.0098836421966553\n",
      "categorical_accuracy on predicted test dataset: 0.1749781221151352\n",
      "recall on predicted test dataset: 0.0\n"
     ]
    }
   ],
   "source": [
    "model.load_weights(weightpath)\n",
    "_, mypredicted_labels, myreallabels, _ = predictMulticlassLabel(model, dlInputsTestPath, \n",
    "                                                                maxlen, vectorDim, myoptimizer, \n",
    "                                                                model_name, randomSeed, labelEncoder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "18b5bbb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Confusion Matrix\n",
      "[[  0  64   0   0   0   0   0   1   0]\n",
      " [  0 178   0   0   0   0   0  20   1]\n",
      " [  0 157   0   0   0   0   0  34   8]\n",
      " [  0  39   0   0   0   0   0  60  25]\n",
      " [  0  34   0   0   0   0   0  79  15]\n",
      " [  0   2   0   0   0   0   0   5   0]\n",
      " [  0  23   0   0   0   0   0   0   0]\n",
      " [  0 197   0   0   0   0   0   2   0]\n",
      " [  0  49   0   0   0   0   0 130  20]]\n",
      "\n",
      "CLASSIFICATION REPORT\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         284     0.0000    0.0000    0.0000        65\n",
      "         664     0.2396    0.8945    0.3779       199\n",
      "         682     0.0000    0.0000    0.0000       199\n",
      "         691     0.0000    0.0000    0.0000       124\n",
      "         693     0.0000    0.0000    0.0000       128\n",
      "         697     0.0000    0.0000    0.0000         7\n",
      "         703     0.0000    0.0000    0.0000        23\n",
      "         707     0.0060    0.0101    0.0075       199\n",
      "         710     0.2899    0.1005    0.1493       199\n",
      "\n",
      "    accuracy                         0.1750      1143\n",
      "   macro avg     0.0595    0.1117    0.0594      1143\n",
      "weighted avg     0.0932    0.1750    0.0931      1143\n",
      "\n",
      "\n",
      "Calculations from CM\n",
      "TP: [  0. 178.   0.   0.   0.   0.   0.   2.  20.]\n",
      "FP: [  0. 565.   0.   0.   0.   0.   0. 329.  49.]\n",
      "TN: [1078.  379.  944. 1019. 1015. 1136. 1120.  615.  895.]\n",
      "FN: [ 65.  21. 199. 124. 128.   7.  23. 197. 179.]\n",
      "TPR: [0.         0.89447236 0.         0.         0.         0.\n",
      " 0.         0.01005025 0.10050251]\n",
      "TNR: [1.         0.40148305 1.         1.         1.         1.\n",
      " 1.         0.65148305 0.94809322]\n",
      "FPR: [0.         0.59851695 0.         0.         0.         0.\n",
      " 0.         0.34851695 0.05190678]\n",
      "FNR: [1.         0.10552764 1.         1.         1.         1.\n",
      " 1.         0.98994975 0.89949749]\n",
      "ACC: [0.94313211 0.48731409 0.82589676 0.89151356 0.888014   0.99387577\n",
      " 0.97987752 0.53980752 0.80052493]\n",
      "Counts: [ 65 199 199 124 128   7  23 199 199]\n",
      "Labels: [284 664 682 691 693 697 703 707 710]\n",
      "\n",
      "Micro Precision: 0.1750\n",
      "Micro Recall: 0.1750\n",
      "Micro FPR: 0.8250\n",
      "Micro FNR: 911.2222\n",
      "Micro F1-score: 0.1750\n",
      "Mean Precision: 0.0595\n",
      "Mean Recall: 0.1117\n",
      "Mean FPR: 0.8883\n",
      "Mean FNR: 0.8883\n",
      "Mean F1-score: 0.0594\n",
      "Weighted Precision: 0.0932\n",
      "Weighted Recall: 0.1750\n",
      "Weighted FPR: 0.8250\n",
      "Weighted FNR: 0.0917\n",
      "Weighted F1-score: 0.0931\n",
      "Accuracy: 0.1750\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1.74978128e-01, 1.74978128e-01, 8.25021872e-01, 9.11222222e+02,\n",
       "        1.74978128e-01, 5.94962980e-02, 1.11669458e-01, 8.88330542e-01,\n",
       "        8.88330542e-01, 5.94133580e-02, 9.32264827e-02, 1.74978128e-01,\n",
       "        8.25021872e-01, 9.16690969e-02, 9.30965215e-02, 1.74978128e-01]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1296x576 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Confusion matrix for each category.\n",
    "getConfusionMatrix_Multiclass(mypredicted_labels, myreallabels, saveFig=True, path=w2vmetricPath, modelName=model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5207b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9ab90ce6",
   "metadata": {},
   "source": [
    "## Phase 2 - PCA transformed\n",
    "### Deep Learning Classification models\n",
    "#### BGRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "85c0124c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting dataset... data\\DLvectors\\Downsampled_vectors\\ALL_vectors.pkl\n"
     ]
    }
   ],
   "source": [
    "from utils.utils import getDataset\n",
    "\n",
    "data = getDataset(vectorDownsampledPath, False, randomSeed)\n",
    "X = data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e78dbf99",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;31mTypeError\u001b[0m: only size-1 arrays can be converted to Python scalars",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_27128/20661176.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecomposition\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPCA\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mpca_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPCA\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_components\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m15\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mpca_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Sum of variance ratios: \"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpca_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexplained_variance_ratio_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\env_vul\\lib\\site-packages\\sklearn\\decomposition\\_pca.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    357\u001b[0m             \u001b[0mReturns\u001b[0m \u001b[0mthe\u001b[0m \u001b[0minstance\u001b[0m \u001b[0mitself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    358\u001b[0m         \"\"\"\n\u001b[1;32m--> 359\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    360\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    361\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\env_vul\\lib\\site-packages\\sklearn\\decomposition\\_pca.py\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    402\u001b[0m                             'TruncatedSVD for a possible alternative.')\n\u001b[0;32m    403\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 404\u001b[1;33m         X = self._validate_data(X, dtype=[np.float64, np.float32],\n\u001b[0m\u001b[0;32m    405\u001b[0m                                 ensure_2d=True, copy=self.copy)\n\u001b[0;32m    406\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\env_vul\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    419\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    420\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'no_validation'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 421\u001b[1;33m             \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    422\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    423\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\env_vul\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;31m# extra_args > 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\env_vul\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[0;32m    671\u001b[0m                     \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcasting\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"unsafe\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    672\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 673\u001b[1;33m                     \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    674\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcomplex_warning\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    675\u001b[0m                 raise ValueError(\"Complex data not supported\\n\"\n",
      "\u001b[1;32m~\\anaconda3\\envs\\env_vul\\lib\\site-packages\\numpy\\core\\_asarray.py\u001b[0m in \u001b[0;36masarray\u001b[1;34m(a, dtype, order)\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m     \"\"\"\n\u001b[1;32m---> 83\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     84\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: setting an array element with a sequence."
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca_model = PCA(n_components=15)\n",
    "pca_model.fit(X)\n",
    "print(\"Sum of variance ratios: \",sum(pca_model.explained_variance_ratio_))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198c5bc6",
   "metadata": {},
   "source": [
    "### Since PCA won't accept our 3D data, we will flatten for PCA like in the example https://www.kaggle.com/mehmetlaudatekman/tutorial-word-embeddings-with-svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8b00d21e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5757, 142, 30)\n",
      "[[ 0.01003225  0.00331451  0.01160636 ...  0.00713296  0.01400539\n",
      "   0.0055488 ]\n",
      " [ 0.000307    0.01076798  0.01116681 ...  0.00503008 -0.00056971\n",
      "  -0.0090057 ]\n",
      " [-0.00717854 -0.01300672 -0.00493445 ... -0.00202337 -0.00311062\n",
      "   0.00969776]\n",
      " ...\n",
      " [-0.00886669 -0.01199083 -0.00152747 ... -0.01219757 -0.01178785\n",
      "  -0.01233405]\n",
      " [ 0.01461655 -0.00014667  0.00143368 ... -0.01504964 -0.010656\n",
      "   0.0081686 ]\n",
      " [-0.01501023  0.00833525 -0.00485044 ... -0.01215666 -0.00609653\n",
      "   0.01577502]]\n"
     ]
    }
   ],
   "source": [
    "from SYSE_1_isVulnerable.preprocess_dl_Input_version5 import process_sequences_shape\n",
    "x_train = process_sequences_shape(X, avg, vectorDim)\n",
    "print(x_train.shape)\n",
    "print(x_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "24ead849",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5757, 4260)\n",
      "[ 0.01003225  0.00331451  0.01160636 ... -0.01215666 -0.00609653\n",
      "  0.01577502]\n"
     ]
    }
   ],
   "source": [
    "x_train = np.reshape(x_train, (x_train.shape[0], 4260))\n",
    "print(x_train.shape)\n",
    "print(x_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e007d76b",
   "metadata": {},
   "source": [
    "**Perform PCA on the 2D dataset**\n",
    "Our flattened dataset now has 4,260 columns.  Let's see how much we can reduce it without losing too much information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cf266c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d2643ef6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n",
      " Sum of variance ratios for n=100:  0.5422524401289694\n",
      " Sum of variance ratios for n=340:  0.7927601861622096\n",
      " Sum of variance ratios for n=580:  0.8871543529051129\n",
      " Sum of variance ratios for n=820:  0.93620351331324\n",
      " Sum of variance ratios for n=1060:  0.9636661007822506\n",
      " Sum of variance ratios for n=1300:  0.979611796440305\n",
      " Sum of variance ratios for n=1540:  0.9888696215273848\n",
      " Sum of variance ratios for n=1780:  0.9941879892717503\n",
      " Sum of variance ratios for n=2020:  0.9971697864198127\n",
      " Sum of variance ratios for n=2260:  0.9987689199828329\n",
      " Sum of variance ratios for n=2500:  0.9995583315115915\n",
      " Sum of variance ratios for n=2740:  0.9998946777851351\n",
      " Sum of variance ratios for n=2980:  0.9999910125399316\n"
     ]
    }
   ],
   "source": [
    "n_components = np.arange(100, 3000, 240)\n",
    "print(len(n_components))\n",
    "\n",
    "for n in n_components:\n",
    "    pca_model = PCA(n_components=n)\n",
    "    pca_model.fit(x_train)\n",
    "    print(f' Sum of variance ratios for n={n}: ', sum(pca_model.explained_variance_ratio_))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d71ec973",
   "metadata": {},
   "source": [
    "We'll choose n=1780 since that still leaves us with about 99.95% variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4f16c9f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PCA(n_components=1780)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca_model = PCA(n_components=1780)\n",
    "pca_model.fit(x_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a217e86a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5757, 1780)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_comps = pca_model.transform(x_train)\n",
    "x_comps.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3dc7c738",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All vectors saved in <_io.BufferedWriter name='data\\\\DLvectors\\\\PCA_vectors\\\\PCA_vectors_balanced.pkl'>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from utils.utils import save_data_to_file\n",
    "\n",
    "# Save PCA transformed data\n",
    "data[0] = x_comps\n",
    "pca_vectors = os.path.join(vectorRootPath, 'PCA_vectors')\n",
    "save_data_to_file(pca_vectors, 'PCA_vectors_balanced.pkl', data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c835fc2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Commencing split for training and testing sets...\n",
      "Samples in Training set:  4614\n",
      "Train set saved in data\\DLvectors\\train\\balanced_train.pkl\n",
      "Samples in Test set:  1143\n",
      "Test set saved in data\\DLvectors\\test\\balanced_test.pkl\n",
      "\n"
     ]
    }
   ],
   "source": [
    "splitTrainTestCategorical('balanced', pca_vectors, vectorTrainPath, \n",
    "                          vectorTestPath, randomSeed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b48030a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Save metadata from filename: data\\DLvectors\\train\\balanced_train.pkl\n",
      "Original category labels:  [284 664 682 691 693 697 703 707 710]\n",
      "Confirm all data indices contain the same amount of data: \n",
      "4614\n",
      "4614\n",
      "4614\n",
      "4614\n",
      "4614\n",
      "4614\n",
      "4614\n",
      "Saved in  data\\DLinputs\\train\\DL_Final_balanced_train.pkl\n",
      "\n",
      "Save metadata from filename: data\\DLvectors\\test\\balanced_test.pkl\n",
      "Original category labels:  [284 664 682 691 693 697 703 707 710]\n",
      "Confirm all data indices contain the same amount of data: \n",
      "1143\n",
      "1143\n",
      "1143\n",
      "1143\n",
      "1143\n",
      "1143\n",
      "1143\n",
      "Saved in  data\\DLinputs\\test\\DL_Final_balanced_test.pkl\n"
     ]
    }
   ],
   "source": [
    "from SYSE_1_isVulnerable.saveKeyData import saveKeyDataMulticlass\n",
    "saveKeyDataMulticlass(vectorTrainPath, labelEncoder, dlInputsTrainPath)\n",
    "saveKeyDataMulticlass(vectorTestPath, labelEncoder, dlInputsTestPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a7fbf447",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from utils.DLCustomModels import create_bgru_model, fit_custom_dl_model, make_or_restore_model, predictMulticlassLabel\n",
    "from SYSE_2_vulnerabilityType.DetectVulType import DetectVulType\n",
    "from SYSE_1_isVulnerable.ConfusionMatrix import getConfusionMatrix_Multiclass\n",
    "\n",
    "myoptimizer = 'adam' #can be changed to ‘adamax’\n",
    "layers = 2\n",
    "dropout = 0.2 \n",
    "BATCHSIZE = 64\n",
    "epochs = 20\n",
    "# Update our dimensions since we have a flattened dataset now\n",
    "maxlen = avg = 1\n",
    "vectorDim = x_comps.shape[1]\n",
    "\n",
    "CLASS_TYPE = 'Group_PCA'\n",
    "VECTOR_TRANSFORMER='w2v'\n",
    "MODEL_TYPE = 'bgru'\n",
    "checkpoint_dir = './ckpt_%s_%s_%s_%s' % (VECTOR_TRANSFORMER, MODEL_TYPE, str(BATCHSIZE), CLASS_TYPE)\n",
    "model_name = '%s_%s_batch=%s_seed=%s_%s' % (MODEL_TYPE.upper(), VECTOR_TRANSFORMER.upper(), BATCHSIZE, randomSeed, CLASS_TYPE)\n",
    "w2vmetricPath = os.path.join('w2vModel','metrics', MODEL_TYPE)\n",
    "weightpath = os.path.join('model', model_name + myoptimizer + str(randomSeed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "18b4ac48",
   "metadata": {},
   "outputs": [],
   "source": [
    "dvt = DetectVulType(create_bgru_model, m_epochs=epochs, batch_size=BATCHSIZE, mask=False, \n",
    "                    dropout=dropout, vectorTrainPath=vectorTrainPath, \n",
    "                    vectorTestPath=vectorTestPath, inputsTrainPath=dlInputsTrainPath, \n",
    "                    inputsTestPath=dlInputsTestPath, checkpoint_dir=checkpoint_dir)\n",
    "dvt.avg = avg\n",
    "dvt.vector_size = vectorDim\n",
    "dvt.density_units = density_units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b3e03466",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(checkpoint_dir):\n",
    "    os.makedirs(checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "501d894b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n",
      "\n",
      "Number of devices: 1\n",
      "Creating a new model\n",
      "\n",
      "Build BGRU Model with maxlen 1 and vector size 1780\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bidirectional_2 (Bidirection (None, 1, 512)            3128832   \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 1, 512)            0         \n",
      "_________________________________________________________________\n",
      "bidirectional_3 (Bidirection (None, 512)               1181184   \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 9)                 4617      \n",
      "=================================================================\n",
      "Total params: 4,314,633\n",
      "Trainable params: 4,314,633\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from utils.DLCustomModels import make_or_restore_model\n",
    "\n",
    "# Open a strategy scope\n",
    "strategy = tf.distribute.MirroredStrategy()\n",
    "print('\\nNumber of devices: {}'.format(strategy.num_replicas_in_sync))\n",
    "with strategy.scope():\n",
    "    latest_epoch, myKerasModel = make_or_restore_model(checkpoint_dir, dvt.get_compiled_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c1ea15d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fitting model with training set...\n",
      "filename:  DL_Final_balanced_train.pkl\n",
      "4614 4614\n",
      "[[0 0 0 ... 0 0 1]\n",
      " [0 1 0 ... 0 0 0]\n",
      " [1 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 1 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "Fitting model start: 2021-12-06 17:51:57.848601\n",
      "Epoch 1/20\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "72/72 [==============================] - 18s 54ms/step - loss: 2.0130 - categorical_accuracy: 0.1864 - recall: 0.0000e+00\n",
      "\n",
      "Epoch 00001: saving model to ./ckpt_w2v_bgru_64_Group_PCA\\ckpt-1\n",
      "INFO:tensorflow:Assets written to: ./ckpt_w2v_bgru_64_Group_PCA\\ckpt-1\\assets\n",
      "Epoch 2/20\n",
      "72/72 [==============================] - 3s 44ms/step - loss: 1.9911 - categorical_accuracy: 0.1803 - recall: 0.0000e+00\n",
      "\n",
      "Epoch 00002: saving model to ./ckpt_w2v_bgru_64_Group_PCA\\ckpt-2\n",
      "INFO:tensorflow:Assets written to: ./ckpt_w2v_bgru_64_Group_PCA\\ckpt-2\\assets\n",
      "Epoch 3/20\n",
      "72/72 [==============================] - 3s 44ms/step - loss: 1.9703 - categorical_accuracy: 0.2088 - recall: 0.0000e+00\n",
      "\n",
      "Epoch 00003: saving model to ./ckpt_w2v_bgru_64_Group_PCA\\ckpt-3\n",
      "INFO:tensorflow:Assets written to: ./ckpt_w2v_bgru_64_Group_PCA\\ckpt-3\\assets\n",
      "Epoch 4/20\n",
      "72/72 [==============================] - 4s 54ms/step - loss: 1.9310 - categorical_accuracy: 0.2496 - recall: 0.0000e+00\n",
      "\n",
      "Epoch 00004: saving model to ./ckpt_w2v_bgru_64_Group_PCA\\ckpt-4\n",
      "INFO:tensorflow:Assets written to: ./ckpt_w2v_bgru_64_Group_PCA\\ckpt-4\\assets\n",
      "Epoch 5/20\n",
      "72/72 [==============================] - 3s 46ms/step - loss: 1.8086 - categorical_accuracy: 0.3689 - recall: 4.3403e-04\n",
      "\n",
      "Epoch 00005: saving model to ./ckpt_w2v_bgru_64_Group_PCA\\ckpt-5\n",
      "INFO:tensorflow:Assets written to: ./ckpt_w2v_bgru_64_Group_PCA\\ckpt-5\\assets\n",
      "Epoch 6/20\n",
      "72/72 [==============================] - 3s 45ms/step - loss: 1.5120 - categorical_accuracy: 0.5289 - recall: 0.1013\n",
      "\n",
      "Epoch 00006: saving model to ./ckpt_w2v_bgru_64_Group_PCA\\ckpt-6\n",
      "INFO:tensorflow:Assets written to: ./ckpt_w2v_bgru_64_Group_PCA\\ckpt-6\\assets\n",
      "Epoch 7/20\n",
      "72/72 [==============================] - 3s 42ms/step - loss: 1.1296 - categorical_accuracy: 0.6547 - recall: 0.3767\n",
      "\n",
      "Epoch 00007: saving model to ./ckpt_w2v_bgru_64_Group_PCA\\ckpt-7\n",
      "INFO:tensorflow:Assets written to: ./ckpt_w2v_bgru_64_Group_PCA\\ckpt-7\\assets\n",
      "Epoch 8/20\n",
      "72/72 [==============================] - 3s 42ms/step - loss: 0.8819 - categorical_accuracy: 0.7342 - recall: 0.5664\n",
      "\n",
      "Epoch 00008: saving model to ./ckpt_w2v_bgru_64_Group_PCA\\ckpt-8\n",
      "INFO:tensorflow:Assets written to: ./ckpt_w2v_bgru_64_Group_PCA\\ckpt-8\\assets\n",
      "Epoch 9/20\n",
      "72/72 [==============================] - 3s 43ms/step - loss: 0.7331 - categorical_accuracy: 0.7823 - recall: 0.6680\n",
      "\n",
      "Epoch 00009: saving model to ./ckpt_w2v_bgru_64_Group_PCA\\ckpt-9\n",
      "INFO:tensorflow:Assets written to: ./ckpt_w2v_bgru_64_Group_PCA\\ckpt-9\\assets\n",
      "Epoch 10/20\n",
      "72/72 [==============================] - 3s 42ms/step - loss: 0.6293 - categorical_accuracy: 0.8105 - recall: 0.7144\n",
      "\n",
      "Epoch 00010: saving model to ./ckpt_w2v_bgru_64_Group_PCA\\ckpt-10\n",
      "INFO:tensorflow:Assets written to: ./ckpt_w2v_bgru_64_Group_PCA\\ckpt-10\\assets\n",
      "Epoch 11/20\n",
      "72/72 [==============================] - 3s 42ms/step - loss: 0.5626 - categorical_accuracy: 0.8260 - recall: 0.7485\n",
      "\n",
      "Epoch 00011: saving model to ./ckpt_w2v_bgru_64_Group_PCA\\ckpt-11\n",
      "INFO:tensorflow:Assets written to: ./ckpt_w2v_bgru_64_Group_PCA\\ckpt-11\\assets\n",
      "Epoch 12/20\n",
      "72/72 [==============================] - 3s 43ms/step - loss: 0.5092 - categorical_accuracy: 0.8416 - recall: 0.7834\n",
      "\n",
      "Epoch 00012: saving model to ./ckpt_w2v_bgru_64_Group_PCA\\ckpt-12\n",
      "INFO:tensorflow:Assets written to: ./ckpt_w2v_bgru_64_Group_PCA\\ckpt-12\\assets\n",
      "Epoch 13/20\n",
      "72/72 [==============================] - 3s 44ms/step - loss: 0.4722 - categorical_accuracy: 0.8513 - recall: 0.7990\n",
      "\n",
      "Epoch 00013: saving model to ./ckpt_w2v_bgru_64_Group_PCA\\ckpt-13\n",
      "INFO:tensorflow:Assets written to: ./ckpt_w2v_bgru_64_Group_PCA\\ckpt-13\\assets\n",
      "Epoch 14/20\n",
      "72/72 [==============================] - 3s 44ms/step - loss: 0.4457 - categorical_accuracy: 0.8592 - recall: 0.8132\n",
      "\n",
      "Epoch 00014: saving model to ./ckpt_w2v_bgru_64_Group_PCA\\ckpt-14\n",
      "INFO:tensorflow:Assets written to: ./ckpt_w2v_bgru_64_Group_PCA\\ckpt-14\\assets\n",
      "Epoch 15/20\n",
      "72/72 [==============================] - 3s 42ms/step - loss: 0.4150 - categorical_accuracy: 0.8689 - recall: 0.8281\n",
      "\n",
      "Epoch 00015: saving model to ./ckpt_w2v_bgru_64_Group_PCA\\ckpt-15\n",
      "INFO:tensorflow:Assets written to: ./ckpt_w2v_bgru_64_Group_PCA\\ckpt-15\\assets\n",
      "Epoch 16/20\n",
      "72/72 [==============================] - 3s 48ms/step - loss: 0.3855 - categorical_accuracy: 0.8730 - recall: 0.8394\n",
      "\n",
      "Epoch 00016: saving model to ./ckpt_w2v_bgru_64_Group_PCA\\ckpt-16\n",
      "INFO:tensorflow:Assets written to: ./ckpt_w2v_bgru_64_Group_PCA\\ckpt-16\\assets\n",
      "Epoch 17/20\n",
      "72/72 [==============================] - 3s 42ms/step - loss: 0.3686 - categorical_accuracy: 0.8796 - recall: 0.8448\n",
      "\n",
      "Epoch 00017: saving model to ./ckpt_w2v_bgru_64_Group_PCA\\ckpt-17\n",
      "INFO:tensorflow:Assets written to: ./ckpt_w2v_bgru_64_Group_PCA\\ckpt-17\\assets\n",
      "Epoch 18/20\n",
      "72/72 [==============================] - 3s 42ms/step - loss: 0.3422 - categorical_accuracy: 0.8928 - recall: 0.8609\n",
      "\n",
      "Epoch 00018: saving model to ./ckpt_w2v_bgru_64_Group_PCA\\ckpt-18\n",
      "INFO:tensorflow:Assets written to: ./ckpt_w2v_bgru_64_Group_PCA\\ckpt-18\\assets\n",
      "Epoch 19/20\n",
      "72/72 [==============================] - 3s 44ms/step - loss: 0.3101 - categorical_accuracy: 0.9008 - recall: 0.8733\n",
      "\n",
      "Epoch 00019: saving model to ./ckpt_w2v_bgru_64_Group_PCA\\ckpt-19\n",
      "INFO:tensorflow:Assets written to: ./ckpt_w2v_bgru_64_Group_PCA\\ckpt-19\\assets\n",
      "Epoch 20/20\n",
      "72/72 [==============================] - 1s 16ms/step - loss: 0.3054 - categorical_accuracy: 0.9028 - recall: 0.8770\n",
      "\n",
      "Epoch 00020: saving model to ./ckpt_w2v_bgru_64_Group_PCA\\ckpt-20\n",
      "INFO:tensorflow:Assets written to: ./ckpt_w2v_bgru_64_Group_PCA\\ckpt-20\\assets\n",
      "Fitting model time total: 0:10:48.305107\n",
      "\n"
     ]
    }
   ],
   "source": [
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        filepath=os.path.join(checkpoint_dir, 'ckpt-{epoch}'),\n",
    "        monitor='val_acc',\n",
    "        verbose=1,\n",
    "        save_freq='epoch'\n",
    "    )\n",
    "]\n",
    "\n",
    "model = fit_custom_dl_model(myKerasModel, weightpath, dlInputsTrainPath, BATCHSIZE, avg, vectorDim, randomSeed, epochs, useGenerator=True, callbacks=callbacks, latest_epoch=latest_epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "de993caf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of dataset: 1143\n",
      "Avg: 1\n",
      "36/36 [==============================] - 1s 4ms/step\n",
      "36/36 [==============================] - 2s 8ms/step - loss: 0.4533 - categorical_accuracy: 0.8443 - recall: 0.8066\n",
      "loss on predicted test dataset: 0.4532804489135742\n",
      "categorical_accuracy on predicted test dataset: 0.8442694544792175\n",
      "recall on predicted test dataset: 0.8066491484642029\n"
     ]
    }
   ],
   "source": [
    "model.load_weights(weightpath)\n",
    "_, mypredicted_labels, myreallabels, _ = predictMulticlassLabel(model, dlInputsTestPath, \n",
    "                                                                maxlen, vectorDim, myoptimizer, \n",
    "                                                                model_name, randomSeed, labelEncoder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c05c8499",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Confusion Matrix\n",
      "[[ 59   0   0   0   0   0   4   2   0]\n",
      " [  4 178  13   0   1   0   0   1   2]\n",
      " [  0  12 179   2   0   0   0   2   4]\n",
      " [  0   1  29  91   0   0   0   1   2]\n",
      " [  7   1   0   0 115   0   4   1   0]\n",
      " [  0   0   0   4   0   0   3   0   0]\n",
      " [  0   0   0   1   0   0  16   0   6]\n",
      " [  1   3   2   4   0   0   1 182   6]\n",
      " [  0  17   8  14   0   0   8   7 145]]\n",
      "\n",
      "CLASSIFICATION REPORT\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         284     0.8310    0.9077    0.8676        65\n",
      "         664     0.8396    0.8945    0.8662       199\n",
      "         682     0.7749    0.8995    0.8326       199\n",
      "         691     0.7845    0.7339    0.7583       124\n",
      "         693     0.9914    0.8984    0.9426       128\n",
      "         697     0.0000    0.0000    0.0000         7\n",
      "         703     0.4444    0.6957    0.5424        23\n",
      "         707     0.9286    0.9146    0.9215       199\n",
      "         710     0.8788    0.7286    0.7967       199\n",
      "\n",
      "    accuracy                         0.8443      1143\n",
      "   macro avg     0.7192    0.7414    0.7253      1143\n",
      "weighted avg     0.8481    0.8443    0.8430      1143\n",
      "\n",
      "\n",
      "Calculations from CM\n",
      "TP: [ 59. 178. 179.  91. 115.   0.  16. 182. 145.]\n",
      "FP: [12. 34. 52. 25.  1.  0. 20. 14. 20.]\n",
      "TN: [1066.  910.  892.  994. 1014. 1136. 1100.  930.  924.]\n",
      "FN: [ 6. 21. 20. 33. 13.  7.  7. 17. 54.]\n",
      "TPR: [0.90769231 0.89447236 0.89949749 0.73387097 0.8984375  0.\n",
      " 0.69565217 0.91457286 0.72864322]\n",
      "TNR: [0.98886827 0.96398305 0.94491525 0.97546614 0.99901478 1.\n",
      " 0.98214286 0.98516949 0.97881356]\n",
      "FPR: [0.01113173 0.03601695 0.05508475 0.02453386 0.00098522 0.\n",
      " 0.01785714 0.01483051 0.02118644]\n",
      "FNR: [0.09230769 0.10552764 0.10050251 0.26612903 0.1015625  1.\n",
      " 0.30434783 0.08542714 0.27135678]\n",
      "ACC: [0.98425197 0.95188101 0.93700787 0.94925634 0.98775153 0.99387577\n",
      " 0.97637795 0.97287839 0.93525809]\n",
      "Counts: [ 65 199 199 124 128   7  23 199 199]\n",
      "Labels: [284 664 682 691 693 697 703 707 710]\n",
      "\n",
      "Micro Precision: 0.8443\n",
      "Micro Recall: 0.8443\n",
      "Micro FPR: 0.1557\n",
      "Micro FNR: 996.2222\n",
      "Micro F1-score: 0.8443\n",
      "Mean Precision: 0.7192\n",
      "Mean Recall: 0.7414\n",
      "Mean FPR: 0.2586\n",
      "Mean FNR: 0.2586\n",
      "Mean F1-score: 0.7253\n",
      "Weighted Precision: 0.8481\n",
      "Weighted Recall: 0.8443\n",
      "Weighted FPR: 0.1557\n",
      "Weighted FNR: 0.0173\n",
      "Weighted F1-score: 0.8430\n",
      "Accuracy: 0.8443\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[8.44269466e-01, 8.44269466e-01, 1.55730534e-01, 9.96222222e+02,\n",
       "        8.44269466e-01, 7.19240684e-01, 7.41426542e-01, 2.58573458e-01,\n",
       "        2.58573458e-01, 7.25326300e-01, 8.48085382e-01, 8.44269466e-01,\n",
       "        1.55730534e-01, 1.73033926e-02, 8.42988487e-01, 8.44269466e-01]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1296x576 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Confusion matrix for each category.\n",
    "getConfusionMatrix_Multiclass(mypredicted_labels, myreallabels, saveFig=True, path=w2vmetricPath, modelName=model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2cd1d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7837fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
