"""
    Testing our model on muVulDeePecker's dataset, compromised of:
     181,641 pieces of code, 138,522 are non-vulnerable and the other 
     43,119 are vulnerable and contain 40 types of vulnerabilities in total. 
     This dataset is available at https://github.com/muVulDeePecker/muVulDeePecker.

     We're only using their vulnerable samples and mapping to our IDs here.
"""

import os, sys

VUL_PATH = os.environ.get('VUL_PATH')
sys.path.insert(1, VUL_PATH)

from SYSE_2_vulnerabilityType.VulnerabilityClassification import VulnerabilityClassification
from utils.Word2VecModel import Word2VecModel
from utils.DLCustomModels import create_bgru_model, create_blstm_model
from utils.utils import GetDataByIndex, drop_non_vulnerable_samples, getDatasetSectionByIndex, print_samples_avg_len_per_class, set_newdataset


def main(VUL_TYPE, slicefile, vectorsALLdir='ALL_vectors', MODEL_TYPE='bgru', model_fn=create_bgru_model, min_num=None):
    RANDOMSEED = 1099
    BATCHSIZE = 64
    LAYERS = 2
    DROPOUT = 0.5
    EPOCHS = 60
    UNITS = 500
    CLASS_TYPE = f'{VUL_TYPE}'
    VECTOR_TRANSFORMER='w2v'
    OPTIMIZER = 'RMSProp'
    ACTIVATION_FN = 'tanh'
    RECURRENT_ACTIVATION = 'sigmoid'
    DENSE_ACT_FN = 'softmax'
    METRICS = ['CategoricalAccuracy']

    all_tokens_path = os.path.join('data', 'tokens_muVDP')
    inputsRootPath = os.path.join('data', 'DLinputs')
    vectorRootPath = os.path.join('data', 'DLvectors')
    vectorsALLPath = os.path.join(vectorRootPath, vectorsALLdir)
    vectorTrainPath = os.path.join(vectorRootPath, 'train')
    vectorTestPath = os.path.join(vectorRootPath, 'test')
    dlInputsTrainPath = os.path.join(inputsRootPath, 'train')
    dlInputsTestPath = os.path.join(inputsRootPath, 'test')

    checkpoint_dir = './ckpt_%s_%s_%s_%s' % (VECTOR_TRANSFORMER, MODEL_TYPE, str(BATCHSIZE), CLASS_TYPE)
    model_name = '%s_%s_epochs_%s_opt_%s_%s_%s_%sunits_%s' % (MODEL_TYPE.upper(), VECTOR_TRANSFORMER.upper(), EPOCHS, OPTIMIZER, ACTIVATION_FN, DENSE_ACT_FN, UNITS, CLASS_TYPE)
    metrics_path = os.path.join(f'metrics', MODEL_TYPE)
    weights_path = os.path.join('model', model_name + 'weights')
    w2vmodelPath = os.path.join('w2vModel','model', f'w2vModel_{VUL_TYPE}')

    VC = VulnerabilityClassification(build_model=model_fn,
        transformer_model=Word2VecModel, transformerPath=w2vmodelPath,
        metricsPath=metrics_path, randomSeed=RANDOMSEED, window=3, m_epochs=EPOCHS,
        modelName=model_name, batch_size=BATCHSIZE, mask=True, dropout=DROPOUT, layers=LAYERS,
        tokensPath=all_tokens_path, vectorsALLPath=vectorsALLPath,
        vectorTrainPath=vectorTrainPath, vectorTestPath=vectorTestPath,
        inputsTrainPath=dlInputsTrainPath, inputsTestPath=dlInputsTestPath,
        checkpoint_dir=checkpoint_dir, weightpath=weights_path, optimizer=OPTIMIZER,
        activation_fn=ACTIVATION_FN, recurrent_activation=RECURRENT_ACTIVATION, dense_activation_fn=DENSE_ACT_FN,
        units=UNITS, metrics=METRICS)

    print('Starting %s & %s' % (CLASS_TYPE, MODEL_TYPE))

    # Reset if need be
    # print('Resetting checkpoint and weights...')
    # VC.reset_checkpoint_and_weights(VC.weightpath)

    # tokenize the vulnerable samples
    # print('\nTokenizing slices...')
    # VC.tokenizeSlicesPerFile(slicefile, VC.tokensPath, focusPointerIndex=-1)

    # Fit W2V transformer & transform our data
    # print('\nTraining W2V model...')
    # VC.init_transformer()
    # VC.fit_transform(VC.transformerPath, VC.tokensPath, VC.vectorsALLPath, getBalanced=False)

    # Drop non-vulnerable samples
    # outputpath = os.path.join(vectorRootPath, 'ALL_vulnerable_vectors_muVDP')
    # drop_non_vulnerable_samples(VC.vectorsALLPath, outputpath)
    # VC.vectorsALLPath = outputpath

    # Get the mapping of labels
    # import pandas as pd
    # labelspath = os.path.join('data', 'CWE', 'label2CWE_muVDP.txt')
    # labels = pd.read_csv(labelspath, sep='\t', header=0)

    # Update the labels, keeping only the labels that match ours
    # set_newdataset(VC.vectorsALLPath, labels)

    ## Split data into training and test set
    # print('\nSplitting train/test...')
    # VC.splitTrainTest(VC.vectorsALLPath, min_num=min_num, vType=VUL_TYPE, dropOtherVtypes=False)

    # To flatten dataset, we must average out the dimension of the dataset which contains the tokens. Averages out the row length per sample based on focuspointer.
    input_path = os.path.join(VC.vectorTrainPath, 'balanced_train.pkl')
    output_path = os.path.join(VC.inputsTrainPath, 'DL_Final_balanced_train.pkl')
    VC.SetVLength(input_path, output_path)  # outputs to data/DLinput
    # Flatten 3D vectors to 2D
    # FlattenVectors(output_path, output_path, VC.avg, VC.vector_size)
    input_path = os.path.join(VC.vectorTestPath, 'balanced_test.pkl')
    output_path = os.path.join(VC.inputsTestPath, 'DL_Final_balanced_test.pkl')
    VC.SetVLength(input_path, output_path)
    # FlattenVectors(output_path, output_path, VC.avg, VC.vector_size)

    # Don't adjust the row length since they're already flattened. Manually set the new vector length & avg.
    VC.vector_size = len(GetDataByIndex(output_path, index=0)[0][0])
    VC.avg = 1

    # Encode our labels
    VC.hotEncodeLabels()
    print(f'Avg: {VC.avg}\tVector length: {VC.vector_size}\tDense units: {VC.density_units}')
    input_path = os.path.join(VC.inputsTrainPath, 'DL_Final_balanced_train.pkl')
    VC.saveKeyData(input_path)
    input_path = os.path.join(VC.inputsTestPath, 'DL_Final_balanced_test.pkl')
    VC.saveKeyData(input_path)

    # Build dl model & predict results
    print('\nBuilding/fitting DL model...')
    VC.build_and_fit()
    print('\nPredicting & Scoring...')
    VC.predict_and_score()

    print('\n\n\n\n\n\n')


VUL_TYPE = 'ALL_muVDP'
sourcefile = 'mvd.txt'
vectorsALLdir='ALL_vectors_muVDP'
min_number = 100

### BGRU ###
main(VUL_TYPE, sourcefile, vectorsALLdir=vectorsALLdir, MODEL_TYPE='bgru', min_num=min_number)

### BLSTM ###
main(VUL_TYPE, sourcefile, vectorsALLdir=vectorsALLdir, MODEL_TYPE='blstm', model_fn=create_blstm_model, min_num=min_number)
