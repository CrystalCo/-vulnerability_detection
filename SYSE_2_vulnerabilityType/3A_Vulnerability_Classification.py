#!/usr/bin/env python
# coding: utf-8

# # Vulnerability Type Classification
# The following models will be used to see which one performs best when attempting to classify a vulnerability type in a program:
# - Decision Tree
# - K-Nearest Neighbors
# - Naive Bayes

# ### Setting variables
import os, sys
import numpy as np
import pandas as pd

vType = "ALL"
randomSeed = 1099
numSamples = 250 #Max Num of slice samples from each file
vectorDim = 100 #num of vector cols
mergeClasses = [703, 697]
slicePath = './data/slicesSource/'
tokenPath = './data/tokenD2V/SARD/'
d2vmodelPath = './d2vModel/model/d2vModel_ALL'
vectorPath =  './data/vectorD2V/'
multiclasspath = './data/CVE/SARD_CVE_to_groups.csv'
# Training & testing split, where training is balanced & testing is not
vectorTrainPath = './data/MLvectors/train/'
vectorTestPath = './data/MLvectors/test/'

# ### Updating path
# Must insert path to directory above in order to access files in other folders
VUL_PATH = os.environ.get('VUL_PATH')
sys.path.insert(1, VUL_PATH)


# ### A. slicesToTokens.py
from SYSE_1_isVulnerable.slicesToTokens import tokenizeSlices_Multiclass
mycase_ID = tokenizeSlices_Multiclass(slicePath, tokenPath, multiclasspath, numSamples, mergeClasses)

# ### B.  isDuplicatedID.py
from SYSE_1_isVulnerable.isDuplicatedID import isDuplicatedID
print("The dataset has duplicated ID: ", isDuplicatedID(mycase_ID))

# ### C. tokensToDocVectors.py
from tokensToDocVectors import createD2VModel, fitD2VModel
myD2Vmodel = createD2VModel(d2vmodelPath, tokenPath, vectorDim)
fitD2VModel(d2vmodelPath, tokenPath, vectorPath)

# ### D. splitTrainTest.py
from SYSE_1_isVulnerable.splitTrainTest import splitTrainTest
splitTrainTest(vType, vectorPath, vectorTrainPath, vectorTestPath, randomSeed, split = 0.8 )

# ### E. downSampling.py
from SYSE_1_isVulnerable.downSampling import appendCaseIDLabels, downsamplingMulticlass, isMulticlassBalanced
categoryCasesDict, downsampleNum = appendCaseIDLabels(vectorTrainPath) # data/mLvectors/ALL_*.pkl
downsamplingMulticlass(categoryCasesDict, downsampleNum, randomSeed, vectorPath, vectorTrainPath)
#Optional used to check if the class label are balanced 
print("Class Labels are balanced: %s\n" % isMulticlassBalanced(vectorTrainPath))



# ### Import data
# #### Get D2V vector datasets 
from MLMethods import getDataset, get_grid, set_group_id
train_dataset, labels, sard_cve_ids, testcases, vtypes = getDataset(vectorTrainPath, 1, randomSeed)
print('Length of training data: ', len(labels))
train_df = pd.DataFrame({'testcase': testcases, 'sard_cve_id': sard_cve_ids, 'vtype': vtypes, 'label': labels})

test_dataset, labels, sard_cve_ids, testcases, vtypes = getDataset(vectorTestPath, 0, randomSeed)
test_df = pd.DataFrame({'testcase': testcases, 'sard_cve_id': sard_cve_ids, 'vtype': vtypes, 'label': labels})

# #### Get group ids per program
groups_df = pd.read_csv('./data/CVE/SARD_CVE_to_groups.csv', index_col=0)

# #### Append the group id associated to each case
set_group_id(train_df, groups_df)
set_group_id(test_df, groups_df)


# #### Drops rows with any missing group ids
null_indices1 = pd.isnull(train_df).any(1).to_numpy().nonzero()[0]
print('Indices with null values (looking for null group ids in particular): ', null_indices1)
print('Dropping any rows in training data that are missing group ids...')
print(train_df.shape)
train_df.dropna(inplace=True)
print(train_df.shape)

null_indices2 = pd.isnull(test_df).any(1).to_numpy().nonzero()[0]
print('\nIndices with null values (looking for null group ids in particular)', null_indices2)
print('Dropping any rows in testing data that are missing group ids...')
print(test_df.shape)
test_df.dropna(inplace=True)
print(test_df.shape)


# #### Set training, testing, and target variables
# Set training data
train_data = pd.DataFrame(train_dataset)
train_data.drop(index=null_indices1, inplace=True)
X_train = train_data.values

# Set testing data
test_data = pd.DataFrame(test_dataset)
test_data.drop(index=null_indices2, inplace=True)
X_test = test_data.values

# Set target attribute 
Y_train = train_df['group_id'].values
Y_train = Y_train.astype('int')

Y_test = test_df['group_id'].values
Y_test = Y_test.astype('int')


# ## Decision Tree
from sklearn import neighbors, tree, naive_bayes

print('\nGrid search on DT')
model = tree.DecisionTreeClassifier()
sample_leaf_fractions = np.linspace(0.0005, 0.02, 15)*13.06
sample_split_fractions = np.linspace(0.0005, 0.015, 10)*13.06
scoring = 'accuracy'
parameters = {
    'criterion': ['entropy','gini'],
    'max_depth': np.linspace(1, 20, 10),
    'min_samples_leaf': sample_leaf_fractions,
    'min_samples_split': sample_split_fractions
}
grid = get_grid(model, parameters, scoring, X_train, Y_train)
treeclf = tree.DecisionTreeClassifier(criterion='entropy', max_depth=13, min_samples_leaf=0.00653, min_samples_split=0.027571)
treeclf.fit(X_train, Y_train)

from MLMethods import measure_performance
print('Measuring performance')
print('Score on training: ', treeclf.score(X_train, Y_train))
print('Score on testing: ', treeclf.score(X_test, Y_test))
measure_performance(X_test, Y_test, treeclf, show_confusion_matrix=False)


# ## K-Nearest Neighbors
# Consider normalizing the data for better score
print('\nGrid search on KNN')
model = neighbors.KNeighborsClassifier()
scoring = 'accuracy'
parameters = {
    'n_neighbors': np.arange(1, 9, 1),
    'weights': ['uniform', 'distance'],
    'p': [1, 2], # 1 = manhattan distance, 2 = euclidean distance
}
grid = get_grid(model, parameters, scoring, X_train, Y_train)
knnclf =  neighbors.KNeighborsClassifier(n_neighbors=4, p=2, weights='distance')
knnclf.fit(X_train, Y_train)
print('Measuring performance')
print('Score on training: ', knnclf.score(X_train, Y_train))
print('Score on testing: ', knnclf.score(X_test, Y_test))
measure_performance(X_test, Y_test, knnclf, show_confusion_matrix=False)


# ## Gaussian Naive Bayes
# I don't expect this method to perform well since Bayes' theorem assumes features are strongly independent, when in fact ours is tightly coupled. https://scikit-learn.org/stable/modules/classes.html?highlight=naive%20bayes#module-sklearn.naive_bayes 
print('\n Gaussian Naive Bayes')
nbclf = naive_bayes.GaussianNB()
nbclf = nbclf.fit(X_train, Y_train)
print('Measuring performance')
print ("Score on Training: ", nbclf.score(X_train, Y_train))
print ("Score on Testing: ", nbclf.score(X_test, Y_test))
measure_performance(X_test, Y_test, nbclf, show_confusion_matrix=False)




