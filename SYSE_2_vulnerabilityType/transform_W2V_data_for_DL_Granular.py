
"""
    Transforms the 2D W2V dataset into 1D.
    This speeds up performance.
"""

import gc, os, sys

import numpy as np

VUL_PATH = os.environ.get('VUL_PATH')
sys.path.insert(1, VUL_PATH)

from SYSE_2_vulnerabilityType.DetectVulType import DetectVulType
from utils.DLCustomModels import create_bgru_model, create_blstm_model
from utils.MLMethods import convert_nested_lists_to_numpy_arrays
from utils.utils import getDataset, save_data_to_file

RANDOMSEED = 1099
CLASS_TYPE = 'Granular_162'
MODEL_TYPE = 'bgru'
VECTOR_TRANSFORMER='w2v'
LAYERS = 2
DROPOUT = 0.2
BATCHSIZE = 64
EPOCHS = 60

vectorRootPath = os.path.join('data','DLvectors')
vectorsALLPath = os.path.join(vectorRootPath, 'ALL_vectors_granular')
vectorTrainPath = os.path.join(vectorRootPath,'train_162classes')
vectorTestPath = os.path.join(vectorRootPath,'test_162classes')
vectorTrainPath_flat = os.path.join(vectorRootPath,'train_162classes_flattened')
vectorTestPath_flat = os.path.join(vectorRootPath,'test_162classes_flattened')
inputRootPath = os.path.join('data', 'DLinputs')
dlInputsTrainPath = os.path.join(inputRootPath,'train_162classes')
dlInputsTestPath = os.path.join(inputRootPath,'test_162classes')
dlInputsTrainPath_flat = os.path.join(inputRootPath,'train_162classes_flattened')
dlInputsTestPath_flat = os.path.join(inputRootPath,'test_162classes_flattened')
w2vmodelPath = os.path.join('w2vModel','model','w2vModel_ALL')

checkpoint_dir = './ckpt_%s_%s_%s_%s' % (VECTOR_TRANSFORMER, MODEL_TYPE, str(BATCHSIZE), CLASS_TYPE)
model_name = '%s_%s_batch=%s_seed=%s_epochs=%s_%s' % (MODEL_TYPE.upper(), VECTOR_TRANSFORMER.upper(), BATCHSIZE, RANDOMSEED, EPOCHS, CLASS_TYPE)
metrics_path = os.path.join(f'{VECTOR_TRANSFORMER}Model', 'metrics', MODEL_TYPE)
weights_path = os.path.join('model', model_name + 'weights')

dvt = DetectVulType(build_model=create_bgru_model, metricsPath=metrics_path,
                    randomSeed=RANDOMSEED, layers=LAYERS, weightpath=weights_path,
                    batch_size=BATCHSIZE, m_epochs=EPOCHS, modelName=model_name, 
                    vectorTrainPath=vectorTrainPath, vectorTestPath=vectorTestPath,
                    inputsTrainPath=dlInputsTrainPath, inputsTestPath=dlInputsTestPath,
                    checkpoint_dir=checkpoint_dir)

## To flatten dataset, we must average out the dimension of the dataset which contains the tokens
dvt.splitTrainTest(vectorsALLPath, 4)
dvt.adjustVectorLength()

# Flatten Train
data = getDataset(dvt.inputsTrainPath)
dvt.avg = len(data[0][0])
assert len(data[0][0][0]) == 30, f'inputs data vector length {len(data[0][0][0])} != vector size 30'
VECTOR_SIZE = len(data[0][0][0])
input_shape_1 = dvt.avg * VECTOR_SIZE
print(f'Avg: {dvt.avg}\nVector length: {VECTOR_SIZE}')
print(f'New vector size will be: {dvt.avg}x{VECTOR_SIZE}={str(input_shape_1)}')
x_train = convert_nested_lists_to_numpy_arrays(data[0], dvt.avg, VECTOR_SIZE)
x_train = np.reshape(x_train, (x_train.shape[0], input_shape_1))
data[0] = x_train
save_data_to_file(vectorTrainPath_flat, 'DL_Final_balanced_train.pkl', data)

# Flatten Test 
data = getDataset(dvt.inputsTestPath)
x_test = convert_nested_lists_to_numpy_arrays(data[0], dvt.avg, VECTOR_SIZE)
x_test = np.reshape(x_test, (x_test.shape[0], input_shape_1))
data[0] = x_test
save_data_to_file(vectorTestPath_flat, 'DL_Final_balanced_test.pkl', data)


dvt.vectorTrainPath = vectorTrainPath_flat
dvt.vectorTestPath = vectorTestPath_flat
dvt.inputsTrainPath = dlInputsTrainPath_flat
dvt.inputsTestPath = dlInputsTestPath_flat

VECTOR_SIZE = input_shape_1
dvt.vector_size = VECTOR_SIZE
dvt.avg = 1
print(f'Avg: {dvt.avg} Vector: {VECTOR_SIZE}')
dvt.reset_model(weights_path)
dvt.encodeLabels()
dvt.saveKeyData(dvt.vectorTrainPath, dvt.inputsTrainPath)
dvt.saveKeyData(dvt.vectorTestPath, dvt.inputsTestPath)
print(f'Density units: {dvt.density_units}. Vector size: {dvt.vector_size}')
dvt.build_and_fit()
dvt.predict_and_score()



############# BLSTM on Granular Samples #############
MODEL_TYPE = 'blstm'
checkpoint_dir = './ckpt_%s_%s_%s_%s' % (VECTOR_TRANSFORMER, MODEL_TYPE, str(BATCHSIZE), CLASS_TYPE)
model_name = '%s_%s_batch=%s_seed=%s_epochs=%s_%s' % (MODEL_TYPE.upper(), VECTOR_TRANSFORMER.upper(), BATCHSIZE, RANDOMSEED, EPOCHS, CLASS_TYPE)
metrics_path = os.path.join(f'{VECTOR_TRANSFORMER}Model', 'metrics', MODEL_TYPE)
weights_path = os.path.join('model', model_name + 'weights')

dvt = DetectVulType(build_model=create_blstm_model,
                    metricsPath=metrics_path,
                    randomSeed=RANDOMSEED, m_epochs=EPOCHS,
                    batch_size=BATCHSIZE, modelName=model_name, 
                    vectorTrainPath=vectorTrainPath_flat, vectorTestPath=vectorTestPath_flat, 
                    inputsTrainPath=dlInputsTrainPath_flat, inputsTestPath=dlInputsTestPath_flat,
                    checkpoint_dir=checkpoint_dir, weightpath=weights_path)
data = getDataset(dvt.vectorTrainPath)
VECTOR_SIZE = len(data[0][0])
del data
gc.collect()

dvt.avg = 1
dvt.vector_size = VECTOR_SIZE
dvt.reset_check_weights(weights_path)
dvt.encodeLabels()
print(f'Density units: {dvt.density_units}. Vector size: {dvt.vector_size}')
dvt.build_and_fit()
dvt.predict_and_score()
