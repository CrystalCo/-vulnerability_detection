import pandas as pd
import numpy as np
from sklearn.base import BaseEstimator
from sklearn.model_selection import GridSearchCV
from sklearn.neighbors import KNeighborsClassifier
from sklearn.pipeline import Pipeline
from gensim.models.doc2vec import Doc2Vec, TaggedDocument

from SYSE_2_vulnerabilityType.tokensToDocVectors import DirofCorpus, setDataVector
from SYSE_2_vulnerabilityType.MLMethods import getDataset
from SYSE_1_isVulnerable.splitTrainTest import splitTrainTest

class Doc2VecModel(BaseEstimator):
    
    def __init__(self, alpha=0.01, negative=10, epochs=1, vector_size=100, window=1):
        # Grid search to find best params for vector_sizes, epochs, window
        # Once we have best params for this, test on dm_mean, dm_concat, & dbow_words
        self.d2v_model = None
        self.alpha = alpha
        self.negative = negative
        self.epochs = epochs
        self.vector_size = vector_size
        self.window = window

    def fit(self, x, y=None):
        # Initialize model
        self.d2v_model = Doc2Vec(alpha=self.alpha,  min_count=0, max_vocab_size=None, hs=0, negative=self.negative, workers=4, window=self.window, vector_size=self.vector_size)

        # Tag docs
        tagged_documents = []
        for index in range(len(x)):
            tagged_documents.append(TaggedDocument(words=x[index], tags=[index]))
        # Build vocabulary
        self.d2v_model.build_vocab(tagged_documents)
        # Train model
        self.d2v_model.train(tagged_documents, total_examples=len(tagged_documents), epochs=self.epochs)
        return self

    def transform(self, x):
        x_vectors = []
        for index in range(len(x)):
            x_vectors.append(self.d2v_model.infer_vector(x[index]))
        x_vectors = pd.DataFrame(x_vectors)
        return x_vectors

    def fit_transform(self, x, y=None):
        self.fit(x)
        return self.transform(x)

