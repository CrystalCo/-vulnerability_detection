import pandas as pd
import numpy as np
from sklearn.base import BaseEstimator
from sklearn.model_selection import GridSearchCV
from sklearn.neighbors import KNeighborsClassifier
from sklearn.pipeline import Pipeline
from gensim.models.doc2vec import Doc2Vec, TaggedDocument

from SYSE_2_vulnerabilityType.tokensToDocVectors import DirofCorpus, setDataVector
from SYSE_2_vulnerabilityType.MLMethods import getDataset
from SYSE_1_isVulnerable.splitTrainTest import splitTrainTest

class Doc2VecModel(BaseEstimator):
    
    def __init__(self, epochs=1, vector_size=100, window=1):
        # Grid search to find best params for vector_sizes, epochs, window
        # Once we have best params for this, test on dm_mean, dm_concat, & dbow_words
        self.d2v_model = None
        self.token_path = './data/token/SARD/'
        self.vectorPath = './data/vector/'
        self.d2vmodelPath = './d2vModel/model/d2vModel_ALL'
        self.epochs = epochs
        self.vector_size = vector_size
        self.window = window

    def fit(self, raw_documents=None, y=None):
        # Initialize, build, & train model
        self.d2v_model = Doc2Vec(documents=DirofCorpus(self.token_path), alpha=0.01, epochs=self.epochs,  min_count=0, max_vocab_size=None, sg=1, hs=0, negative=10, workers=1, window=self.window, vector_size=self.vector_size)

        # TODO: fit the model
        setDataVector(self.token_path, self.vectorPath, self.d2v_model)

        return self

    def transform(self, raw_documents):
        X = []
        for index, row in raw_documents.iteritems():
            X.append(self.d2v_model.infer_vector(row))
        X = pd.DataFrame(X, index=raw_documents.index)
        return X

    def fit_transform(self, raw_documents, y=None):
        self.fit(raw_documents)
        return self.transform(raw_documents)

