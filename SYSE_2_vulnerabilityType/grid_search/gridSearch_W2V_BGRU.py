import os, sys
from numpy.lib.npyio import save

from sklearn.model_selection import GridSearchCV
from sklearn.pipeline import Pipeline

#### Setting variables
vType = "ALL"
randomSeed = 1099
numSamples = 420627
slicePath = os.path.join('data','slicesSource')
tokenPath = os.path.join('data','token','SARD')
multiclasspath = os.path.join('data','CVE','SARD_CVE_to_groups.csv')
w2vmodelPath = os.path.join('w2vModel','model','w2vModel_ALL')
vectorPath = os.path.join('data','vector')
vectorTypePath = os.path.join('data','DLvectors')
vectorTrainPath = os.path.join(vectorTypePath,'train')
vectorTestPath = os.path.join(vectorTypePath,'test')
dlInputsTrainPath = os.path.join('data','DLinputs','train')
dlInputsTestPath  = os.path.join('data','DLinputs','test')
VUL_PATH = os.environ.get('VUL_PATH') 
sys.path.insert(1, VUL_PATH)


#### Preprocessing
print('TOKENIZING SLICES...')
from SYSE_1_isVulnerable.slicesToTokens import tokenizeSlices_Multiclass
testcase_ids, testcase_ids_per_group = tokenizeSlices_Multiclass(slicePath, tokenPath, multiclasspath, numSamples)

print('\nCOMBINING VECTORS...')
from utils.combineVectors import combine_vectors_from_files
combine_vectors_from_files(tokenPath, vectorTypePath, randomSeed)


print('\nDOWNSAMPLING THE DATA...')
from utils.utils import downsample, getDataset
all_data = getDataset(vectorTypePath, False, randomSeed)
downsampled_data = downsample(all_data, 0, 1000)
downsampled_data = downsample(downsampled_data, 664, 1000)
downsampled_data = downsample(downsampled_data, 710, 1000)
downsampled_data = downsample(downsampled_data, 707, 1000)
downsampled_data = downsample(downsampled_data, 682, 1000)


print('\nSAVING BALANCED DATA TO FILE...')
from utils.utils import save_data_to_file
save_data_to_file(vectorTypePath, 'balanced_vectors.pkl', downsampled_data)


print('\nSPLITTING INTO TRAINING/TESTING SETS...')
from SYSE_1_isVulnerable.splitTrainTest import splitTrainTestCategorical
splitTrainTestCategorical('balanced', vectorTypePath, vectorTrainPath, vectorTestPath, randomSeed, dropClass=0)


print('\nHOT ENCODING CATEGORICAL LABELS...')
from utils.utils import num_classes, encode_target
categories = num_classes(vectorTrainPath)
density_units = len(categories)
mapping, labelEncoder = encode_target(categories)

from SYSE_1_isVulnerable.adjustVectorLen import meanLen
avg = meanLen(vectorTrainPath)

print('\nREPLACING GROUP IDS WITH ENCODED LABELS IN FILE...')
from SYSE_1_isVulnerable.saveKeyData import saveKeyDataMulticlass
# Leave split train/test files in vector train path so as not to interfere with W2V+BGRU on default params file (3B DL Categorical); 
# Update input train path with encoded labels instead
saveKeyDataMulticlass(vectorTrainPath, labelEncoder, dlInputsTrainPath)
saveKeyDataMulticlass(vectorTestPath, labelEncoder, dlInputsTestPath)


#### Grid Search for best parameters on W2V & BGRU models
from joblib import Memory
from shutil import rmtree

from utils.Word2VecModel import Word2VecModel
from utils.KerasClassifier import KerasClassifier
from utils.DLCustomModels import create_bgru_model

# Create a temporary folder to store the transformers of the pipeline
location = 'cachedir'
memory = Memory(location=location, verbose=2)

bgru_estimator = KerasClassifier(build_fn=create_bgru_model, verbose=3)
pipe = Pipeline([('word2vec', Word2VecModel()), ('bgru', bgru_estimator)], memory=memory, verbose=True)

param_grid = {
    'word2vec__alpha': [0.05],
    'word2vec__negative': [10, 15],
    'word2vec__epochs': [10, 20],
    'word2vec__sample': [0.001],
    'word2vec__seed': [randomSeed],
    'word2vec__window': [2, 3, 4, 5],
    'word2vec__workers': [1],
    'word2vec__vector_size': [30],
    'bgru__maxlen': [avg],
    'bgru__density': [density_units],
    'bgru__dropout': [0.2],
    'bgru__epochs': [20], 
    'bgru__batch_size':[16],
    'bgru__vector_dim':['word2vec__vector_size']
}

grid = GridSearchCV(estimator=pipe,
                    param_grid=param_grid,
                    cv=3,
                    error_score=0,
                    n_jobs=1, # change to -1 to use all processors on server
                    verbose=2)

print('\nGRID SEARCH ON TRAINING DATASET COMMENCING...')
from utils.utils import getDataset

data = getDataset(dlInputsTrainPath, False, randomSeed)
x_train = data[0]
y_train = data[-2]

grid.fit(x_train, y_train)

print('\nGRID SEARCH ON TRAINING DATASET COMPLETE. RESULTS:\n')
print("Best Parameters: {}\n".format(grid.best_params_))
print("Best accuracy: {}\n".format(grid.best_score_))
print("Finished.")

# Delete the temporary cache before exiting
memory.clear(warn=False)
rmtree(location)
