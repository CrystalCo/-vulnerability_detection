import os, sys

from sklearn.model_selection import GridSearchCV
from sklearn.pipeline import Pipeline

#### Setting variables
vType = "ALL"
randomSeed = 1099
numSamples = 1000
slicePath = './data/slicesSource/'
tokenPath = './data/token/SARD/'
multiclasspath = './data/CVE/SARD_CVE_to_groups.csv'
vectorTrainPath = './data/DLvectors/train/'
vectorTestPath = './data/DLvectors/test/'
inputsTrainPath = './data/DLinputs/train/'
inputsTestPath = './data/DLinputs/test/'
VUL_PATH = os.environ.get('VUL_PATH') 
sys.path.insert(1, VUL_PATH)


#### Preprocessing
print('TOKENIZING SLICES...')
from SYSE_1_isVulnerable.slicesToTokens import tokenizeSlices_Multiclass
testcase_ids, testcase_ids_per_group = tokenizeSlices_Multiclass(slicePath, tokenPath, multiclasspath, numSamples)

print('\nSPLITTING INTO TRAINING/TESTING SETS...')
from SYSE_1_isVulnerable.splitTrainTest import splitTrainTest
splitTrainTest(vType, tokenPath, vectorTrainPath, vectorTestPath, randomSeed, split = 0.8 )

print('\nHOT ENCODING CATEGORICAL LABELS...')
from utils.utils import num_classes, encode_target
categories = num_classes(vectorTrainPath)
num_units = len(categories)
mapping, labelEncoder = encode_target(categories)

from SYSE_1_isVulnerable.adjustVectorLen import meanLen
avg = meanLen(vectorTrainPath, vType)

print('\nSAVING KEY DATA...')
from SYSE_1_isVulnerable.saveKeyData import saveKeyDataMulticlass
# Leave split train/test files in vector train path so as not to interfere with W2V+BGRU on default params file (3B DL Categorical); 
# Update input train path with encoded labels instead
saveKeyDataMulticlass(vectorTrainPath, labelEncoder, inputsTrainPath)
saveKeyDataMulticlass(vectorTestPath, labelEncoder, inputsTestPath)


#### Grid Search for best parameters on W2V & BGRU models
from joblib import Memory
from shutil import rmtree

from utils.Word2VecModel import Word2VecModel
from utils.KerasClassifier import KerasClassifier
from utils.DLCustomModels import create_bgru_model

# Create a temporary folder to store the transformers of the pipeline
location = 'cachedir2'
memory = Memory(location=location, verbose=2)

bgru_estimator = KerasClassifier(build_fn=create_bgru_model, verbose=3)
pipe = Pipeline([('word2vec', Word2VecModel()), ('bgru', bgru_estimator)], memory=memory, verbose=True)

param_grid = {
    'word2vec__alpha': [0.01, 0.05],
    'word2vec__negative': [10, 15],
    'word2vec__epochs': [10],
    'word2vec__sample': [0.001],
    'word2vec__seed': [randomSeed],
    'word2vec__window': [2, 4, 6],
    'word2vec__workers': [1],
    'word2vec__vector_size': [30],
    'bgru__maxlen': [avg],
    'bgru__units2': [num_units],
    'bgru__dropout': [0.1, 0.2],
    'bgru__epochs': [10], 
    'bgru__batch_size':[32],
    'bgru__vector_dim':[30],
}

grid = GridSearchCV(estimator=pipe,
                    param_grid=param_grid,
                    cv=2,
                    error_score=0,
                    n_jobs=5, # change to -1 to use all processors on server
                    verbose=3)

print('\nGRID SEARCH ON TRAINING DATASET COMMENCING...')
from utils.utils import getDataset

data = getDataset(inputsTrainPath, False, randomSeed)
x_train = data[0]
y_train = data[-2]

grid.fit(x_train, y_train)

print('\nGRID SEARCH ON TRAINING DATASET COMPLETE. RESULTS:\n')
print("Best Parameters: {}\n".format(grid.best_params_))
print("Best accuracy: {}\n".format(grid.best_score_))
print("Finished.")

# Delete the temporary cache before exiting
memory.clear(warn=False)
rmtree(location)
