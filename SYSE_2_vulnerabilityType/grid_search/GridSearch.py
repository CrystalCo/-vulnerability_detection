import os, sys
from shutil import rmtree

VUL_PATH = os.environ.get('VUL_PATH')
sys.path.insert(1, VUL_PATH)

import keras
import numpy as np
import tensorflow as tf

from SYSE_1_isVulnerable.slicesToTokens import tokenizeSlices_Multiclass
from SYSE_1_isVulnerable.splitTrainTest import splitTrainTestCategorical
from SYSE_1_isVulnerable.adjustVectorLen import meanLen
from SYSE_1_isVulnerable.saveKeyData import saveKeyDataMulticlass
from SYSE_1_isVulnerable.DLModel import buildBGRU2, fitModel2
from SYSE_1_isVulnerable.ConfusionMatrix import getConfusionMatrix_Multiclass
from SYSE_1_isVulnerable.DLPrediction import predictMulticlassLabel
from SYSE_1_isVulnerable.evaluateModels import roc_auc_score_multiclass
from utils.Word2VecModel import Word2VecModel
from utils.DLCustomModels import make_or_restore_model
from utils.transformDataDimensions import tranformDimsByFocus
from utils.utils import downsample, getDataset, encode_target, save_data_to_file

class GridSearch():
    
    def __init__(self, build_model, transformer_model=None, params={}, metrics=[], cv=1, vectorBalancedPath=os.path.join('data','DLvectors', 'ALL_balanced'), vectorTypePath=os.path.join('data','DLvectors'), vectorTrainPath=os.path.join('data','DLvectors','train'), vectorTestPath=os.path.join('data','DLvectors','test'), inputsTrainPath=os.path.join('data','DLinputs','train'), inputsTestPath=os.path.join('data','DLinputs','test')) -> None:
        self.build_model = build_model
        self.model = None
        self.transformer_model = transformer_model
        self.transformer = None
        self.params = params
        self.metrics = metrics
        self.cv = cv
        self.avg = 0
        self.randomSeed = None
        self.tokenFocusPath = os.path.join('data', 'tokenFocus') # files in here replaced the functions index with focus pointer index; ALL_tokens.pkl
        self.vector_size = None # Gets initialized in run_all()
        self.vectorTypePath = vectorTypePath
        self.vectorBalancedPath = vectorBalancedPath
        self.vectorTrainPath = vectorTrainPath
        self.vectorTestPath = vectorTestPath
        self.inputsTrainPath = inputsTrainPath
        self.inputsTestPath = inputsTestPath
        self.checkpoint_dir = "./ckpt"
        
    def tokenizeSlices(self):
        # Initializes the dataset
        slicePath = os.path.join('data','slicesSource')
        tokenPath = os.path.join(self.tokenFocusPath,'SARD')
        multiclasspath = os.path.join('data','CVE','SARD_CVE_to_groups.csv')
        tokenizeSlices_Multiclass(slicePath, tokenPath, multiclasspath, 420627)

    def downsample(self):        
        print('\nDOWNSAMPLING THE DATA...')
        all_data = getDataset(self.tokenFocusPath, False, self.randomSeed)
        original_labels =  [x[1] for x in all_data[-2]]
        # Since we are not merging any classes, we'll replace the group id section with the flattened version containing the original label
        all_data[-2] = original_labels
        downsampled_data = downsample(all_data, 664, 1000)
        downsampled_data = downsample(downsampled_data, 710, 1000)
        downsampled_data = downsample(downsampled_data, 707, 1000)
        downsampled_data = downsample(downsampled_data, 682, 1000)

        print('SAVING BALANCED, DOWNSAMPLES DATA TO FILE...')
        save_data_to_file(self.vectorTypePath, 'balanced_vectors.pkl', downsampled_data)

    def fit_transform(self):
        # Fit on all the data & transform to vectors
        data = getDataset(dataset_path=self.vectorTypePath, getBalanced=True)
        X = data[0]
        X = self.transformer.fit_transform(X) # takes ~1min 30sec
        data[0] = X

        # Save data to a file
        save_data_to_file(self.vectorBalancedPath, 'balanced_vectors.pkl', data)
        
    def splitTrainTest(self):
        # Split into train/test
        splitTrainTestCategorical('balanced', self.vectorBalancedPath, self.vectorTrainPath, self.vectorTestPath, randomSeed=self.randomSeed)

    def adjustVectorLength(self):
        # Adjust vector length TODO: Why do we do the avg instead of all the lines? Will our models perform worse if too many zero-padded vectors?
        self.avg = meanLen(self.vectorTrainPath)

        tranformDimsByFocus(self.vectorTrainPath, self.vectorTestPath, self.inputsTrainPath, self.inputsTestPath, self.avg, self.vector_size)
        print(f'New Vector Length (rows x cols): {self.avg} x {self.vector_size}\n')

    def encodeLabels(self):
        # ### Get number of unique classes for density value
        train_data = getDataset(self.vectorTrainPath, getBalanced=True)
        categories = np.unique(train_data[-2])
        self.labelEncoder = encode_target(categories)[1]
        self.density_units = categories.shape[0]

        saveKeyDataMulticlass(self.inputsTrainPath, self.labelEncoder)
        saveKeyDataMulticlass(self.inputsTestPath, self.labelEncoder)

    def build_and_fit(self):
        """
            builds & fits the estimator model
        """
        self.optimizer = 'adam' #can be changed to ‘adamax’
        self.layers = 2
        self.dropout = 0.2 
        self.batch_size = 16
        epochs = 20
        self.activation_fn = 'softmax'

        # Prepare a directory to store all the checkpoints.
        if not os.path.exists(self.checkpoint_dir):
            os.makedirs(self.checkpoint_dir)

        # Open a strategy scope
        strategy = tf.distribute.MirroredStrategy()
        print('Number of devices: {}'.format(strategy.num_replicas_in_sync))
        with strategy.scope():
            myKerasModel = make_or_restore_model(self.checkpoint_dir, self.get_compiled_model)

        callbacks = [
            # This callback saves a SavedModel every epoch
            # We include the current epoch in the folder name.
            keras.callbacks.ModelCheckpoint(
                filepath=self.checkpoint_dir + "/ckpt-{epoch}", save_freq="epoch"
                )
        ]

        # Fit BGRU Model with trained data and save the model for later use
        self.weightpath = os.path.join('model', 'BRGU_ALL' + self.optimizer +str(self.randomSeed))
        self.model = fitModel2(myKerasModel, self.weightpath, self.inputsTrainPath, self.batch_size, self.avg, self.vector_size, self.randomSeed, epochs, useGenerator=True, callbacks=callbacks)

    def get_compiled_model(self):
        return self.build_model(self.avg, self.vector_size, self.layers, self.dropout, self.optimizer, self.density_units, activation_fn=self.activation_fn)

    def predict_and_score(self):
        modelName = 'BGRU%s_' % self.randomSeed

        # Prediction
        self.model.load_weights(self.weightpath)
        thresholds_dl_labels, mypredicted_labels, myreallabels, outputs_dict = predictMulticlassLabel(self.model, self.inputsTestPath, self.avg, self.vector_size, self.optimizer, modelName, self.randomSeed, self.labelEncoder, saveOutput=False)

        # Confusion Matrix
        metrics = getConfusionMatrix_Multiclass(mypredicted_labels, myreallabels, saveFig=False, path='None')
        accuracy = metrics[0]
        weighted_precision = metrics[-3]
        weighted_recall = metrics[-2]
        weighted_f1 = metrics[-1]

        # ROC
        roc_auc_dict = roc_auc_score_multiclass(myreallabels, mypredicted_labels)
        print(f'\n{modelName} ROC AUC Score\n{roc_auc_dict}\n')

        return outputs_dict, accuracy, weighted_precision, weighted_recall, weighted_f1


    def run_all(self, startOver=True):
        # standard vars
        id = 1
        cv=3
        randomSeeds = [0, 871, 1099]
        # W2V params
        alpha = 0.05
        epochs = 15
        sample = 0.001
        workers = -1
        negatives = [10, 15]
        windows = [2, 3, 4, 5]
        vector_sizes = [5, 15, 30, 50]
        # Save outputs to file
        filepath = os.path.join('gs_results', 'w2v_bgru_gs_results.txt')
        columns = '\t'.join(['ID', 'cv', 'randomSeed', 'TRANSFORMER', 'alpha', 'epochs', 'sample', 'negative', 'window', 'vector_size', 'MODEL', 'input_shape0-maxlen', 'input_shape1-vector_size', 'layers', 'dropout', 'batch_size', 'epochs', 'optimizer', 'density', 'final_activation_fn', 'loss', 'categorical_accuracy', 'recall', 'accuracy', 'weighted_precision', 'weighted_recall', 'weighted_f1'])

        if startOver:
            f = open(filepath, 'w')
            f.write(columns + '\n')
            f.close()

            if os.path.exists(self.checkpoint_dir):
                rmtree(self.checkpoint_dir)

        for negative in negatives:
            for window in windows:
                for vector_size in vector_sizes:
                    self.vector_size = vector_size
                    for i in range(cv):
                        self.randomSeed = randomSeeds[i]

                        # Init transformer
                        self.transformer = self.transformer_model(vector_size=vector_size, alpha=alpha, negative=negative, sample=sample, epochs=epochs, seed=self.randomSeed, window=window, workers=workers)
                        
                        # Fit transformer & transform our data
                        self.fit_transform()
                        # Split data into training and test set
                        self.splitTrainTest()
                        # Average out the row length per sample based on focuspointer
                        self.adjustVectorLength()
                        # Hot encode labels
                        self.encodeLabels()
                        # Build & fit the model
                        assert vector_size == self.vector_size, "input vector size for BGRU model should match vector size used in W2V"
                        print(f'Building model on data W2V transformed with paramaters: ID {id}, CV:{cv}, Seed: {self.randomSeed}, Alpha: {alpha}, Epochs: {epochs}, Sample: {sample},')
                        print(f'Negative: {negative}, Window: {window}, Vector length: {vector_size}.')
                        print(f'On BGRU with input_shape0 (maxlen): {str(self.avg)}, input_shape1 (vector_size): {str(self.vector_size)}, layers:{self.layers}, dropout: {self.dropout}, batch_size: {self.batch_size}, epochs: 20, optimizer: {self.optimizer}, density: {self.density_units}, final_activation_fn: {self.activation_fn}')
                        self.build_and_fit()
                        # Predict and score on test set
                        outputs_dict, accuracy, weighted_precision, weighted_recall, weighted_f1 = self.predict_and_score()

                        # Save params and scores to a file
                        data = '\t'.join([str(id), str(i), str(self.randomSeed), 'W2V', str(alpha), str(epochs), str(sample), str(negative), str(window), str(vector_size), 'BGRU', str(self.avg), str(self.vector_size), str(self.layers), str(self.dropout), str(self.batch_size), '20', self.optimizer, str(self.density_units), self.activation_fn, str(outputs_dict['loss']), str(outputs_dict['categorical_accuracy']), str(outputs_dict['recall']), str(accuracy), str(weighted_precision), str(weighted_recall), str(weighted_f1)])

                        f = open(filepath, 'a')
                        f.write(data + '\n')
                        f.close()

                        # Remove our checkpoint so the new model won't load our previous model
                        rmtree(self.checkpoint_dir)



gs = GridSearch(build_model=buildBGRU2, transformer_model=Word2VecModel)
# gs.tokenizeSlices() # Initializes the dataset (if need be)
# gs.downsample() # Downsample the dataset after tokenizing slices
# If the 2 methods above have already been run, you may comment them out and simply run below:
gs.run_all(startOver=True)

