import pandas as pd
import numpy as np
from sklearn.base import BaseEstimator
from sklearn.model_selection import GridSearchCV
from sklearn.cluster import AgglomerativeClustering
from sklearn.metrics import silhouette_score
from sklearn.pipeline import Pipeline
from gensim.models.doc2vec import Doc2Vec, TaggedDocument

from SYSE_2_vulnerabilityType.tokensToDocVectors import DirofCorpus
from SYSE_2_vulnerabilityType.MLMethods import getDataset

class Doc2VecModel(BaseEstimator):
    
    def __init__(self, epochs=1, vector_size=100, window=1):
        # Grid search to find best params for vector_sizes, epochs, window
        # Once we have best params for this, test on dm_mean, dm_concat, & dbow_words
        self.d2v_model = None
        self.token_path = './data/token/SARD/'
        self.d2vmodelPath = './d2vModel/model/d2vModel_ALL'
        self.epochs = epochs
        self.vector_size = vector_size
        self.window = window

    def fit(self, raw_documents, y=None):
        # Initialize, build, & train model
        self.d2v_model = Doc2Vec(documents=DirofCorpus(self.token_path), alpha=0.01, epochs=self.epochs,  min_count=0, max_vocab_size=None, sg=1, hs=0, negative=10, workers=1, window=self.window, vector_size=self.vector_size)

        # TODO: fit the model

        return self

    def transform(self, raw_documents):
        X = []
        for index, row in raw_documents.iteritems():
            X.append(self.d2v_model.infer_vector(row))
        X = pd.DataFrame(X, index=raw_documents.index)
        return X

    def fit_transform(self, raw_documents, y=None):
        self.fit(raw_documents)
        return self.transform(raw_documents)

def cv_silhouette_scorer(estimator, X):
    estimator.fit(X)
    cluster_labels = estimator.labels_
    num_labels = len(set(cluster_labels))
    num_samples = len(X)
    if num_labels == 1 or num_labels == num_samples:
        return -1
    else:
        return silhouette_score(X, cluster_labels)

param_grid = {'doc2vec__window': [1, 2, 3, 4, 5],
              'doc2vec__epochs': [1, 5],
              'doc2vec__vector_size': [100, 250],
              'agg__n_clusters': np.arange(4, 12),
}

pipe_log = Pipeline([('doc2vec', Doc2VecModel()), ('agg', AgglomerativeClustering(affinity='euclidean', linkage='ward'))])

log_grid = GridSearchCV(pipe_log, 
                        param_grid=param_grid,
                        scoring=cv_silhouette_scorer,
                        return_train_score=True,
                        verbose=1)

fitted = log_grid.fit(dataset["posts"], dataset["type"])

# Best parameters
print("Best Parameters: {}\n".format(log_grid.best_params_))
print("Best accuracy: {}\n".format(log_grid.best_score_))
print("Finished.")
