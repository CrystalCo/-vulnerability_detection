#!/usr/bin/env python
# coding: utf-8

# # Vulnerability Type Classification using ML
# The following models will be used to see which one performs best when attempting to classify a vulnerability type in a program:
# - K-Nearest Neighbors (KNN)
# - Random Forest Classifier (RFC)
# - Support Vector Classifer (SVC)

# ## Setting Variables

import gc, os, sys

import numpy as np
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC

sys.path.insert(1, os.environ.get('VUL_PATH'))

from SYSE_1_isVulnerable.ConfusionMatrix import getConfusionMatrix_Multiclass
from SYSE_2_vulnerabilityType.DetectVulType import DetectVulType
from utils.MLMethods import convert_nested_lists_to_numpy_arrays, fit_transform_PCA, predict_and_score, test_and_get_n_for_PCA
from utils.transformDataDimensions import getAvgLength, truncateRows
from utils.utils import getDataset, save_data_to_file

vectorRootPath = os.path.join('data', 'MLvectors')
pca_vector_train_path = os.path.join(vectorRootPath, 'PCA_train_162classes')
pca_vector_test_path = os.path.join(vectorRootPath, 'PCA_test_162classes')
inputsRootPath = os.path.join('data', 'MLinputs')
pca_inputs_train_path= os.path.join(inputsRootPath, 'PCA_train_162classes')
pca_inputs_test_path= os.path.join(inputsRootPath, 'PCA_test_162classes')


################################## PCA on full dataset (Granular IDs) #############################
#####                                 Flatten 3D vectors to 2D                                #####
"""
vectorALLPath = os.path.join('data', 'DLvectors', 'ALL_vectors_granular')
data = getDataset(vectorALLPath, False)
avg = getAvgLength(data[0])
assert len(data[0][0][0]) == 30, f'inputs data vector length {len(data[0][0][0])} != vector size 30'
VECTOR_SIZE = len(data[0][0][0])
print(f'Avg: {avg}\nVector length: {VECTOR_SIZE}')

# To flatten dataset, we must average out the dimension of the dataset which contains the tokens
print("Averaging out token row length on full data set...")
dataPath = os.path.join(vectorALLPath, 'ALL_vectors.pkl')
data = truncateRows(dataPath, avg, VECTOR_SIZE)

# Reshape from 3D to 2D
input_shape_1 = avg * VECTOR_SIZE
x = convert_nested_lists_to_numpy_arrays(data[0], avg, VECTOR_SIZE)
x = np.reshape(x, (x.shape[0], input_shape_1))
print(f'New vector size will be: {avg}x{VECTOR_SIZE}={str(input_shape_1)}')
data[0] = x
vectorALLPath = os.path.join('data', 'MLvectors', 'ALL_averaged_vectors_granular')
save_data_to_file(vectorALLPath, 'ALL_vectors.pkl', data)

# Get best n for PCA transformation 
best_n = test_and_get_n_for_PCA(x, sum_of_variance_threshold=0.92)

# Save PCA transformed data
data[0] = fit_transform_PCA(x, best_n)
vectorALLPath = os.path.join('data', 'MLvectors', 'ALL_vectors_granular_PCA')
save_data_to_file(vectorALLPath, 'ALL_vectors.pkl', data)


#####                             Continue to train and test setup                           #####
RANDOMSEED = 1099
CLASS_TYPE = 'Granular_PCA_162'
VECTOR_TRANSFORMER='w2v'

dvt = DetectVulType(build_model=SVC, randomSeed=RANDOMSEED,
                    vectorTrainPath=pca_vector_train_path, vectorTestPath=pca_vector_test_path,
                    inputsTrainPath=pca_inputs_train_path, inputsTestPath=pca_inputs_test_path)
dvt.avg = 1
dvt.vector_size = 3178  # best_n
dvt.splitTrainTest(vectorALLPath)
dvt.encodeLabels()
dvt.saveKeyData(dvt.vectorTrainPath, dvt.inputsTrainPath)
dvt.saveKeyData(dvt.vectorTestPath, dvt.inputsTestPath)
"""
RANDOMSEED = 1099
CLASS_TYPE = 'Granular_PCA_162'
VECTOR_TRANSFORMER='w2v'

##### Get train/test sets to pass to ML model
data = getDataset(pca_vector_train_path, RANDOMSEED=RANDOMSEED)
x_train = data[0]
y_train = data[-2]
assert len(x_train[0]) == len(x_train[-1]), f'vector lengths dont match in x_train. {len(x_train[0])} != {len(x_train[-1])}'

data = getDataset(pca_vector_test_path, RANDOMSEED=RANDOMSEED)
x_test = data[0]
y_test = data[-2]
assert len(x_test[0]) == len(x_test[-1]), f'vector lengths dont match in x_test. {len(x_test[0])} != {len(x_test[-1])}'

del data
gc.collect()

print(f'Length of x_train: {len(x_train)}\nLength of columns: {len(x_train[0])}')
print(f'\nLength of x_test: {len(x_test)}\nLength of columns: {len(x_test[0])}')
print(f'y_test[:3]:\n{y_test[:3]}\nNumber of labels: {len(y_test)}')
assert len(x_train[0]) == len(x_test[0]), f'Feature length of x_train {len(x_train[0])} should match that of x_test {len(x_test[0])}'


##### Support Vector Classifier
print('\nCommencing SVC')
MODEL_TYPE = 'svc'
metrics_path = os.path.join(f'{VECTOR_TRANSFORMER}Model', 'metrics', MODEL_TYPE)
model_name = '%s_%s_seed=%s_%s' % (MODEL_TYPE.upper(), VECTOR_TRANSFORMER.upper(), RANDOMSEED, CLASS_TYPE)
svc = SVC(class_weight='balanced', C=5, gamma=6).fit(x_train, y_train)
y_pred = predict_and_score(svc, 'Support Vector Classifier', x_test, y_test, 'test')

# Convert back to original labels - only needed if we used hot-encoded labels from MLinputs path.
# decoded_y_test = dvt.labelEncoder.inverse_transform(y_test)
# decoded_y_pred = dvt.labelEncoder.inverse_transform(y_pred)
# assert decoded_y_pred.shape == decoded_y_test.shape, f'Predicted array shape {decoded_y_pred.shape} should match True label array shape {decoded_y_test.shape}'

# Performance Metrics
getConfusionMatrix_Multiclass(y_pred, y_test, path=metrics_path, modelName=model_name)



"""
#### K-Nearest Neighbors
print('Commencing KNN')
MODEL_TYPE = 'knn'
metrics_path = os.path.join(f'{VECTOR_TRANSFORMER}Model', 'metrics', MODEL_TYPE)
model_name = '%s_%s_seed=%s_%s' % (MODEL_TYPE.upper(), VECTOR_TRANSFORMER.upper(), RANDOMSEED, CLASS_TYPE)
knn = KNeighborsClassifier(n_neighbors=4, 
                            weights='distance', 
                            p=2,
                            n_jobs=-1).fit(x_train, y_train)
y_pred = predict_and_score(knn, 'KNN', x_test, y_test, 'test')

# Convert back to original labels
decoded_y_test = dvt.labelEncoder.inverse_transform(y_test)
decoded_y_pred = dvt.labelEncoder.inverse_transform(y_pred)
assert decoded_y_pred.shape == decoded_y_test.shape, f'Predicted array shape {decoded_y_pred.shape} should match True label array shape {decoded_y_test.shape}'

# Performance Metrics
getConfusionMatrix_Multiclass(decoded_y_pred, decoded_y_test, saveFig=True, path=metrics_path, modelName=model_name)




##### Random Forest Classifier
print('\nCommencing RF')
MODEL_TYPE = 'rfc'
metrics_path = os.path.join(f'{VECTOR_TRANSFORMER}Model', 'metrics', MODEL_TYPE)
model_name = '%s_%s_seed=%s_%s' % (MODEL_TYPE.upper(), VECTOR_TRANSFORMER.upper(), RANDOMSEED, CLASS_TYPE)
rfc = RandomForestClassifier(max_features=None, n_estimators=300, criterion='entropy', 
                            max_depth=15, max_samples=0.8, min_samples_leaf=2, 
                            min_samples_split=3, n_jobs=-1, random_state=RANDOMSEED,
                            class_weight='balanced').fit(x_train, y_train)
y_pred = predict_and_score(rfc, 'Random Forest Classifier', x_test, y_test, 'test')
print(f'y_pred[:10]: ', y_pred[:10])

# Convert back to original labels
decoded_y_test = dvt.labelEncoder.inverse_transform(y_test)
decoded_y_pred = dvt.labelEncoder.inverse_transform(y_pred)
assert decoded_y_pred.shape == decoded_y_test.shape, f'Predicted array shape {decoded_y_pred.shape} should match True label array shape {decoded_y_test.shape}'

# Performance Metrics
getConfusionMatrix_Multiclass(decoded_y_pred, decoded_y_test, path=metrics_path, modelName=model_name)

"""





