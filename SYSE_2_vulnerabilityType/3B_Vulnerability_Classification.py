#!/usr/bin/env python
# coding: utf-8

# # Vulnerability Type Classification using DL
# Uses BGRU & BLSTM models to classify vulnerability type

# ### Setting variables
import os, sys
vType = "ALL"
randomSeed = 1099
numSamples = 50 #Max Num of slice samples from each file
vectorDim = 30 #num of vector cols
slicePath = './data/slicesSource/'
tokenPath = './data/token/SARD/'
w2vmodelPath = './w2vModel/model/w2vModel_ALL'
vectorPath =  './data/vector/'
groupLabelsPath = './data/CVE/SARD_CVE_to_groups.csv'
# Training & testing split, where training is balanced & testing is not
vectorTrainPath = './data/DLvectors/train/'
vectorTestPath = './data/DLvectors/test/'
# Balanced Train & Test sets with same length vectors for DL models
dlInputsTrainPath = './data/DLinputs/train/'
dlInputsTestPath  = './data/DLinputs/test/'

VUL_PATH = os.environ.get('VUL_PATH')
sys.path.insert(1, VUL_PATH)


# ### A. slicesToTokens.py
# Tokens are all the words in a slice mapped to predefined tokens. Length of tokens varies by program (aka slice). Each test case ID will get its own pkl file (in the SARD directory) that contains the tokenized influential words in index 0, its label in index 2, etc.

from SYSE_1_isVulnerable.slicesToTokens import tokenizeSlices
# mycase_ID = tokenizeSlices(slicePath, tokenPath, numSamples)


# ### B.  isDuplicatedID.py
from SYSE_1_isVulnerable.isDuplicatedID import isDuplicatedID
# print("The dataset has duplicated ID: ", isDuplicatedID(mycase_ID))


# ### C. tokensToVectors.py
from SYSE_1_isVulnerable.tokensToVectors import createW2VModel, fitW2VModel
# myW2Vmodel = createW2VModel(w2vmodelPath, tokenPath, vectorDim)
# fitW2VModel(w2vmodelPath, tokenPath, vectorPath)

# ### D. splitTrainTest.py
# Randomly shuffles vector pkl files to choose from. Combines all the [token vectors, labels, functions, filenames, vul' types, test case ids of slices] (one array w/6 nested arrays of those entries ^), & creates 2 files with this (training/testing).
from SYSE_1_isVulnerable.splitTrainTest import splitTrainTest
# splitTrainTest(vType, vectorPath, vectorTrainPath, vectorTestPath,randomSeed, split = 0.8 )


# ### E. downSampling.py
from SYSE_1_isVulnerable.downSampling import appendCaseIDLabel0, downsampling, isClassBalanced
caseID_one,caseID_zero,downsampleNum = appendCaseIDLabel0(vectorTrainPath)
downsampling (caseID_one,caseID_zero, downsampleNum , randomSeed, vectorPath, vectorTrainPath)
#Optional used to check if the class label are balanced 
print("Class Label is balanced: " , isClassBalanced(vectorTrainPath))


# #### Input: 	
# - Train set in train.pkl
# 
# #### Output:   
# - Balanced Train set in balancedClassTrain.pkl, Saved in ./data/DLvectors/train/ 

# ### F. adjustVectorLen.py
# Ensure each program (aka slice) contains the same amount of rows (i.e. tokens) in the data matrix.


from SYSE_1_isVulnerable.adjustVectorLen import meanLen, tranformVectorLen
avg = meanLen(vectorTrainPath)
tranformVectorLen(vectorTrainPath, vectorTestPath, dlInputsTrainPath, dlInputsTestPath, avg, vectorDim, vType)
print("New Vector Length (rows x cols): " ,avg, " x " ,vectorDim)


# #### Input: 	
# - Balanced Train & Test sets in ./data/DLvectors/
# 
# #### Output:   
# - Balanced Train & Test sets with same length vectors in ./data/DLInputs/DL_Final_balanced*.pkl

# ### G. saveKeyData.py
# Used to fit BGRU model.

# In[8]:


from SYSE_1_isVulnerable.saveKeyData import saveKeyData2
saveKeyData2(dlInputsTrainPath, groupLabelsPath)
saveKeyData2(dlInputsTestPath, groupLabelsPath)


# #### Input: 	
# - Balanced Final Train & Test Final sets in ./data/DLvectors/
# -  Set of SARD & CVE IDs that map to a group id in ./data/CVE/SARD_CVE_to_groups.csv
# 
# #### Output:   
# - ../data/DLinputs/train/KeyData_DL_Final_balancedClassTrain.pkl.txt
# - ../data/DLinputs/test/KeyData_DL_Final_ALL_test.pkl.txt
# 
# 
# Format:
# 
# caseID, realLabel, vType
# 
# 135,284,PTR
# 
# 8322,664,API
# 
# ...

# ### H. DLModel.py

# In[9]:


from SYSE_1_isVulnerable.DLModel import *
myoptimizer = 'adam' #can be changed to ‘adamax’
maxlen = avg #avg calculated from part 5.6
layers = 2
dropout = 0.2 
batchSize = 32
vectorDim = 30


# #### Input: 	
# - final train set 
# 
# #### Output:   
# - fitted BGRU model saved in './model/BRGU_ALL'

# ### Network Architechture

# ![Screen%20Shot%202020-05-08%20at%204.07.21%20PM.png](attachment:Screen%20Shot%202020-05-08%20at%204.07.21%20PM.png)

# ### Part A: BGRU

# In[10]:


#Build BGRU Model with parameters 
myKerasModel =  buildBGRU(maxlen, vectorDim, layers, dropout,myoptimizer )

#Fit BGRU Model with trained data and saved the model for later use
weightpath = './model/BRGU_ALL' + myoptimizer +str(randomSeed)
mymodel = fitModel(myKerasModel, weightpath, dlInputsTrainPath, batchSize, maxlen, vectorDim, randomSeed)


# ### Part B: BLSTM

# In[11]:


#Build BLSTMModel with parameters 
myKerasModel =  buildBLSTM(maxlen, vectorDim, layers, dropout,myoptimizer )

#Fit BLSTM Model with trained data and saved the model for later use
weightpath = '../model/BLSTM_ALL' + myoptimizer +str(randomSeed)
mymodel = fitModel(myKerasModel, weightpath, dlInputsTrainPath, batchSize, maxlen, vectorDim, randomSeed)


# ### I. DLPrediction.py

# In[12]:


#all parameters are same as section 5.8
from DLModel import *
from DLPrediction import *
myoptimizer = 'adam'
maxlen = avg
layers = 2
dropout = 0.2 
batchSize = 32


# #### Input: 	
# - final test set and saved model
# 
# #### Output:   
# - output values and predicted values from Model saved to excel file: OutputSummary_adamRandomseed.xlsx

# ### Part A: BGRU

# In[13]:


modelName = 'BGRU'
weightpath = '../model/BRGU_ALL' + myoptimizer +str(randomSeed)
myKerasModelADAM =  buildBGRU(maxlen,vectorDim, layers, dropout,myoptimizer )
myKerasModelADAM.load_weights(weightpath)
testID_label, output_dl_labels, mypredicted_labels, myreallabels, myvtypelabels  = predictLabel(myKerasModelADAM, dlInputsTestPath, maxlen, vectorDim, myoptimizer, modelName, randomSeed)


# ### Part B: BLSTM

# In[14]:


modelName = 'BLSTM'
weightpath = '../model/BLSTM_ALL' + myoptimizer +str(randomSeed)
myKerasModelADAM2 =  buildBLSTM(maxlen,vectorDim,layers,dropout,myoptimizer )
myKerasModelADAM2.load_weights(weightpath)
testID_label2, output_dl_labels2, mypredicted_labels2, myreallabels2, myvtypelabels  = predictLabel(myKerasModelADAM2, dlInputsTestPath, maxlen, vectorDim, myoptimizer, modelName, randomSeed)


# ## J. ConfusionMatrix.py

# #### Input: 	
# - predicted label and real label from part I
# 
# #### Output:   
# - confusion matrix 

# In[15]:


from ConfusionMatrix import *
getConfusionMatrix(mypredicted_labels, myreallabels)#BGRU
getConfusionMatrix(mypredicted_labels2, myreallabels2)#BLSTM


# ---

# --- 

# # Output Analysis

# In[16]:


import numpy as np
import pandas as pd
import os

fileName = "OutputSummary_adamBGRU1099.xlsx"
DLdata = pd.read_excel(fileName)
DLdata.head(10)


# In[17]:


import seaborn as sns

sns.set(rc={'figure.figsize':(11.7,8.27)})
ax = sns.boxplot(x="Vtype", y='DLOutput', hue='RealLabel', data=DLdata, linewidth=1.5, palette="Set3")


# ## K. evaluateModels.py

# In[18]:


from evaluateModels import *
thresdArray = [0.40, 0.45,0.5, 0.53, 0.55, 0.58, 0.60, 0.65, 0.70, 0.8]
mydata, recall, precision, specificity, F1, Accuracy, balanceAccuracy  = combinedPredictions(thresdArray, DLdata)
mydata.to_excel("predictionWithDiffThreshold_gruadam.xlsx")  
mydata.head(10)


# ## L. plotCounts.py

# In[19]:


from plotCounts import *
colName = "Metric0.5"
mydata = mydata
plotHistogram(mydata, colName)


# In[20]:


plotBar(mydata, colName)


# In[21]:


myTable = generateMetricTabel(thresdArray, recall, precision, specificity, F1, Accuracy, balanceAccuracy)
print(myTable)


# ## Appendix A : Plot Recall VS Precision 

# In[22]:


import matplotlib.pyplot as plt
plt.scatter(precision, recall, color='blue')
plt.plot(precision, recall, color='blue')
plt.xlabel('precision')
plt.ylabel('recall')
plt.title('Recall VS Precision')
plt.legend()


# ## Appendix B : Plot F1, balancedAccuracy with Different Threshold

# In[23]:


import matplotlib.pyplot as plt
plt.scatter(thresdArray, F1, color='orange')
plt.plot(thresdArray, F1, color='orange', label="F1")
plt.scatter(thresdArray,balanceAccuracy, color='purple')
plt.plot(thresdArray, balanceAccuracy, color='purple', label="balanceAccuracy")
plt.xlabel('Threshold')
plt.ylabel('Rate')
plt.title('Metrics with Different Threshold')
plt.legend()


# ## Appendix C : Plot Accuracy, balancedAccuracy with Different Thresholds

# In[24]:


plt.scatter(thresdArray,balanceAccuracy, color='purple')
plt.plot(thresdArray, balanceAccuracy, color='purple', label="balanceAccuracy")
plt.scatter(thresdArray,Accuracy, color='pink')
plt.plot(thresdArray, Accuracy, color='pink', label="Accuracy")
plt.xlabel('Threshold')
plt.ylabel('Rate')
plt.title('Metrics with Different Threshold')
plt.legend()


# ## Appendix D : Plot Accuracy, Specificity with Different Thresholds

# In[25]:


plt.scatter(thresdArray,specificity, color='blue')
plt.plot(thresdArray, specificity, color='blue', label="specificity")
plt.scatter(thresdArray,Accuracy, color='pink')
plt.plot(thresdArray, Accuracy, color='pink', label="Accuracy")
plt.xlabel('Threshold')
plt.ylabel('Rate')
plt.title('Metrics with Different Threshold')
plt.legend()

