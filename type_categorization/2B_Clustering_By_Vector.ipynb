{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96125366",
   "metadata": {},
   "source": [
    "# Method 2:  Clustering by Vector\n",
    "The goal remains to group weaknesses/vulnerabilities by similarity.  In method 1, group definitions were extracted from the [CWE website's](https://cwe.mitre.org/index.html), which contained a predefined hierarchical graph for CWE IDs.  Then, the tree was traversed to return a subset that contained the CWE IDs found in the original dataset (from the source code slices) and their overarching primary group listing.  This produced 15 groups that varied in size from 52 to 274,281.  For example, the source code contained 52 slices of code containing a potential CWE-398 weakness.  ['Code Quality' (CWE-398)](https://cwe.mitre.org/data/definitions/700.html) is a top-level category according to the CWE website.  No other CWE-IDs were obtained from our slices that fell under the CWE-398's branch, and therefore Group #14 is named 'Code Quality' and has a size of 52.  On the other hand, 'Improper Control of a Resource Through its Lifetime' (CWE-664) contained a majority of CWE-IDs (75 out of the 166 unique CWE-IDs in the source code slices) and sample counts, totaling 274,281 samples, which is approximately 66.3% of all the samples/groups.\n",
    "\n",
    "In this method, an attempt is made to group source code slices by converting them into vectors & clustering the vectors with a similarity distance.\n",
    "\n",
    "First, a preliminary cluster analysis will be made using the original source code & files to vectorize them.\n",
    "\n",
    "Second, a cluster analysis will be made using a slightly modified source code which will include slices that have a groupid assigned to them.  This is because out of the 420,627 slices, 1,369 slices contained deprecated or obsolete CWE-IDs and 5,286 slices obtained by CVEs did not have a CWE-ID associated with them.  Thus, a maximum of 413,972 slices can be used in the training & testing samples in order to compare the accuracy results to the preliminary cluster analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0714737",
   "metadata": {},
   "source": [
    "# 1. Preliminary Cluster Analysis\n",
    "Using the original source code & files to vectorize the source code for the clustering models.\n",
    "\n",
    "### Setting variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "352d0207",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "vType = \"ALL\"\n",
    "randomSeed = 1099\n",
    "numSamples = 250 #Max Num of slice samples from each file\n",
    "vectorDim = 30 #num of vector cols\n",
    "slicePath = '../data/slicesSource/'\n",
    "tokenPath = '../data/token/SARD/'\n",
    "w2vmodelPath = '../w2vModel/model/w2vModel_ALL'\n",
    "vectorPath =  '../data/vector/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3504199b",
   "metadata": {},
   "source": [
    "### Updating path\n",
    "Must insert path to directory above in order to access files in main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "deb0e015",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(1, '/Users/crystalcontreras/Desktop/DePaul/VulnerabilitiesResearch/vulnerability_detection')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb8d438",
   "metadata": {},
   "source": [
    "### A. slicesToTokens.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef1b9b66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slice Files To be Processed:  Arithmetic expression.txt\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/token/SARD/'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-2b4db3dfae00>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mslicesToTokens\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtokenizeSlices\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmycase_ID\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizeSlices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslicePath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenPath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumSamples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Desktop/DePaul/VulnerabilitiesResearch/vulnerability_detection/main/slicesToTokens.py\u001b[0m in \u001b[0;36mtokenizeSlices\u001b[0;34m(slicepath, corpuspath, totalSamples)\u001b[0m\n\u001b[1;32m     56\u001b[0m                 \u001b[0;32mdel\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0mtestcase_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mtestcase_id\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpuspath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m                 \u001b[0mtestcase_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestcase_id\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1000000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/token/SARD/'"
     ]
    }
   ],
   "source": [
    "from main.slicesToTokens import tokenizeSlices\n",
    "mycase_ID = tokenizeSlices(slicePath, tokenPath, numSamples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eedf5dde",
   "metadata": {},
   "source": [
    "## 2. Pre-Defined Cluster Analysis\n",
    "Using a modified token/vector which includes a group id to calculate the accuracy of the groupings.\n",
    "\n",
    "\n",
    "### A. slicesToTokens.py\n",
    "Which slices we obtain for the tokens is important.   In order to avoid choosing slices that contained an obsolete, deprecated, or no CWE-ID, we either need to \n",
    "1. create new source code files containing a subset of slices with valid CWE-IDs associated with them (ones that aren't obsolete or missing); or \n",
    "2. create a modified version of the slicesToTokens.py file that will return slices with an SARD or CVE-ID that have valid CWE's associated with them.\n",
    "\n",
    "Additionally, I would like to include the group ID obtained from the 2A_Clustering_By_Asbtraction.ipynb file in the slice so I can measure the accuracy of the clustering model and classification predictions later on.\n",
    "\n",
    "<br />\n",
    "\n",
    "Option 1 requires a function that will match the SARD ID to the SARD ID in the *CWE_IDs.txt* file to retrieve the associated CWE-ID, the CVE ID to the *CVE_DF.csv* file to retrieve the associated CWE-ID (or 'NaN' if none), filter out any results that contain an obsolete, deprecated, or NaN ID, and output results to new files.\n",
    "\n",
    "**Pros:** Cleaner; It may make the subsequent code run faster since we won't have to do the additional filter steps of option 2; Would allow me to integrate the Group ID into the slice.\n",
    "\n",
    "**Cons:** Takes up more storage space; May be redundant since it's technically a subset of the original files; \n",
    "\n",
    "<br />\n",
    "\n",
    "Option 2 requires the same as option 1, except that function will need to be done at runtime, and return the results instead of outputting results to a file.\n",
    "\n",
    "**Pros:** Requires significantly less storage space than option 1.\n",
    "\n",
    "**Cons:** May contain duplicate code or would require the modification of the original file; Would make the slices-to-tokens conversion slower, which may not be too big of a problem if we only need to run it once. \n",
    "\n",
    "<br />\n",
    "\n",
    "It is hard to quantify whether removing the 6,655 slices to make the code run faster at the cost of taking up more space would be better than making the adjustments during runtime.  Also, I still need to evaluate how easy or hard it would be to integrate the Group ID into the slice or token if doing it at runtime.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95695d56",
   "metadata": {},
   "source": [
    "### Create a modified version of the slicesToTokens.py file that will return slices with an SARD or CVE-ID that have valid CWE's associated with them.\n",
    "#### Create a function that will: \n",
    "- match the SARD ID to the SARD ID in the CWE_IDs.txt file to retrieve the associated CWE-ID, \n",
    "- match the CVE ID to the CVE_DF.csv file to retrieve the associated CWE-ID (or 'NaN' if none), \n",
    "- filter out any results that contain an obsolete, deprecated, or NaN ID, and \n",
    "- output results.\n",
    "\n",
    "<br />\n",
    "<p style='color: red'>TO DO: maybe we don't have to filter them at the token level since the SARD/CVE IDs are included in the filename.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84602420",
   "metadata": {},
   "outputs": [],
   "source": [
    "from slicesToTokens2 import *\n",
    "mycase_ID = tokenizeSlices2(slicePath, tokenPath, numSamples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952a6e79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d789b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33bcb21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a442c624",
   "metadata": {},
   "source": [
    "#### Include the group ID obtained from the 2A_Clustering_By_Asbtraction.ipynb file in the slice so I can measure the accuracy of the clustering model and classification predictions later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b31ee7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c62085",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
