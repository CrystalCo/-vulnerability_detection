import os
import requests

"""
    Collects SARD test case IDs from the titles, gets the CWEs from the SARD’s website, & then gets the remaining attributes from the CWE website.

    PHASE 1: Collect test case IDs
        1)	Extract SARD IDs & CVE IDs from source code slice titles.
    PHASE 2: Scrape Internet for attributes
        2)	Extract CWE IDs from the SARD website.
        3)	Extract 'Name', ‘Description’, 'Abstraction', & ‘Relationship’ sections from the CWE website.
        4)  Extract CWE IDs for CVEs from multiple websites.
    PHASE 3: Format data
        5)	Use pandas to merge CVE IDs and attributes
        6)	Create dummy variables for categorical attributes
    PHASE 4: Organize data
        7)	Save the data collected in a data structure.
"""
def _get_IDs_from_datafile(dataFile):
    data = open(dataFile, 'r')
    ids = [] 
    for line in data:
        ids.append(line.rstrip('\n'))
    data.close()
    # Remove header
    ids.pop(0)
    return ids

def get_SARD_CVE_IDs(slicepath, totalSamples=None):
    """
        Extract SARD test case IDs (for CWEs) & CVE IDs from source code slice titles.
        INPUT:
            slicepath := the path where the slice source files exist. Expects a str.
            totalSamples := for testing on a small sample of slices. If None, all slices will be used. Expects int if not None.
        OUTPUT:
            Array of SARD test case IDs as strings.
            Array of CVE IDs as strings.
    """
    CVE_IDs = set()
    SARD_IDs = set()
    CVE_IDs_count = 0
    SARD_IDs_count = 0

    for filename in os.listdir(slicepath):
        SARD_IDs_by_file = set()
        CVE_IDs_by_file = set()
        if (filename.endswith(".txt") is False):
            continue
        filepath = os.path.join(slicepath, filename)
        f1 = open(filepath)
        slicelists = f1.read().split("------------------------------")
        f1.close()

        if slicelists[0] == '':
            del slicelists[0]
        if slicelists[-1] == '' or slicelists[-1] == '\n' or slicelists[-1] == '\r\n':
            del slicelists[-1]

        if (totalSamples is not None):
            # limit number of slices scanned
            slicelists = slicelists[:totalSamples]

        sard_local_count = 0   # number of SARD test case IDs found in the file
        cve_local_count = 0    # If missing SARD test case IDs, must be a CVE case
        total_slices_count = 0 
        for slicelist in slicelists:
            sentences = slicelist.split('\n')
            if sentences[0] == '\r' or sentences[0] == '':
                # Remove newlines that appear at the beginning of a title
                del sentences[0]
            if sentences == []:
                continue
            if sentences[-1] == '':
                del sentences[-1]
            if sentences[-1] == '\r':
                del sentences[-1]

            case_ID = sentences[0].split(" ")
            case_ID = case_ID[1].split("/")[0]

            if case_ID.isdigit():
                # Must be SARD ID
                SARD_IDs_by_file.add(case_ID)
                SARD_IDs.add(case_ID)
                sard_local_count += 1
            else:
                # Must be CVE
                CVE_IDs_by_file.add(case_ID)
                CVE_IDs.add(case_ID)
                cve_local_count += 1
            total_slices_count += 1
            # TODO: Is it possible for mult' SARD IDs to be in the title?
            # TODO: Is it possible for diff' CVE IDs to be in the title?
        CVE_IDs_count += cve_local_count
        SARD_IDs_count += sard_local_count

        print(f'Filename: {filename}')
        if (totalSamples):
            print(f'SARD test case IDs by file: {SARD_IDs_by_file}')
            print(f'CVE test case IDs by file: {CVE_IDs_by_file}')
        
        print(f'SARD test case IDs count by file: {sard_local_count}')
        print(f'CVE test case IDs count by file: {cve_local_count}')
        print(f'Total slices extracted: {total_slices_count} \n')
    
    print('Total SARD test case IDs found in files: %s' % SARD_IDs_count)
    print('Total CVE test case IDs found in files: %s' % CVE_IDs_count)
    total = SARD_IDs_count + CVE_IDs_count
    print('Total slices found: %d' % total)
    return CVE_IDs, SARD_IDs

def get_CWEs_from_SARD(SARD_IDs, scrapeOut, dataOut):
    """
        Extract CWE IDs from the SARD website.
        INPUT:
            SARD_IDS := Expects an array of strings with the SARD test case IDs we want to extract from the SARD site.
            scrapeOut := Expects a string path to which we can write the data from the SARD site.
            dataOut := Expects TextIOWrapper (i.e. a file we can write out the results to).
        OUTPUT:
            Writes results to the dataOut file.
    """
    for SARD_ID in SARD_IDs:
        url = 'https://samate.nist.gov/SARD/view_testcase.php?tID=' + SARD_ID
        try:
            page = requests.get(url)
        except Exception as e:
            print(f"Exception with url {url}: {e}")
            continue

        with open(scrapeOut, 'w') as f:
            for line in page.text:
                try:
                    f.write(line)
                except Exception as e:
                    # ignore except error for unidentified characters in html code
                    print(f"Exception with writing to scrapeOut file: {e}")
                    pass

        with open(scrapeOut, 'r') as f:
            cwe_id = _parse_SARD_page(f)
            dataOut.write(cwe_id + '\n')
            
def get_CWE_data(CWE_IDs, scrapeOut, dataOut):
    """
        Extract 'Name', ‘Description’, 'Abstraction', & ‘Relationship’ sections from the CWE website.
        INPUT:
            CWE_IDs := Expects an array of strings with the CWE ID we want from the CWE site.
            scrapeOut := Expects a string path to which we can write the data from the CWE site.
            dataOut := Expects TextIOWrapper (i.e. a file we can write out the results to).
        OUTPUT:
            Writes results to the dataOut file.
    """
    for CWE_ID in CWE_IDs:
        cid = CWE_ID.split('-')[1]
        cid = int(cid) # Strips 0s from the beginning of the CWE id
        url = 'https://cwe.mitre.org/data/definitions/%d.html' % cid
        page = requests.get(url)

        with open(scrapeOut, 'w') as f:
            for line in page.text:
                try:
                    f.write(line)
                except Exception as e:
                    # ignore except error for unidentified characters in html code
                    print(f"Exception with writing to scrapeOut file: {e}")
                    pass
        
        with open(scrapeOut, 'r') as f:
            # [name, abstraction, description, relationships] 
            CWE_Data = _parse_CWE_page(f, cid)
            
            # ['CWE_ID', 'Name', 'Abstraction', 'Description', 'Relationships', 'URL']
            out = '\t'.join([cid, CWE_Data[0], CWE_Data[1], CWE_Data[2], ','.join(CWE_Data[3]), url])
            dataOut.write(out + '\n')

def get_CVE_data(CVE_IDs, scrapeOut, dataOut):
    """
        Extract CWE IDs for CVEs from multiple websites.

        INPUT:
            CVE_IDs := Expects an array of strings of the CVE IDs.
            scrapeOut := Expects a string path to which we can write the data from the NIST & CVEDetails sites.
            dataOut := Expects TextIOWrapper (i.e. a file we can write out the results to).
        OUTPUT:
            Writes results to the dataOut file.
    """
    for CVE_ID in CVE_IDs:
        url1 = 'https://nvd.nist.gov/vuln/detail/%s' % CVE_ID
        url2 = 'https://www.cvedetails.com/cve/%s' % CVE_ID

        page = requests.get(url1)
        # TODO: See if it works here. If so, replace other instances
        set_scrape_out(scrapeOut, page)

        with open(scrapeOut, 'r') as f:
            CVE_Data1 = _parse_NIST_page(f)
            # get cve description and CWE
            # 'CVE_ID', 'URL1', 'Description1', 'CWE1', 'URL2', 'Description2', 'CWE2'
            out = '\t'.join([CVE_ID, url1, CVE_Data1[0], CVE_Data1[1]])
            dataOut.write(out + '\n')

def _parse_NIST_page(dataFile):
    description_delim = 'vuln-description">'
    cwe_id_delim = '<td data-testid="vuln-CWEs-link-0">'
    span_delim = '<span>'
    nextLine = False

    # 'Description1', 'CWE1'
    CVE_Data = []

    for line in dataFile:
        if description_delim in line:
            description_start = line.split(description_delim)[1]
            description = description_start.split('</p')[0]
            CVE_Data.append(description)
        if cwe_id_delim in line:
            nextLine = True
            continue
        if nextLine and span_delim in line:
            cwe_start = line.split(span_delim)[0]
            cwe = cwe_start.split('</span')[0]
            CVE_Data.append(cwe)
            nextLine = False
    
    return CVE_Data

def _parse_SARD_page(dataFile):
    flaw_delim = '<a target="_blank" href="https://cwe.mitre.org/data/definitions/'
    next_line = False

    for line in dataFile:
        if line.find(flaw_delim) != -1:
            next_line = True
            continue
        if next_line and 'CWE' in line:
            cwe_start = line.index('CWE')
            cwe_end = line.index(': ')
            cwe = line[cwe_start:cwe_end]
            return cwe   # CWE ID    ex. CWE-476

def _parse_CWE_page(dataFile, CWE_ID):
    name_delim = 'text-bottom">CWE-%s: ' % CWE_ID
    abstraction_delim = 'Abstraction: <span style="font-weight:normal">'
    # description_delim = '<div name="oc_121_Description" id="oc_121_Description" class="expandblock"><div class="detail"><div class="indent">'
    description_delim = '<div name="oc_%s_Description" id="oc_%s_Description" class="expandblock"><div class="detail"><div class="indent">' % (CWE_ID, CWE_ID)
    relationships_delim = 'language, and resource.</span></span></td><td valign="top">'

    # name, abstraction, description, relationship ids
    CWE_Data = ["", "", "", []]

    for line in dataFile:
        if name_delim in line:
            name_start = line.split(name_delim)[1]
            name = name_start.split('</h2')[0]
            CWE_Data[0] = name

            abstraction_start = line.split(abstraction_delim)[1]
            abstraction = abstraction_start.split('</span')[0]
            CWE_Data[1] = abstraction

        if description_delim in line:
            desc_start = line.split(description_delim)[1]
            description = desc_start.split('</div')[0]
            CWE_Data[2] = description

        if relationships_delim in line:
            id_start = line.split(relationships_delim)[1]
            relationship_id = id_start.split('</td')[0]
            CWE_Data[3].append(relationship_id)

    return CWE_Data

def set_scrape_out(scrapeOut, page):
    """
        Writes source lines from a scraped webpage (page) to a designated file (scrapeOut).
    """
    with open(scrapeOut, 'w') as f:
        for line in page.text:
            try:
                f.write(line)
            except Exception as e:
                # ignore except error for unidentified characters in html code
                print(f"Exception with writing to scrapeOut file: {e}")
                pass

def set_scrapeOut_to_dataOut(scrapeOut, dataOut, IDs, header, get_data_method):
    dataOut = open(dataOut, 'w')
    dataOut.write(header)
    dataOut.write('\n')
    get_data_method(IDs, scrapeOut, dataOut)
    dataOut.close()

def _set_dataOut(dataOut, header, data):
    dataOut = open(dataOut, 'w')
    dataOut.write(header + '\n')
    for d in data:
        dataOut.write(d + '\n')
    dataOut.close()

def test__get_SARD_CVE_IDs(slicePath, totalSamples=None):
    # 1. Collect CVE & SARD test case IDs
    CVE_IDs, SARD_IDs = get_SARD_CVE_IDs(slicePath, totalSamples)
    if totalSamples:
        print('Complete set of SARD test case IDs: %s' % SARD_IDs)
        print('Complete set of CVE test case IDs: %s' % CVE_IDs)
    return CVE_IDs, SARD_IDs

def test__get_CWE_IDs_from_SARD(scrapeOut, dataOut, SARD_IDs):
    # 2. Scrape Internet for CWE ids from SARD website
    dataOut = open(dataOut, 'w')
    dataOut.write('CWE_ID')
    dataOut.write('\n')
    get_CWEs_from_SARD(SARD_IDs, scrapeOut, dataOut)
    dataOut.close()

def test__get_CWE_Data(scrapeOut, dataOut, IDs):
    # 2. Extract 'Name', ‘Description’, 'Abstraction', & ‘Relationship’ sections from the CWE website.
    dataOut = open(dataOut, 'w')
    dataOut.write('\t'.join(['CWE_ID', 'Name', 'Abstraction', 'Description', 'Relationships', 'URL']))
    dataOut.write('\n')
    get_CWE_data(IDs, scrapeOut, dataOut)
    dataOut.close()


if __name__ == "__main__":
    # Phase 1. Collect CVE & SARD test case IDs
    slicePath = './data/slicesSource/'
    totalSamples = 100         # for testing
    CVE_IDs, SARD_IDs = test__get_SARD_CVE_IDs(slicePath)

    # Save CVE IDs so we don't have to run the top part every time
    dataOut = './data/CVE/CVE_IDs.txt'
    _set_dataOut(dataOut, 'CVE_ID', CVE_IDs)

    # Phase 2. Scrape Internet for CWE attributes
    # 2)	Extract CWE IDs from the SARD website.
    # scrapeOut = './data/CWE/scrapeOut.txt'
    # dataOut = './data/CWE/CWE_IDs.txt'
    # test__get_CWE_IDs_from_SARD(scrapeOut, dataOut, SARD_IDs)

    # 3)	Extract 'Name', ‘Description’, 'Abstraction', & ‘Relationship’ sections from the CWE website.
    # CWE_IDs = _get_IDs_from_datafile(dataOut)
    # sample_CWE_IDs = CWE_IDs[:totalSamples]   # for testing
    # dataOut = './data/CWE/CWE_Data.txt'
    # test__get_CWE_Data(scrapeOut, dataOut, CWE_IDs)

    # 4)  Extract CWE IDs for CVEs from multiple websites.
    CVE_samples = _get_IDs_from_datafile('./data/CVE/CVE_IDs.txt')
    scrapeOut = './data/CVE/scrapeOut.txt'
    dataOut = './data/CVE/CVE_Data.txt'
    header = '\t'.join(['CVE_ID', 'URL1', 'Description1', 'CWE1', 'URL2', 'Description2', 'CWE2'])
    # TODO: Test this method here.  If it works, apply to other test__ methods
    set_scrapeOut_to_dataOut(scrapeOut, dataOut, CVE_IDs, header, get_CVE_data)
