import os
import requests

"""
    Collects SARD test case IDs from the titles, gets the CWEs from the SARD’s website, & then gets the remaining attributes from the CWE website.

    PHASE 1: Collect test case IDs
        1)	Extract SARD IDs & CVE IDs from source code slice titles.
    PHASE 2: Scrape Internet for attributes
        2)	Extract CWE IDs from the SARD website.
        3)	Extract 'Name', ‘Description’, 'Abstraction', & ‘Relationship’ sections from the CWE website.
        4)  Extract CWE IDs for CVEs from multiple websites.
"""
def _get_IDs_from_datafile(dataFile):
    data = open(dataFile, 'r')
    ids = [] 
    for line in data:
        ids.append(line.rstrip('\n'))
    data.close()
    # Remove header
    ids.pop(0)
    return ids

def get_SARD_CVE_IDs(slicepath, totalSamples=None):
    """
        Extract SARD test case IDs (for CWEs) & CVE IDs from source code slice titles.
        INPUT:
            slicepath := the path where the slice source files exist. Expects a str.
            totalSamples := for testing on a small sample of slices. If None, all slices will be used. Expects int if not None.
        OUTPUT:
            Array of SARD test case IDs as strings.
            Array of CVE IDs as strings.
    """
    CVE_IDs = set()
    SARD_IDs = set()
    CVE_IDs_count = 0
    SARD_IDs_count = 0

    for filename in os.listdir(slicepath):
        SARD_IDs_by_file = set()
        CVE_IDs_by_file = set()
        if (filename.endswith(".txt") is False):
            continue
        filepath = os.path.join(slicepath, filename)
        f1 = open(filepath)
        slicelists = f1.read().split("------------------------------")
        f1.close()

        if slicelists[0] == '':
            del slicelists[0]
        if slicelists[-1] == '' or slicelists[-1] == '\n' or slicelists[-1] == '\r\n':
            del slicelists[-1]

        if (totalSamples is not None):
            # limit number of slices scanned
            slicelists = slicelists[:totalSamples]

        sard_local_count = 0   # number of SARD test case IDs found in the file
        cve_local_count = 0    # If missing SARD test case IDs, must be a CVE case
        total_slices_count = 0 
        for slicelist in slicelists:
            sentences = slicelist.split('\n')
            if sentences[0] == '\r' or sentences[0] == '':
                # Remove newlines that appear at the beginning of a title
                del sentences[0]
            if sentences == []:
                continue
            if sentences[-1] == '':
                del sentences[-1]
            if sentences[-1] == '\r':
                del sentences[-1]

            case_ID = sentences[0].split(" ")
            case_ID = case_ID[1].split("/")[0]

            if case_ID.isdigit():
                # Must be SARD ID
                SARD_IDs_by_file.add(case_ID)
                SARD_IDs.add(case_ID)
                sard_local_count += 1
            else:
                # Must be CVE
                CVE_IDs_by_file.add(case_ID)
                CVE_IDs.add(case_ID)
                cve_local_count += 1
            total_slices_count += 1
            # TODO: Is it possible for mult' SARD IDs to be in the title?
            # TODO: Is it possible for diff' CVE IDs to be in the title?
        CVE_IDs_count += cve_local_count
        SARD_IDs_count += sard_local_count

        print(f'Filename: {filename}')
        if (totalSamples):
            print(f'SARD test case IDs by file: {SARD_IDs_by_file}')
            print(f'CVE test case IDs by file: {CVE_IDs_by_file}')
        
        print(f'SARD test case IDs count by file: {sard_local_count}')
        print(f'CVE test case IDs count by file: {cve_local_count}')
        print(f'Total slices extracted: {total_slices_count} \n')
    
    print('Total SARD test case IDs found in files: %s' % SARD_IDs_count)
    print('Total CVE test case IDs found in files: %s' % CVE_IDs_count)
    total = SARD_IDs_count + CVE_IDs_count
    print('Total slices found: %d' % total)
    return CVE_IDs, SARD_IDs

def get_CWEs_from_SARD(SARD_IDs, scrapeOut, dataOut):
    """
        Extract CWE IDs from the SARD website.
        INPUT:
            SARD_IDS := Expects an array of strings with the SARD test case IDs we want to extract from the SARD site.
            scrapeOut := Expects a string path to which we can write the data from the SARD site.
            dataOut := Expects TextIOWrapper (i.e. a file we can write out the results to).
        OUTPUT:
            Writes results to the dataOut file.
    """
    for SARD_ID in SARD_IDs:
        url = 'https://samate.nist.gov/SARD/view_testcase.php?tID=' + SARD_ID
        try:
            page = requests.get(url)
        except Exception as e:
            print(f"Exception with url {url}: {e}")
            continue

        set_scrape_out(scrapeOut, page)

        with open(scrapeOut, 'r') as f:
            cwe_id = _parse_SARD_page(f)
            dataOut.write(cwe_id + '\n')
            
def get_CWE_data(CWE_IDs, scrapeOut, dataOut):
    """
        Extract 'Name', ‘Description’, 'Abstraction', & ‘Relationship’ sections from the CWE website.
        INPUT:
            CWE_IDs := Expects an array of strings with the CWE ID we want from the CWE site.
            scrapeOut := Expects a string path to which we can write the data from the CWE site.
            dataOut := Expects TextIOWrapper (i.e. a file we can write out the results to).
        OUTPUT:
            Writes results to the dataOut file.
    """
    for CWE_ID in CWE_IDs:
        cid = str(int(CWE_ID)) # Strips 0s from the beginning of the CWE id
        url = 'https://cwe.mitre.org/data/definitions/%s.html' % cid
        page = requests.get(url)

        set_scrape_out(scrapeOut, page)
        
        with open(scrapeOut, 'r') as f:
            # [name, abstraction, description, relationships] 
            CWE_Data = _parse_CWE_page(f, cid)
            
            # ['CWE_ID', 'Name', 'Abstraction', 'Description', 'Relationships', 'URL']
            out = '\t'.join([cid, CWE_Data[0], CWE_Data[1], CWE_Data[2], ','.join(CWE_Data[3]), url])
            dataOut.write(out + '\n')

def get_CVE_data(CVE_IDs, scrapeOut, dataOut):
    """
        Extract CWE IDs for CVEs from multiple websites.

        INPUT:
            CVE_IDs := Expects an array of strings of the CVE IDs.
            scrapeOut := Expects a string path to which we can write the data from the NIST & CVEDetails sites.
            dataOut := Expects TextIOWrapper (i.e. a file we can write out the results to).
        OUTPUT:
            Writes results to the dataOut file.
    """
    for CVE_ID in CVE_IDs:
        url1 = 'https://nvd.nist.gov/vuln/detail/%s' % CVE_ID
        url2 = 'https://www.cvedetails.com/cve/%s' % CVE_ID

        # parsing URL1
        page = requests.get(url1)
        set_scrape_out(scrapeOut, page)
        f = open(scrapeOut, 'r')
        CVE_Data1 = _parse_NIST_page(f)

        # parsing URL2
        page = requests.get(url2)
        set_scrape_out(scrapeOut, page)
        f = open(scrapeOut, 'r')
        CVE_Data2 = _parse_CVEDetail_page(f)

        # 'CVE_ID', 'URL1', 'Description1', 'CWE1', 'URL2', 'Description2', 'CWE2'
        out = '\t'.join([CVE_ID, url1, CVE_Data1[0], CVE_Data1[1], url2, CVE_Data2[0], CVE_Data2[1]])
        dataOut.write(out + '\n')

def _parse_CVEDetail_page(dataFile):
    description_delim = 'cvedetailssummary'
    cwe_id_delim1 = '<th>CWE ID'
    cwe_id_delim2 = 'CWE definition">'
    cwe_id_delim3 = '<td>'
    nextLine1 = False
    nextLine2 = False
    CVE_Data = []

    for line in dataFile:
        if description_delim in line:
            nextLine1 = True
            continue
        if nextLine1:
            description = line.split('\t')[1]
            CVE_Data.append(description)
            nextLine1 = False
            continue
        if cwe_id_delim1 in line:
            nextLine2 = True
            continue
        if nextLine2 and cwe_id_delim2 in line:
            cwe_start = line.split(cwe_id_delim2)[1]
            cwe = cwe_start.split('</a')[0]
            CVE_Data.append(cwe)
            nextLine2 = False
        elif nextLine2 and cwe_id_delim3 in line:
            cwe_start = line.split(cwe_id_delim3)[1]
            cwe = cwe_start.split('</td')[0]
            CVE_Data.append(cwe)
            nextLine2 = False
        
    return CVE_Data

def _parse_NIST_page(dataFile):
    description_delim = 'vuln-description">'
    cwe_id_delim1 = '<td data-testid="vuln-CWEs-link-0">'
    cwe_id_delim2 = 'CWE-'  # if CWE ID exists
    cwe_id_delim3 = '<span>'            # if CWE ID doesn't exist
    nextLine = False

    # 'Description1', 'CWE1'
    CVE_Data = []

    for line in dataFile:
        if description_delim in line:
            description_start = line.split(description_delim)[1]
            description = description_start.split('</p')[0]
            CVE_Data.append(description)
        if cwe_id_delim1 in line:
            nextLine = True
            continue
        if nextLine and cwe_id_delim3 in line:
            cwe_start = line.split(cwe_id_delim3)[1]
            cwe = cwe_start.split('</span')[0]
            CVE_Data.append(cwe)
            nextLine = False
        elif nextLine and cwe_id_delim2 in line:
            cwe_start = line.split(cwe_id_delim2)[1]
            cwe = cwe_start.split('</a')[0]
            CVE_Data.append(cwe)
            nextLine = False
    
    return CVE_Data

def _parse_SARD_page(dataFile):
    cwe_delim = '<a target="_blank" href="https://cwe.mitre.org/data/definitions/'

    for line in dataFile:
        if cwe_delim in line:
            cwe_start = line.split(cwe_delim)[1]
            cwe = cwe_start.split('.html')[0]
            return cwe   # CWE ID    ex. CWE-476

def _parse_CWE_page(dataFile, CWE_ID):
    name_delim = 'text-bottom">CWE-%s: ' % CWE_ID
    abstraction_delim = 'Abstraction: <span style="font-weight:normal">'
    # description_delim = '<div name="oc_121_Description" id="oc_121_Description" class="expandblock"><div class="detail"><div class="indent">'
    description_delim = '<div name="oc_%s_Description" id="oc_%s_Description" class="expandblock"><div class="detail"><div class="indent">' % (CWE_ID, CWE_ID)
    relationships_delim = 'language, and resource.</span></span></td><td valign="top">'

    # name, abstraction, description, relationship ids
    CWE_Data = ["", "", "", []]

    for line in dataFile:
        if name_delim in line:
            name_start = line.split(name_delim)[1]
            name = name_start.split('</h2')[0]
            CWE_Data[0] = name

            abstraction_start = line.split(abstraction_delim)[1]
            abstraction = abstraction_start.split('</span')[0]
            CWE_Data[1] = abstraction

        if description_delim in line:
            desc_start = line.split(description_delim)[1]
            description = desc_start.split('</div')[0]
            CWE_Data[2] = description

        if relationships_delim in line:
            id_start = line.split(relationships_delim)[1]
            relationship_id = id_start.split('</td')[0]
            CWE_Data[3].append(relationship_id)

    return CWE_Data

def set_scrape_out(scrapeOut, page):
    """
        Writes source lines from a scraped webpage (page) to a designated file (scrapeOut).
    """
    with open(scrapeOut, 'w') as f:
        for line in page.text:
            try:
                f.write(line)
            except Exception as e:
                # ignore except error for unidentified characters in html code
                print(f"Exception with writing to scrapeOut file: {e}")
                pass

def set_scrapeOut_to_dataOut(scrapeOut, dataOut, IDs, header, get_data_method):
    """
        Extracts the information we need from a file that contains source code (scrapeOut) & 
        inserts it into a tab dilineated file (dataOut).
        INPUT:
            scrapeOut   := txt file with html scraped from a website
            dataOut     := txt file to output the extracted info wanted from scrapeOut, to be separated by tabs
            IDs         := set of IDs for the method being used (e.g. SARD, CWE, CVE)
            header      := header for the dataOut file
            get_data_method := method that will retrieve the html from a web that'll be transformed into the desired dataOut format.
                               requires arguments: IDs, scrapeOut, & dataOut
        OUTPUT:
            Writes results to the dataOut file.
    """
    dataOut = open(dataOut, 'w')
    dataOut.write(header + '\n')
    get_data_method(IDs, scrapeOut, dataOut)
    dataOut.close()

def _set_dataOut(dataOut, header, data):
    """
        Tool that outputs an array (data) to a txt file (dataOut) split per line.
    """
    dataOut = open(dataOut, 'w')
    dataOut.write(header + '\n')
    for d in data:
        dataOut.write(d + '\n')
    dataOut.close()

def test__get_SARD_CVE_IDs(slicePath, totalSamples=None):
    """
        1. Collect CVE & SARD test case IDs.
        INPUT:
            slicepath := the path where the slice source files exist. Expects a str.
            totalSamples := for testing on a small sample of slices. If None, all slices will be used. Expects int if not None.
        OUTPUT:
            Array of SARD test case IDs as strings.
            Array of CVE IDs as strings.
    """
    CVE_IDs, SARD_IDs = get_SARD_CVE_IDs(slicePath, totalSamples)
    if totalSamples:
        print('Complete set of SARD test case IDs: %s' % SARD_IDs)
        print('Complete set of CVE test case IDs: %s' % CVE_IDs)
        print()
    return CVE_IDs, SARD_IDs

def test__get_CVE_data(isTest, CVE_IDs, totalSamples=None):
    """
        Saves CVE data scraped from the web into a tab delimited format on a txt file.
        Running the entire script with isTest = False will save all the CVE IDs to its own txt file for debugging purposes.
        In case we need to debug only this portion, we can later set isTest to True after having those CVE IDs stored
        in order to speed up the testing.

        INPUT:
            isTest := boolean. False will use CVE_IDs passed in.  True will use a subset of CVE_IDs from CVE_IDs.txt
            CVE_IDs := an array of CVE IDs. Can be empty if isTest = True.
            totalSamples := the amount of CVE IDs we want to use for testing from the subset of CVE IDs in CVE_IDs.txt
        OUTPUT:
            CVE data scraped from the web into a tab delimited format on the dataOut file.
    """
    if isTest and type(totalSamples) != int:
        raise Exception("totalSamples value must be an int > 0 if this is a test.")

    scrapeOut = './data/CVE/scrapeOut.txt'
    dataOut = './data/CVE/CVE_Data.txt'
    header = '\t'.join(['CVE_ID', 'URL1', 'Description1', 'CWE1', 'URL2', 'Description2', 'CWE2'])
    if not isTest:
        set_scrapeOut_to_dataOut(scrapeOut, dataOut, CVE_IDs, header, get_CVE_data)
    else:
        cve_id_dataFile = './data/CVE/CVE_IDs.txt'
        CVE_samples = _get_IDs_from_datafile(cve_id_dataFile)
        if len(CVE_samples) > 0:
            CVE_samples = CVE_samples[:totalSamples]
            set_scrapeOut_to_dataOut(scrapeOut, dataOut, CVE_samples, header, get_CVE_data)
        else:
            raise Exception("No CVE IDs found in %s.  Run this file in non-test mode first to generate CVE IDs, then retry in test mode." % cve_id_dataFile)


if __name__ == "__main__":
    # Phase 1. Collect CVE & SARD test case IDs
    slicePath = './data/slicesSource/'
    isTest = False    # Set to true for testing
    if isTest:
        totalSamples = 2
        CVE_IDs, SARD_IDs = test__get_SARD_CVE_IDs(slicePath, totalSamples)
    else:
        CVE_IDs, SARD_IDs = test__get_SARD_CVE_IDs(slicePath)
        # Save CVE IDs so we don't have to run the top part every time. 
        dataOut = './data/CVE/CVE_IDs.txt'
        _set_dataOut(dataOut, 'CVE_ID', CVE_IDs)

    # Phase 2. Scrape Internet for CWE attributes
    scrapeOut = './data/CWE/scrapeOut.txt'
    dataOut = './data/CWE/CWE_IDs.txt'
    set_scrapeOut_to_dataOut(scrapeOut, dataOut, SARD_IDs, 'CWE_ID', get_CWEs_from_SARD)
    print("Extracted CWE IDs from the SARD website.\n")

    CWE_IDs = _get_IDs_from_datafile(dataOut)
    if isTest:
        CWE_IDs = CWE_IDs[:totalSamples]
    dataOut = './data/CWE/CWE_Data.txt'
    header = '\t'.join(['CWE_ID', 'Name', 'Abstraction', 'Description', 'Relationships', 'URL'])
    set_scrapeOut_to_dataOut(scrapeOut, dataOut, CWE_IDs, header, get_CWE_data)
    print("Extracted 'Name', ‘Description’, 'Abstraction', & ‘Relationship’ sections from the CWE website.\n")

    if isTest:
        test__get_CVE_data(isTest, CVE_IDs, totalSamples)
    else:
        test__get_CVE_data(isTest, CVE_IDs)
    print("Extracted CWE IDs for CVEs from multiple websites.\n")

