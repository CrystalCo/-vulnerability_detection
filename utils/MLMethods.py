import os

import numpy as np
from matplotlib import pyplot as plt
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, f1_score
from sklearn.model_selection import GridSearchCV



def get_grid(model, parameters, scoring, X, Y=None, vb=1, cv=3, return_train_score=True):
    grid = GridSearchCV(estimator=model, param_grid=parameters, scoring=scoring, cv=cv, return_train_score=return_train_score, verbose=vb) # if we get a warning on the server about iid, add parameter `iid=False`
    grid.fit(X,Y)
    print("Best estimator: ", grid.best_estimator_)
    print("Best params: ", grid.best_params_)
    print("Best Score: ", grid.best_score_)
    return grid

def get_metrics_from_logs(logs_path, metric_name):
    model_name_parser = 'BUILDING MODEL '
    metrics_per_epoch_parser = 'step - '
    epochs_parser = 'Epoch 1/'
    models = []
    metric_scores_all = []
    num_epochs = 0

    for filename in os.listdir(logs_path):
        f = open(os.path.join(logs_path, filename))
        model_name = ''
        metric_scores = []
        
        for line in f:
            if model_name_parser in line:
                # parse model name and stats (e.g. BGRU_W2V_batch=64+seed=1099_Group)
                line = line.rstrip('\n')
                model_name = line.split(model_name_parser)[1]
            if epochs_parser in line:
                line = line.rstrip('\n')
                num_epochs = int(line.split(epochs_parser)[1]) 
            if metrics_per_epoch_parser in line:
                # parse metrics from "step - loss" to get loss, categorical accuracy, and recall
                line = line.rstrip('\n')
                metrics_per_epoch = line.split(metrics_per_epoch_parser)[1].split('-')
                for metric in metrics_per_epoch:
                    if metric_name in metric:
                        metric = metric.rstrip()
                        metric_score =  float(metric.split(':')[1])
                        break
                metric_scores.append(metric_score)
        f.close()
        print(model_name)
        # range [:num_epochs] for training. range [num_epochs:] is for test set 
        metric_scores = metric_scores[:num_epochs]
        print(f'Epochs: {num_epochs}')
        print(f'{metric_name}: {metric_scores}')
        assert  len(metric_scores)==num_epochs, f'Length of metric scores {len(metric_scores)} should match number of epochs {num_epochs}.'
        print()
        models.append(model_name)
        metric_scores_all.append(metric_scores)

    return models, np.array(metric_scores_all)

def measure_performance(X, y, clf, show_accuracy=True, show_classification_report=True, show_confusion_matrix=True, plot_cm=True):
    y_pred = clf.predict(X)   
    if show_accuracy:
         print ("Accuracy:{0:.3f}".format(accuracy_score(y, y_pred)),"\n")
    
    if show_classification_report:
        print ("Classification report")
        print (classification_report(y, y_pred),"\n")
      
    if show_confusion_matrix:
        mat = confusion_matrix(y, y_pred)
        print ("Confusion matrix")
        print (mat,"\n")

    if plot_cm:
        mat = confusion_matrix(y, y_pred)
        target_names = np.unique(y)
        plot_confusion_matrix(mat, target_names)
        
def plot_confusion_matrix(mat, target_names):
    import seaborn as sns; sns.set()

    fig, ax = plt.subplots(figsize=(8,8))
    ax = sns.heatmap(mat.T, square=True, linecolor='grey', linewidths=1, annot=True, 
                fmt='d', cbar=True, cmap='Reds', ax=ax, annot_kws={"fontsize":12, "weight":"bold"},
                xticklabels=target_names,
                yticklabels=target_names)
    bottom, top = ax.get_ylim()
    ax.set_ylim(bottom + 0.5, top - 0.5)
    plt.xlabel('true label')
    plt.ylabel('predicted label')

def plot_loss_vs_X_per_epoch(model_name, losses, metric, metric_name, save_path=''):
    print(f'Loss vs {metric_name} for {model_name}')
    y_max = np.max([max(losses), max(metric)])
    y_max += 0.25
    epochs = np.arange(1, len(losses)+1)
    plt.plot(epochs, metric, label=metric_name, c='g')
    plt.plot(epochs, losses, label='Loss', c='b')
    plt.ylim([0, y_max])
    plt.legend()
    plt.xlabel('Epochs')
    if save_path != '':
        save_file = os.path.join(save_path, model_name + '_loss_vs_' + metric_name)
        print(f'Saving Loss vs {metric_name} plot to {save_file}')
        plt.savefig(save_file)
    else:
        plt.show()
    plt.clf()


def plot_params(param_values, param_name, train_scores, test_scores, save_file='', ylabel='Average Silhoutte Scores'):
    # plot the training and testing scores in a log scale
    plt.plot(param_values, train_scores, label='Train', alpha=0.4, lw=2, c='b')
    plt.plot(param_values, test_scores, label='X-Val', alpha=0.4, lw=2, c='g')
    plt.legend(loc=7)
    plt.xlabel(param_name)
    plt.ylabel(ylabel)
    if save_file != '':
        plt.title(save_file)
        plt.savefig(save_file)

def plot_pie(data, title='Proportions of samples per group', explode=(0, 0, 0, 0, 0, 0, 0, 0.1, 0.3, 0.6)):
    labels = data['label'].value_counts().index
    sizes = data['label'].value_counts().values

    _, ax1 = plt.subplots()
    ax1.pie(sizes, labels=labels, autopct='%1.1f%%', explode=explode)
    ax1.set_title(title)
    plt.show()


def predict_and_score(model, model_name, X, y_true, datatype):
    """
        model := instantiated classifier 
        model_name := the name of our classifier
        X := 2D array; X_train, X_test, X_val
        y_true := 1D array; y_train, y_test, y_val
        datatype := str; training, test, validation
        
        returns y_pred
    """
    y_pred = model.predict(X)
    avg = model.score(X, y_true)
    f1 = f1_score(y_true, y_pred, average='weighted')
    print(f'Our benchmark model for {model_name} with parameters {model.get_params()} returned a mean accuracy score of {avg} and f1-score of {f1} on the {datatype} set.\n')

    return y_pred

