from matplotlib import cm
from matplotlib import pyplot as plt
import numpy as np
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, f1_score
from sklearn.model_selection import GridSearchCV



def get_grid(model, parameters, scoring, X, Y=None, vb=1, cv=3, return_train_score=True):
    grid = GridSearchCV(estimator=model, param_grid=parameters, scoring=scoring, cv=cv, return_train_score=return_train_score, verbose=vb) # if we get a warning on the server about iid, add parameter `iid=False`
    grid.fit(X,Y)
    print("Best estimator: ", grid.best_estimator_)
    print("Best params: ", grid.best_params_)
    print("Best Score: ", grid.best_score_)
    return grid

def measure_performance(X, y, clf, show_accuracy=True, show_classification_report=True, show_confusion_matrix=True, plot_cm=True):
    y_pred = clf.predict(X)   
    if show_accuracy:
         print ("Accuracy:{0:.3f}".format(accuracy_score(y, y_pred)),"\n")
    
    if show_classification_report:
        print ("Classification report")
        print (classification_report(y, y_pred),"\n")
      
    if show_confusion_matrix:
        mat = confusion_matrix(y, y_pred)
        print ("Confusion matrix")
        print (mat,"\n")

    if plot_cm:
        mat = confusion_matrix(y, y_pred)
        target_names = np.unique(y)
        plot_confusion_matrix(mat, target_names)
        
def plot_confusion_matrix(mat, target_names):
    import seaborn as sns; sns.set()

    fig, ax = plt.subplots(figsize=(8,8))
    ax = sns.heatmap(mat.T, square=True, linecolor='grey', linewidths=1, annot=True, 
                fmt='d', cbar=True, cmap='Reds', ax=ax, annot_kws={"fontsize":12, "weight":"bold"},
                xticklabels=target_names,
                yticklabels=target_names)
    bottom, top = ax.get_ylim()
    ax.set_ylim(bottom + 0.5, top - 0.5)
    plt.xlabel('true label')
    plt.ylabel('predicted label')


def plot_params(param_values, param_name, train_scores, test_scores, save_file='', ylabel='Average Silhoutte Scores'):
    # plot the training and testing scores in a log scale
    plt.plot(param_values, train_scores, label='Train', alpha=0.4, lw=2, c='b')
    plt.plot(param_values, test_scores, label='X-Val', alpha=0.4, lw=2, c='g')
    plt.legend(loc=7)
    plt.xlabel(param_name)
    plt.ylabel(ylabel)
    if save_file != '':
        plt.title(save_file)
        plt.savefig(save_file)

def plot_pie(data, title='Proportions of samples per group', explode=(0, 0, 0, 0, 0, 0, 0, 0.1, 0.3, 0.6)):
    labels = data['label'].value_counts().index
    sizes = data['label'].value_counts().values

    fig1, ax1 = plt.subplots()
    ax1.pie(sizes, labels=labels, autopct='%1.1f%%', explode=explode)
    ax1.set_title(title)
    plt.show()


def predict_and_score(model, model_name, X, y_true, datatype):
    """
        model := instantiated classifier 
        model_name := the name of our classifier
        X := 2D array; X_train, X_test, X_val
        y_true := 1D array; y_train, y_test, y_val
        datatype := str; training, test, validation
        
        returns y_pred
    """
    y_pred = model.predict(X)
    avg = model.score(X, y_true)
    f1 = f1_score(y_true, y_pred, average='weighted')
    print(f'Our benchmark model for {model_name} with parameters {model.get_params()} returned a mean accuracy score of {avg} and f1-score of {f1} on the {datatype} set.\n')

    return y_pred

