#Split Data to Train/Test (80/20)
import pickle
import os
import numpy as np
import gc

from utils.utils import drop_classes_by_count, drop_classes_by_type, getDataset, randomly_shuffle_dataset, save_data_to_file, split_train_test_sets

def splitTrainTestCategorical(vType, vectorAllPath, vectorTrainPath, vectorTestPath, randomSeed, split=0.8, dropClasses=[], min_num=None):
    """
        Stratify shuffle data
        outputs 2 files: one for training, and one for testing.
    """
    print("\nCommencing split for training and testing sets...")

    # [list of tokens, labels (0 for non-vulnerable, 1 for vulnerable), function list in each program, filenames, vulnerability types, CWE-IDs, test case ids].
    group_set = getDataset(vectorAllPath, getBalanced=False)

    # Get ratios
    classes, y_indices = np.unique(group_set[-2], return_inverse=True)
    class_counts = np.bincount(y_indices)

    if len(class_counts) < 2:
        raise ValueError("The least populated class in y has only 1 member, which is too few. The minimum number of groups for any class cannot be less than 2.")

    # Find the sorted list of instances for each class
    class_indices = np.split(np.argsort(y_indices, kind='mergesort'), np.cumsum(class_counts)[:-1])
    print(f'Classes: {classes}.\n.\nClass Counts:{class_counts}.\n')

    # filter out a class if applicable
    if (dropClasses != []):
        classes, class_counts, class_indices = drop_classes_by_type(vType, classes, class_counts, class_indices, dropClasses)
    elif (min_num != None):
        classes, class_counts, class_indices = drop_classes_by_count(classes, class_counts, class_indices, min_num)
    print(f'Final Class: {classes}.\tClass Count:{class_counts}.')

    # Split into training and test sets
    train_set, test_set = split_train_test_sets(class_counts, class_indices, split, group_set)
    
    # randomseed shuffle
    randomly_shuffle_dataset(train_set, randomSeed)
    randomly_shuffle_dataset(test_set, randomSeed)


    print("Samples in Training set: ", len(train_set[-1]))
    newpath = os.path.join(vectorTrainPath, vType + "_train.pkl")
    f_train = open(newpath, 'wb')
    pickle.dump(train_set, f_train, protocol=pickle.HIGHEST_PROTOCOL)
    f_train.close()
    del train_set
    gc.collect()
    print(f'Train set saved in {newpath}')
 
    print("Samples in Test set: ", len(test_set[-1]))
    newpath = os.path.join(vectorTestPath, vType + "_test.pkl")
    f_test = open(newpath, 'wb')
    pickle.dump(test_set, f_test, protocol=pickle.HIGHEST_PROTOCOL)
    f_test.close()
    del test_set
    gc.collect()
    print(f'Test set saved in {newpath}\n')


def splitTrainTestByType(vType, vectorAllPath, vectorTrainPath, vectorTestPath, randomSeed=1099, split=0.8):
    """
        Stratify shuffle data.  Outputs 2 files: one for training, and one for testing.
        vType           := str; splits the data by vulernable syntax characteristic type (AE, ARR, API, or PTR)
        vectorAllPath   := str; input path that contains the entire dataset.
        vectorTrainPath := str; output path for the newly split training dataset.
        vectorTestPath  := str; output path for the newly split test dataset.
        randomSeed      := int; random seed number to shuffle data by.
        split           := float between [0, 1]; percentage to split for training set.  Remainder gets put into test set.
    """
    print(f'\nCommencing split for training and testing sets by vulnerability syntax characteric {vType}...')

    # [list of tokens, labels (0 for non-vulnerable, 1 for vulnerable), function list in each program, filenames, vulnerability types, CWE-IDs, test case ids].
    group_set = getDataset(vectorAllPath, getBalanced=False)

    # Get ratios by vType
    classes, y_indices = np.unique(group_set[4], return_inverse=True)
    class_counts = np.bincount(y_indices)

    if len(class_counts) < 2:
        raise ValueError("The least populated class in y has only 1 member, which is too few. The minimum number of groups for any class cannot be less than 2.")

    # Find the sorted list of instances for each class
    class_indices = np.split(np.argsort(y_indices, kind='mergesort'), np.cumsum(class_counts)[:-1])
    print(f'Classes: {classes}.\n\nClass Counts:{class_counts}.\n')

    # filter out other classes
    classes, class_counts, class_indices = drop_classes_by_type(vType, classes, class_counts, class_indices)
    print(f'Final Class: {classes}.\tClass Count:{class_counts}.')
 
    # Split into training and test sets
    train_set, test_set = split_train_test_sets(class_counts, class_indices, split, group_set)
    
    # randomseed shuffle
    randomly_shuffle_dataset(train_set, randomSeed)
    randomly_shuffle_dataset(test_set, randomSeed)

    print("Samples in Training set: ", len(train_set[-1]))
    save_data_to_file(vectorTrainPath, "balanced_train.pkl", train_set)
    del train_set
    gc.collect()
    print(f'Train set saved in {vectorTrainPath}/balanced_train.pkl')
 
    print("Samples in Test set: ", len(test_set[-1]))
    save_data_to_file(vectorTestPath, "balanced_test.pkl", test_set)
    del test_set
    gc.collect()
    print(f'Test set saved in {vectorTestPath}/balanced_test.pkl\n')


