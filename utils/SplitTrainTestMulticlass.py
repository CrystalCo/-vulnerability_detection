#Split Data to Train/Test (80/20)
import gc

from utils.utils import drop_classes_by_count, drop_classes_by_type, getDataset, randomly_shuffle_dataset, save_data_to_file, split_train_test_sets, get_class_indices_by_index

def splitTrainTestCategorical(vType, vectorAllPath, vectorTrainPath, vectorTestPath, randomSeed, split=0.8, min_num=None, dropNonVtypes=False):
    """
        Stratify shuffle data.  Outputs 2 files: one for training, and one for testing.
        vType           := str; splits the data by vulernable syntax characteristic type (AE, ARR, API, or PTR)
        vectorAllPath   := str; input path that contains the entire dataset.
        vectorTrainPath := str; output path for the newly split training dataset.
        vectorTestPath  := str; output path for the newly split test dataset.
        randomSeed      := int; random seed number to shuffle data by.
        split           := float between [0, 1]; percentage to split for training set.  Remainder gets put into test set.
        min_num         := int; minimum number of samples a class should have. Classes that dont meet this threshold are dropped.
        dropNonVtypes      := bool; whether to only test on the vType passed in.
    """
    print("\nCommencing split for training and testing sets...")

    # [list of tokens, labels (0 for non-vulnerable, 1 for vulnerable), function list in each program, filenames, vulnerability types, CWE-IDs, test case ids].
    group_set = getDataset(vectorAllPath, getBalanced=False)

    # filter out a classes if applicable
    if (dropNonVtypes):
        assert(vType in ['AE', 'ARR', 'API', 'PTR'])
        print(f'\nFiltering out vulnerability syntax characteric types that are not of type {vType}...')
        classes, class_counts, class_indices = get_class_indices_by_index(group_set, index=4)
        classes, class_counts, class_indices = drop_classes_by_type(vType, classes, class_counts, class_indices)
        # if we're also dropping by count, reset the original data to be able to replicate our process of extracting by x.
        if (min_num != None):
            new_set = [[], [], [], [], [],[], []]
            for indices in class_indices:
                for i in indices:
                    for group_index in range(len(group_set)):
                        new_set[group_index].append(group_set[group_index][i])
            group_set = new_set
    
    classes, class_counts, class_indices = get_class_indices_by_index(group_set, index=-2)
    if (min_num != None):
        print(f'\nFiltering out classes with less than {min_num} samples...')
        classes, class_counts, class_indices = drop_classes_by_count(classes, class_counts, class_indices, min_num)    
    _summarize_distribution(classes, class_counts)
    
    # Split into training and test sets
    train_set, test_set = split_train_test_sets(class_counts, class_indices, split, group_set)
    classes, class_counts, class_indices = get_class_indices_by_index(train_set, index=-2)
    print('Training set Classes:')
    _summarize_distribution(classes, class_counts)

    classes, class_counts, class_indices = get_class_indices_by_index(test_set, index=-2)
    print('Test set Classes:')
    _summarize_distribution(classes, class_counts)
    
    # randomseed shuffle
    randomly_shuffle_dataset(train_set, randomSeed)
    randomly_shuffle_dataset(test_set, randomSeed)

    # Save results
    print("Samples in Training set: ", len(train_set[-1]))
    save_data_to_file(vectorTrainPath, "balanced_train.pkl", train_set)
    del train_set
    gc.collect()
    print(f'Train set saved in {vectorTrainPath}/balanced_train.pkl')
 
    print("Samples in Test set: ", len(test_set[-1]))
    save_data_to_file(vectorTestPath, "balanced_test.pkl", test_set)
    del test_set
    gc.collect()
    print(f'Test set saved in {vectorTestPath}/balanced_test.pkl\n')


def _summarize_distribution(y, counts):
    # summarize distribution
    for k,v in zip(y, counts):
        per = v / sum(counts) * 100
        print('Encoded Label=%d,\tn=%d (%.3f%%)' % (k, v, per))
