#Split Data to Train/Test (80/20)
import gc

from utils.utils import drop_classes_by_count, getDataset, randomly_shuffle_dataset, save_data_to_file, split_train_test_sets, get_class_indices_by_index, preprocess_data_filter_by_type

def splitTrainTestCategorical(vType, vectorAllPath, vectorTrainPath, vectorTestPath, randomSeed, split=0.8, min_num=None, dropNonVtypes=False):
    """
        Stratify shuffle data.  Outputs 2 files: one for training, and one for testing.
        vType           := str; splits the data by vulernable syntax characteristic type (AE, ARR, API, or PTR)
        vectorAllPath   := str; input path that contains the entire dataset.
        vectorTrainPath := str; output path for the newly split training dataset.
        vectorTestPath  := str; output path for the newly split test dataset.
        randomSeed      := int; random seed number to shuffle data by.
        split           := float between [0, 1]; percentage to split for training set.  Remainder gets put into test set.
        min_num         := int; minimum number of samples a class should have. Classes that dont meet this threshold are dropped.
        dropNonVtypes      := bool; whether to only test on the vType passed in.
    """
    print("\nCommencing split for training and testing sets...")

    # [list of tokens, labels (0 for non-vulnerable, 1 for vulnerable), function list in each program, filenames, vulnerability types, CWE-IDs, test case ids].
    data = getDataset(vectorAllPath, getBalanced=False)

    # filter out a classes if applicable
    if (dropNonVtypes):
        data = preprocess_data_filter_by_type(vType, data)
        
    classes, class_counts, class_indices = get_class_indices_by_index(data, index=-2)
    if (min_num != None):
        print(f'\nFiltering out classes with less than {min_num} samples...')
        classes, class_counts, class_indices = drop_classes_by_count(classes, class_counts, class_indices, min_num)
    summarize_distribution_of_class_counts(classes, class_counts)
    
    # Split into training and test sets
    train_set, test_set = split_train_test_sets(class_counts, class_indices, split, data)
    classes, class_counts, class_indices = get_class_indices_by_index(train_set, index=-2)
    print('\nTraining set Classes:')
    summarize_distribution_of_class_counts(classes, class_counts)

    classes, class_counts, class_indices = get_class_indices_by_index(test_set, index=-2)
    print('\nTest set Classes:')
    summarize_distribution_of_class_counts(classes, class_counts)
    
    # randomseed shuffle
    randomly_shuffle_dataset(train_set, randomSeed)
    randomly_shuffle_dataset(test_set, randomSeed)

    # Save results
    print("Samples in Training set: ", len(train_set[-1]))
    save_data_to_file(vectorTrainPath, "balanced_train.pkl", train_set)
    print(f'Train set saved in {vectorTrainPath}/balanced_train.pkl')
    del train_set
    gc.collect()
 
    print("Samples in Test set: ", len(test_set[-1]))
    save_data_to_file(vectorTestPath, "balanced_test.pkl", test_set)
    print(f'Test set saved in {vectorTestPath}/balanced_test.pkl\n')
    del test_set
    gc.collect()


def summarize_distribution_of_class_counts(y, counts):
    # summarize distribution
    for k,v in zip(y, counts):
        per = v / sum(counts) * 100
        print('Encoded Label=%d,\tn=%d (%.3f%%)' % (k, v, per))
