import os, pickle 

import numpy as np

from utils.utils import GetData, SaveData, save_data_to_file
from utils.MLMethods import convert_nested_lists_to_numpy_arrays

def FlattenVectors(input_path, output_path, avg, vector_size):
    """ Flatten 3D vectors to 2D. """
    print('\nFlattening vectors to 2D')
    data = GetData(input_path)
    assert type(data[0][0][0]) == list or type(data[0][0][0]) == np.ndarray, f'Expected data[0][0][0] to be of type list or array. Instead it is type {type(data[0][0][0])}'

    # Reshape from 3D to 2D
    input_shape_1 = avg * vector_size
    x = convert_nested_lists_to_numpy_arrays(data[0], avg, vector_size)
    x = np.reshape(x, (x.shape[0], input_shape_1))
    print(f'New vector size will be: {avg}x{vector_size}={str(input_shape_1)}')
    data[0] = x
    SaveData(output_path, data)

def getAvgLength(arr):
    """
        Return avg length of the inner arrays within an array,
        and the list of all the lengths in case you want to pick a value other than the avg.
    """
    totalSamples = len(arr)
    print("Total Samples: ", totalSamples)
    totalVectorLen = 0
    for i in range(totalSamples): 
        totalVectorLen += len(arr[i])
    return int(totalVectorLen/totalSamples)

def tranformDimsByFocus(fromTrainDataPath, fromTestDataPath, toTrainDataPath, toTestDataPath, maxLen, vector_dim, vType='balanced'):
    """
        Transforms the length of inner arrays within an array to `maxlen` size. 
        If shape of data is < maxlen, pads remainder with zeros.  If shape
        is > maxlen, truncates (maxlen/2) to the left and right of focus pointer.

        INPUTS:
            string paths that point to 
        OUTPUT:
            Reshaped data
    """
    print("Trimming training set...")
    setTrimmedDataset(fromTrainDataPath, toTrainDataPath, maxLen, vector_dim, vType)
    
    print("\nTrimming test set...")
    setTrimmedDataset(fromTestDataPath, toTestDataPath, maxLen, vector_dim, vType)


def setTrimmedDataset(input_path, output_path, maxLen, vector_dim, vType, outputFilename=''):
    for filename in os.listdir(input_path):
        if not filename.endswith(".pkl"):
            continue
        if vType not in filename:
            continue
        # Transform Data
        dataPath = os.path.join(input_path, filename)
        data = truncateRows(dataPath, maxLen, vector_dim)
        outputFilename = outputFilename if outputFilename != '' else "DL_Final_" + filename
        save_data_to_file(output_path, outputFilename, data)

def TrimData(input_path, output_path, maxLen, vector_dim):
        """New, improved version. Will eventually replace setTrimmedDataset()"""
        data = truncateRows(input_path, maxLen, vector_dim)
        SaveData(output_path, data)


def truncateRows(dataSetpath, maxlen, vector_dim):
    """
        Truncates row length of data to match maxlen size, centering around 
        the focuspointer to ensure the data returned still contains the part 
        where the vulnerability lies in order to produce suitable data.
    """
    f1 = open(dataSetpath, 'rb')
    data = pickle.load(f1)
    X, focuspointers = data[0], data[2]
    f1.close()

    new_X = []
    
    for x, focus in zip(X, focuspointers):
        if len(x) <  maxlen:
            remaining_length = maxlen - len(x)
            fill_0 = np.zeros((remaining_length, vector_dim))
            x = np.concatenate((x, fill_0))
            new_X.append(x)
        elif len(x) == maxlen:
            new_X.append(x)
        else:
            startpoint = int(focus - round(maxlen / 2.0))
            endpoint =  int(startpoint + maxlen)
            if startpoint < 0:
                startpoint = 0
                endpoint = maxlen
            if endpoint >= len(x):
                startpoint = -maxlen
                endpoint = None
            new_X.append(x[startpoint:endpoint])

    data[0] = new_X

    return data

def generator_of_data(data, labels, batchsize, maxlen, vector_dim):
    """
        Generator is used when there is too much data to fit into memory. 
        Generates batch_size number of the samples of the data at a time.
    """
    iter_num = int(len(data) / batchsize)
    i = 0
    
    while iter_num:
        batchdata = data[i:i + batchsize]
        # batchdata = process_sequences_shape(batchdata, maxlen, vector_dim)
        batched_labels = labels[i:i + batchsize]
        yield (batchdata, batched_labels)
        i = i + batchsize
        
        iter_num -= 1
        if iter_num == 0:
            iter_num = int(len(data) / batchsize)
            i = 0

def transformInput (onevectorsample, maxlen,vecdim):
    """Used in prediction."""
    nb_samples = np.zeros((1, maxlen, vecdim))
    sequence = onevectorsample

    i = 0
    m = 0
    for vectors in sequence:#500
        n = 0
        for values in vectors:#30
            nb_samples[i][m][n] += values
            n += 1
        m += 1
         
    print(np.shape(nb_samples))
    return nb_samples

