import os, pickle 

import numpy as np

def getAvgLength(arr):
    """
        Return avg length of the inner arrays within an array,
        and the list of all the lengths in case you want to pick a value other than the avg.
    """
    totalSamples = len(arr)
    totalVectorLen = 0
    vectorLengths = []

    for a in arr:
        v_length = len(a) 
        totalVectorLen += v_length
        vectorLengths.append(v_length)

    vectorLengths.sort()

    meanLen = int(totalVectorLen/totalSamples)
    return meanLen, vectorLengths


def tranformDimsByFocus(fromTrainDataPath, fromTestDataPath, toTrainDataPath, toTestDataPath, maxLen, vector_dim, vType='balanced'):
    """
        Transforms the length of inner arrays within an array to `maxlen` size. 
        If shape of data is < maxlen, pads remainder with zeros.  If shape
        is > maxlen, truncates (maxlen/2) to the left and right of focus pointer.

        INPUTS:
            string paths that point to 
        OUTPUT:
            Reshaped data
    """
    print("Loading data...")
    print("Train set")
    for filename in os.listdir(fromTrainDataPath):
        if not filename.endswith(".pkl"):
            continue
        if vType not in filename:
            continue
        # Transform Data
        dataPath = os.path.join(fromTrainDataPath, filename)
        data = truncateRows(dataPath, maxLen, vector_dim)
        dataPath = os.path.join(toTrainDataPath, "DL_Final_" + filename)
        f_train = open(dataPath, 'wb')
        pickle.dump(data, f_train)
        f_train.close()
    
    print("\nTest set")
    for filename in os.listdir(fromTestDataPath):
        if not (vType in filename):
            continue
        if not (filename.endswith(".pkl")):
            continue
        dataPath = os.path.join(fromTestDataPath, filename)
        data = truncateRows(dataPath, maxLen, vector_dim)
        dataPath = os.path.join(toTestDataPath, "DL_Final_" + filename)
        f_test = open(dataPath, 'wb')
        pickle.dump(data, f_test)
        f_test.close()


def truncateRows(dataSetpath, maxlen, vector_dim):
    """
        Truncates row length of data to match maxlen size, centering around 
        the focuspointer to ensure the data returned still contains the part 
        where the vulnerability lies in order to produce suitable data.
    """
    f1 = open(dataSetpath, 'rb')
    data = pickle.load(f1)
    X, focuspointers = data[0], data[2]
    f1.close()

    new_X = []
    
    for x, focus in zip(X, focuspointers):
        if len(x) <  maxlen:
            remaining_length = maxlen - len(x)
            fill_0 = np.zeros((remaining_length, vector_dim))
            x = np.concatenate((x, fill_0))
            new_X.append(x)
        elif len(x) == maxlen:
            new_X.append(x)
        else:
            startpoint = int(focus - round(maxlen / 2.0))
            endpoint =  int(startpoint + maxlen)
            if startpoint < 0:
                startpoint = 0
                endpoint = maxlen
            if endpoint >= len(x):
                startpoint = -maxlen
                endpoint = None
            new_X.append(x[startpoint:endpoint])

    data[0] = new_X

    return data

def process_sequences_shape(samples, maxLen, vector_dim):
    """
        Returns a 3-dimensional np array of size samples x maxLen x vector_dim,
        where samples = data, maxlen = number of rows/tokens per sample, 
        and vector_dim = number of features
    """
    batch_size = len(samples)
    reshaped_samples = np.array(samples)
    reshaped_samples = np.reshape(reshaped_samples, (batch_size, maxLen, vector_dim))

    nb_samples = np.zeros((batch_size, maxLen, vector_dim))

    i = 0
    for sample in samples:
        m = 0
        for vectors in sample:
            n = 0
            for values in vectors:
                if m >= maxLen:
                    continue
                elif n >= vector_dim:
                    break
                try:
                    nb_samples[i][m][n] += values
                    n += 1
                except Exception as e:
                    print(e)
                    print(f'i: {i} type:{type(i)}')
                    print(f'm: {m} type:{type(m)}')
                    print(f'n: {n} type:{type(n)}')
                    print(f'values: {values} type:{type(values)}')
                    raise(e)
            m += 1
        i += 1
    reshaped_samples = np.array(samples)
    reshaped_samples = np.reshape(reshaped_samples, (batch_size, maxLen, vector_dim))
    print(reshaped_samples == nb_samples)
    return nb_samples


def generator_of_data(data, labels, batchsize, maxlen, vector_dim):
    """
        Generator is used when there is too much data to fit into memory. 
        Generates batch_size number of the samples of the data at a time.
    """
    iter_num = int(len(data) / batchsize)
    i = 0
    
    while iter_num:
        batchdata = data[i:i + batchsize]
        # batchdata = process_sequences_shape(batchdata, maxlen, vector_dim)
        batched_labels = labels[i:i + batchsize]
        yield (batchdata, batched_labels)
        i = i + batchsize
        
        iter_num -= 1
        if iter_num == 0:
            iter_num = int(len(data) / batchsize)
            i = 0

def transformInput (onevectorsample, maxlen,vecdim):
    """Used in prediction."""
    nb_samples = np.zeros((1, maxlen, vecdim))
    sequence = onevectorsample

    i = 0
    m = 0
    for vectors in sequence:#500
        n = 0
        for values in vectors:#30
            nb_samples[i][m][n] += values
            n += 1
        m += 1
         
    print(np.shape(nb_samples))
    return nb_samples

