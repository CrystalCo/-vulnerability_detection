import os, pickle 

import numpy as np

from utils.utils import save_data_to_file

def getAvgLength(arr):
    """
        Return avg length of the inner arrays within an array,
        and the list of all the lengths in case you want to pick a value other than the avg.
    """
    totalSamples = len(arr)
    print("Total Samples: ", totalSamples)
    totalVectorLen = 0
    for i in range(totalSamples): 
        totalVectorLen += len(arr[i])
    return int(totalVectorLen/totalSamples)

def tranformDimsByFocus(fromTrainDataPath, fromTestDataPath, toTrainDataPath, toTestDataPath, maxLen, vector_dim, vType='balanced'):
    """
        Transforms the length of inner arrays within an array to `maxlen` size. 
        If shape of data is < maxlen, pads remainder with zeros.  If shape
        is > maxlen, truncates (maxlen/2) to the left and right of focus pointer.

        INPUTS:
            string paths that point to 
        OUTPUT:
            Reshaped data
    """
    print("Trimming training set...")
    setTrimmedDataset(fromTrainDataPath, toTrainDataPath, maxLen, vector_dim, vType)
    
    print("\nTrimming test set...")
    setTrimmedDataset(fromTestDataPath, toTestDataPath, maxLen, vector_dim, vType)


def setTrimmedDataset(inputPath, outputPath, maxLen, vector_dim, vType, outputFilename=''):
    for filename in os.listdir(inputPath):
        if not filename.endswith(".pkl"):
            continue
        if vType not in filename:
            continue
        # Transform Data
        dataPath = os.path.join(inputPath, filename)
        data = truncateRows(dataPath, maxLen, vector_dim)
        outputFilename = outputFilename if outputFilename != '' else "DL_Final_" + filename
        save_data_to_file(outputPath, outputFilename, data)


def truncateRows(dataSetpath, maxlen, vector_dim):
    """
        Truncates row length of data to match maxlen size, centering around 
        the focuspointer to ensure the data returned still contains the part 
        where the vulnerability lies in order to produce suitable data.
    """
    f1 = open(dataSetpath, 'rb')
    data = pickle.load(f1)
    X, focuspointers = data[0], data[2]
    f1.close()

    new_X = []
    
    for x, focus in zip(X, focuspointers):
        if len(x) <  maxlen:
            remaining_length = maxlen - len(x)
            fill_0 = np.zeros((remaining_length, vector_dim))
            x = np.concatenate((x, fill_0))
            new_X.append(x)
        elif len(x) == maxlen:
            new_X.append(x)
        else:
            startpoint = int(focus - round(maxlen / 2.0))
            endpoint =  int(startpoint + maxlen)
            if startpoint < 0:
                startpoint = 0
                endpoint = maxlen
            if endpoint >= len(x):
                startpoint = -maxlen
                endpoint = None
            new_X.append(x[startpoint:endpoint])

    data[0] = new_X

    return data

def generator_of_data(data, labels, batchsize, maxlen, vector_dim):
    """
        Generator is used when there is too much data to fit into memory. 
        Generates batch_size number of the samples of the data at a time.
    """
    iter_num = int(len(data) / batchsize)
    i = 0
    
    while iter_num:
        batchdata = data[i:i + batchsize]
        # batchdata = process_sequences_shape(batchdata, maxlen, vector_dim)
        batched_labels = labels[i:i + batchsize]
        yield (batchdata, batched_labels)
        i = i + batchsize
        
        iter_num -= 1
        if iter_num == 0:
            iter_num = int(len(data) / batchsize)
            i = 0

def transformInput (onevectorsample, maxlen,vecdim):
    """Used in prediction."""
    nb_samples = np.zeros((1, maxlen, vecdim))
    sequence = onevectorsample

    i = 0
    m = 0
    for vectors in sequence:#500
        n = 0
        for values in vectors:#30
            nb_samples[i][m][n] += values
            n += 1
        m += 1
         
    print(np.shape(nb_samples))
    return nb_samples

