import os, sys

VUL_PATH = os.environ.get('VUL_PATH')
sys.path.insert(1, VUL_PATH)

import numpy as np
import tensorflow as tf

from SYSE_1_isVulnerable.splitTrainTest import splitTrainTestCategorical
from SYSE_1_isVulnerable.adjustVectorLen import meanLen, tranformVectorLen
from SYSE_1_isVulnerable.saveKeyData import saveKeyDataMulticlass
from SYSE_1_isVulnerable.DLModel import buildBGRU2, fitModel2
from SYSE_1_isVulnerable.ConfusionMatrix import getConfusionMatrix_Multiclass
from SYSE_1_isVulnerable.DLPrediction import predictMulticlassLabel
from SYSE_1_isVulnerable.evaluateModels import roc_auc_score_multiclass
from Word2VecModel import Word2VecModel
from utils.utils import getDataset, encode_target, save_data_to_file

class GridSearch():
    
    def __init__(self, build_model, transformer_model=None, params={}, metrics=[], cv=1, vectorBalancedPath=os.path.join('data','DLvectors', 'ALL_balanced'), vectorTypePath=os.path.join('data','DLvectors'), vectorTrainPath=os.path.join('data','vector','train'), vectorTestPath=os.path.join('data','vector','test'), inputsTrainPath=os.path.join('data','DLinputs','train'), inputsTestPath=os.path.join('data','DLinputs','test')) -> None:
        self.build_model = build_model
        self.model = None
        self.transformer_model = transformer_model
        self.transformer = None
        self.params = params
        self.metrics = metrics
        self.cv = cv
        self.avg = 0
        self.vector_size = params['vector_size']
        self.vectorTypePath = vectorTypePath
        self.vectorBalancedPath = vectorBalancedPath
        self.vectorTrainPath = vectorTrainPath
        self.vectorTestPath = vectorTestPath
        self.inputsTrainPath = inputsTrainPath
        self.inputsTestPath = inputsTestPath
        

    def fit_transform(self):
        # Fit on all the data & transform to vectors
        data = getDataset(dataset_path=self.vectorTypePath, getBalanced=True) # gets ALL data from data/_Lvectors/balanced_vectors.pkl
        X = data[0]
        X = self.transformer.fit_transform(X)
        data[0] = X

        # Save data to a file
        save_data_to_file(self.vectorBalancedPath, 'balanced_vectors.pkl', data)
        
    def splitTrainTest(self):
        # Split into train/test
        splitTrainTestCategorical('balanced', self.vectorBalancedPath, self.vectorTrainPath, self.vectorTestPath, randomSeed=self.randomSeed, dropClass=0)

    def adjustVectorLength(self):
        # Adjust vector length
        self.avg = meanLen(self.vectorTrainPath)
        tranformVectorLen(self.vectorTrainPath, self.vectorTestPath, self.inputsTrainPath, self.inputsTestPath, self.avg, self.vector_size)
        print(f'New Vector Length (rows x cols): {self.avg} x {self.vector_size}\n')

    def encodeLabels(self):
        # ### Get number of unique classes for density value
        train_data = getDataset(self.vectorTrainPath, getBalanced=True)
        categories = np.unique(train_data[-2])
        self.labelEncoder = encode_target(categories)[1]
        self.density_units = categories.shape[0]

        saveKeyDataMulticlass(self.inputsTrainPath, self.labelEncoder)

    def build_and_fit(self):
        """
            builds & fits the estimator model,
            predicts on the test set,
            outputs final scores.
        """
        self.optimizer = 'adam' #can be changed to ‘adamax’
        self.layers = 2
        self.dropout = 0.2 
        self.batch_size = 16
        epochs = 20
        self.activation_fn = 'softmax'

        # Open a strategy scope
        strategy = tf.distribute.MirroredStrategy()
        print('Number of devices: {}'.format(strategy.num_replicas_in_sync))
        with strategy.scope():
            myKerasModel = self.build_model(self.avg, self.vector_size, self.layers, self.dropout, self.optimizer, self.density_units, activation_fn=self.activation_fn)

        # Fit BGRU Model with trained data and save the model for later use
        self.weightpath = os.path.join('model', 'BRGU_ALL' + self.optimizer +str(self.randomSeed))
        self.model = fitModel2(myKerasModel, self.weightpath, self.inputsTrainPath, self.batch_size, self.avg, self.vector_size, self.randomSeed, epochs)

    def predict_and_score(self):
        modelName = 'BGRU%s_' % self.randomSeed

        # Prediction
        self.model.load_weights(self.weightpath)
        thresholds_dl_labels, mypredicted_labels, myreallabels, outputs_dict = predictMulticlassLabel(self.model, self.inputsTestPath, self.avg, self.vector_size, self.optimizer, modelName, self.randomSeed, self.labelEncoder)

        # Confusion Matrix
        metrics = getConfusionMatrix_Multiclass(mypredicted_labels, myreallabels, saveFig=False, path='None')
        accuracy = metrics[0]
        weighted_precision = metrics[-3]
        weighted_recall = metrics[-2]
        weighted_f1 = metrics[-1]

        # ROC
        roc_auc_dict = roc_auc_score_multiclass(myreallabels, mypredicted_labels)
        print(f'\n{modelName} ROC AUC Score\n{roc_auc_dict}\n')

        return outputs_dict, accuracy, weighted_precision, weighted_recall, weighted_f1


    def run_all(self):
        # standard vars
        id = 1
        cv=3
        randomSeeds = [0, 871, 1099]
        # W2V params
        alpha = [0.05]
        epochs = [15]
        sample = [0.001]
        workers = [10]
        negatives = [10, 15]
        windows = [2, 3, 4, 5]
        vector_sizes = [5, 15, 30, 50]
        # Save outputs to file
        filepath = os.path.join('gs_results', 'gs_w2v_bgru.txt')
        columns = '\t'.join(['ID', 'cv', 'randomSeed', 'TRANSFORMER', 'alpha', 'epochs', 'sample', 'negative', 'window', 'vector_size', 'MODEL', 'input_shape0-maxlen', 'input_shape1-vector_size', 'layers', 'dropout', 'batch_size', 'epochs', 'optimizer', 'density', 'final_activation_fn', 'loss', 'categorical_accuracy', 'recall', 'accuracy', 'weighted_precision', 'weighted_recall', 'weighted_f1'])

        f = open(filepath, 'w')
        f.write(columns + '\n')
        f.close()

        for negative in negatives:
            for window in windows:
                for vector_size in vector_sizes:
                    for i in range(cv):
                        self.randomSeed = randomSeeds[i]

                        # Init transformer
                        self.transformer = self.transformer(vector_size=vector_size, alpha=alpha, negative=negative, sample=sample, epochs=epochs, seed=self.randomSeed, window=window, workers=workers)
                        
                        # Fit transformer & transform our data
                        self.fit_transform()
                        # Split data into training and test set
                        self.splitTrainTest()
                        # Average out the row length per sample based on focuspointer
                        self.adjustVectorLength()
                        # Hot encode labels
                        self.encodeLabels()
                        # Build & fit the model
                        assert vector_size == self.vector_size, "input vector size for BGRU model should match vector size used in W2V"
                        self.build_and_fit()
                        # Predict and score on test set
                        outputs_dict, accuracy, weighted_precision, weighted_recall, weighted_f1 = self.predict_and_score()

                        # Save params and scores to a file
                        data = '\t'.join([id, i, self.randomSeed, 'W2V', alpha, epochs, sample, negative, window, vector_size, 'BGRU', self.avg, self.vector_size, self.layers, self.dropout, self.batch_size, 20, self.optimizer, self.density_units, self.activation_fn, outputs_dict['loss'], outputs_dict['categorical_accuracy'], outputs_dict['recall'], accuracy, weighted_precision, weighted_recall, weighted_f1])

                        f = open(filepath, 'a')
                        f.write(data + '\n')
                        f.close()


gs = GridSearch(model=buildBGRU2, transformer=Word2VecModel)

gs.run_all()

