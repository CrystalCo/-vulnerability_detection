import os, sys
from shutil import rmtree

VUL_PATH = os.environ.get('VUL_PATH')
sys.path.insert(1, VUL_PATH)

from SYSE_2_vulnerabilityType.VulnerabilityClassification import VulnerabilityClassification
from utils.DLCustomModels import create_bgru_model
from utils.Word2VecModel import Word2VecModel

class GridSearch():
    
    def __init__(self, build_model, transformer_model=None, params={}, metrics=[], cv=1) -> None:
        self.build_model = build_model
        self.model = None
        self.transformer_model = transformer_model
        self.transformer = None
        self.params = params
        self.metrics = metrics
        self.cv = cv


    def run_all(self, startOver=True):
        # standard vars
        id = 1
        cv=3
        randomSeeds = [0, 871, 1099]
        # W2V params
        alpha = 0.05
        epochs = 15
        sample = 0.001
        workers = -1
        negatives = [10, 15]
        windows = [2, 3, 4, 5]
        vector_sizes = [15, 30, 50]
        # Save outputs to file
        filepath = os.path.join('gs_results', 'w2v_bgru_gs_results.txt')
        columns = '\t'.join(['ID', 'cv', 'randomSeed', 'TRANSFORMER', 'alpha', 'epochs', 'sample', 'negative', 'window', 'vector_size', 'MODEL', 'input_shape0-maxlen', 'input_shape1-vector_size', 'layers', 'dropout', 'batch_size', 'epochs', 'optimizer', 'density', 'final_activation_fn', 'loss', 'categorical_accuracy', 'recall', 'accuracy', 'weighted_precision', 'weighted_recall', 'weighted_f1'])

        if startOver:
            f = open(filepath, 'w')
            f.write(columns + '\n')
            f.close()

            if os.path.exists(self.checkpoint_dir):
                rmtree(self.checkpoint_dir)

        for negative in negatives:
            for window in windows:
                for vector_size in vector_sizes:
                    self.vector_size = vector_size
                    for i in range(cv):
                        self.randomSeed = randomSeeds[i]

                        dvt = VulnerabilityClassification(self.build_model, useGenerator=True, transformer_model=Word2VecModel)

                        # Init transformer
                        dvt.transformer = self.transformer_model(vector_size=vector_size, alpha=alpha, negative=negative, sample=sample, epochs=epochs, seed=self.randomSeed, window=window, workers=workers)
                        
                        # Fit transformer & transform our data
                        self.fit_transform()
                        # Split data into training and test set
                        self.splitTrainTest()
                        # Average out the row length per sample based on focuspointer
                        self.adjustVectorLength()
                        # Hot encode labels
                        self.encodeLabels()
                        # Build & fit the model
                        assert vector_size == self.vector_size, "input vector size for BGRU model should match vector size used in W2V"
                        print(f'Building model on data W2V transformed with paramaters: ID {id}, CV:{cv}, Seed: {self.randomSeed}, Alpha: {alpha}, Epochs: {epochs}, Sample: {sample},')
                        print(f'Negative: {negative}, Window: {window}, Vector length: {vector_size}.')
                        print(f'On BGRU with input_shape0 (maxlen): {str(self.avg)}, input_shape1 (vector_size): {str(self.vector_size)}, layers: 2, dropout: 0.2, batch_size: 16, epochs: 20, optimizer: adam, density: {self.density_units}, final_activation_fn: softmax')
                        self.build_and_fit()
                        # Predict and score on test set
                        outputs_dict, accuracy, weighted_precision, weighted_recall, weighted_f1 = self.predict_and_score()

                        # Save params and scores to a file
                        data = '\t'.join([str(id), str(i), str(self.randomSeed), 'W2V', str(alpha), str(epochs), str(sample), str(negative), str(window), str(vector_size), 'BGRU', str(self.avg), str(self.vector_size), str(self.layers), str(self.dropout), str(self.batch_size), '20', self.optimizer, str(self.density_units), self.activation_fn, str(outputs_dict['loss']), str(outputs_dict['categorical_accuracy']), str(outputs_dict['recall']), str(accuracy), str(weighted_precision), str(weighted_recall), str(weighted_f1)])

                        f = open(filepath, 'a')
                        f.write(data + '\n')
                        f.close()

                        # Remove our checkpoint so the new model won't load our previous model
                        rmtree(self.checkpoint_dir)



gs = GridSearch(build_model=create_bgru_model, transformer_model=Word2VecModel)
# gs.tokenizeSlices() # Initializes the dataset (if need be)
# gs.downsample() # Downsample the dataset after tokenizing slices
# If the 2 methods above have already been run, you may comment them out and simply run below:
gs.run_all(startOver=True)

