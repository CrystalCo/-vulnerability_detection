import os, sys

VUL_PATH = os.environ.get('VUL_PATH')
sys.path.insert(1, VUL_PATH)

from SYSE_2_vulnerabilityType.VulnerabilityClassification import VulnerabilityClassification
from utils.transformDataDimensions import FlattenVectors
from utils.utils import GetDataByIndex, drop_non_vulnerable_samples
from utils.Word2VecModel import Word2VecModel

class PreProcess():
    def __init__(self, vectorsALLdir='ALL_vectors', min_num=None):
        self.RANDOMSEED = 1099
        self.all_tokens_path = os.path.join('data', 'tokens')
        self.inputsRootPath = os.path.join('data', 'DLinputs')
        self.vectorRootPath = os.path.join('data', 'DLvectors')
        self.vectorsALLPath = os.path.join(self.vectorRootPath, vectorsALLdir)
        self.vectorTrainPath = os.path.join(self.vectorRootPath, 'train')
        self.vectorTestPath = os.path.join(self.vectorRootPath, 'test')
        self.dlInputsTrainPath = os.path.join(self.inputsRootPath, 'train')
        self.dlInputsTestPath = os.path.join(self.inputsRootPath, 'test')
        self.w2vmodelPath = os.path.join('w2vModel','model', 'w2vModel')
        self.min_num = min_num
        self.VC = VulnerabilityClassification(
            transformer_model=Word2VecModel, randomSeed=self.RANDOMSEED, window=3,tokensPath=self.all_tokens_path, vectorsALLPath=self.vectorsALLPath,
            vectorTrainPath=self.vectorTrainPath, vectorTestPath=self.vectorTestPath,
            inputsTrainPath=self.dlInputsTrainPath, inputsTestPath=self.dlInputsTestPath)


    def init_vc(self, DROPOUT, UNITS, OPTIMIZER, ACTIVATION_FN, RECURRENT_ACTIVATION, model_fn, MODEL_TYPE='bgru', VUL_TYPE='syse'):
        BATCHSIZE = 64
        LAYERS = 2
        EPOCHS = 60
        VECTOR_TRANSFORMER='w2v'
        CLASS_TYPE = f'{VUL_TYPE}_min_{self.min_num}'
        DENSE_ACT_FN = 'softmax'
        METRICS = ['CategoricalAccuracy']

        checkpoint_dir = './ckpt_%s_%s_%s_%s_%s' % (MODEL_TYPE, str(DROPOUT), str(UNITS), OPTIMIZER, ACTIVATION_FN)
        model_name = '%s_%s_epochs_%s_opt_%s_%s_%s_%sunits_%s' % (MODEL_TYPE.upper(), VECTOR_TRANSFORMER.upper(), EPOCHS, OPTIMIZER, ACTIVATION_FN, DENSE_ACT_FN, UNITS, CLASS_TYPE)
        metrics_path = os.path.join(f'metrics', MODEL_TYPE)
        weights_path = os.path.join('model', model_name + '_weights')
        w2vmodelPath = os.path.join('w2vModel','model', f'w2vModel_{VUL_TYPE}')

        self.VC = VulnerabilityClassification(build_model=model_fn,
            transformer_model=Word2VecModel, transformerPath=w2vmodelPath,
            metricsPath=metrics_path, randomSeed=self.RANDOMSEED, window=3, m_epochs=EPOCHS,
            modelName=model_name, batch_size=BATCHSIZE, mask=True, dropout=DROPOUT, layers=LAYERS,
            tokensPath=self.all_tokens_path, vectorsALLPath=self.vectorsALLPath,
            vectorTrainPath=self.vectorTrainPath, vectorTestPath=self.vectorTestPath,
            inputsTrainPath=self.dlInputsTrainPath, inputsTestPath=self.dlInputsTestPath,
            checkpoint_dir=checkpoint_dir, weightpath=weights_path, optimizer=OPTIMIZER,
            activation_fn=ACTIVATION_FN, recurrent_activation=RECURRENT_ACTIVATION, dense_activation_fn=DENSE_ACT_FN, units=UNITS, metrics=METRICS)

    def preprocess_all(self):
        # tokenize the vulnerable samples
        # print('\nTokenizing slices...')
        # self.VC.tokenizeAllSourceSlices(outputFilename='ALL_tokens_syse.pkl', numSamples=9999999)

        # Fit W2V transformer & transform our data
        # print('\nTraining W2V model...')
        # self.VC.init_transformer()
        # self.VC.fit_transform(self.VC.transformerPath, self.VC.tokensPath, self.VC.vectorsALLPath, getBalanced=False)

        # Drop non-vulnerable samples
        input_path  = os.path.join(self.VC.vectorsALLPath, 'ALL_vectors.pkl')
        output_path = os.path.join(self.vectorRootPath, 'vectors', 'ALL_vectors.pkl')
        drop_non_vulnerable_samples(input_path, output_path)
        self.VC.vectorsALLPath = output_path

        # Split data into training and test set, dropping any classes that have < min number of samples
        print('\nSplitting train/test...')
        self.VC.splitTrainTest(self.VC.vectorsALLPath, min_num=self.min_num, dropOtherVtypes=False)
        
        # Flattening the dataset allows our code to run much faster.
        # First, we must average out the dimension of the dataset which contains the tokens. 
        # Trims the row length per sample based on focuspointer & avg.
        print('\nAdjusting Vector Length...')
        input_path = os.path.join(self.VC.vectorTrainPath, 'balanced_train.pkl')
        output_path = os.path.join(self.VC.inputsTrainPath, 'DL_Final_balanced_train.pkl')
        self.VC.SetVLength(input_path, output_path)    # outputs to data/DLinput
        FlattenVectors(output_path, output_path, self.VC.avg, self.VC.vector_size)    # Flatten 3D vectors to 2D
        input_path = os.path.join(self.VC.vectorTestPath, 'balanced_test.pkl')
        output_path = os.path.join(self.VC.inputsTestPath, 'DL_Final_balanced_test.pkl')
        self.VC.SetVLength(input_path, output_path)
        FlattenVectors(output_path, output_path, self.VC.avg, self.VC.vector_size)

        # set the new vector length & avg.
        self.VC.avg = 1
        self.VC.vector_size = len(GetDataByIndex(output_path, index=0)[0])
        assert self.VC.vector_size > 30, "Incorrect vector length.  {self.VC.vector_size} should be greater than 30 "

        # Encode our labels and save them to our final inputs file
        self.VC.hotEncodeLabels()
        print(f'Avg: {self.VC.avg}\tVector length: {self.VC.vector_size}\tDense units: {self.VC.density_units}')
        input_path = os.path.join(self.VC.inputsTrainPath, 'DL_Final_balanced_train.pkl')
        self.VC.saveKeyData(input_path)
        self.VC.saveKeyData(output_path)

