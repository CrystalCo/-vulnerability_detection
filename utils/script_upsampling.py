#!/usr/bin/env python
# coding: utf-8

# Our goal is to upsample all classes with vulnerable samples of size < 1500 to at least 1500 samples.
# 
# Next, we'll create a python script that will run the upsample for our training and test sets.
# 
# Finally, we will rerun our model on the new upsampled data.


import os, pickle, gc

import numpy as np

# merge trian & test file or get one that already exists


# gather all vulnerable samples
def get_vul_samples(new_set):
    vul_set = [[], [], [], [], [],[], []]

    for i in range(len(new_set[0])):
        if new_set[1][i] == 1:
            vul_set[0].append(new_set[0][i])
            vul_set[1].append(new_set[1][i])
            vul_set[2].append(new_set[2][i])
            vul_set[3].append(new_set[3][i])
            vul_set[4].append(new_set[4][i])
            vul_set[5].append(new_set[5][i])
            vul_set[6].append(new_set[6][i])
    return vul_set


def get_upsampled_data(filepath):
    with open(filepath, 'rb') as f:
        data = pickle.load(f)
    print("Samples in original set: ", len(data[-1]))

    data = get_vul_samples(data)
    print("number of vulnerable samples in original set: ", len(data[-1]))


    # Get ratios
    # return the indices of the unique array that can be used to reconstruct the array.
    classes, y_indices = np.unique(data[-2], return_inverse=True)
    class_counts = np.bincount(y_indices)

    # Find the sorted list of instances for each class
    class_indices = np.split(np.argsort(y_indices, kind='mergesort'),
                            np.cumsum(class_counts)[:-1])
    print(f'Classes: {classes}.\nClass Counts:{class_counts}.\n')


    # Since the numpy functions to upsample didn't work, we'll do it the classic simple python way
    final_set = [[], [], [], [], [],[], []]

    # run on classes with < 1500 samples
    max_size = 1500
    for count, indices in zip(class_counts, class_indices):
        if count < max_size:
            # repeat samples until we reach our target count
            n = 0
            while(n < max_size):            
                # walk the array and append as we go along
                # make sure we can keep going even if we run out
                k = n % count
                final_set[0].append(data[0][indices[k]])
                final_set[1].append(data[1][indices[k]])
                final_set[2].append(data[2][indices[k]])
                final_set[3].append(data[3][indices[k]])
                final_set[4].append(data[4][indices[k]])
                final_set[5].append(data[5][indices[k]])
                final_set[6].append(data[6][indices[k]])
                n += 1
        else:
            for class_i in indices:
                final_set[0].append(data[0][class_i])
                final_set[1].append(data[1][class_i])
                final_set[2].append(data[2][class_i])
                final_set[3].append(data[3][class_i])
                final_set[4].append(data[4][class_i])
                final_set[5].append(data[5][class_i])
                final_set[6].append(data[6][class_i])

    
    return final_set
 

vector_path = os.path.join('data', 'MLvectors') # doing on vectors and not inputs because vectors have our original CWE label while inputs has the hot-encoded labels
in_paths = [os.path.join(vector_path, 'ALL_averaged_vectors_granular', 'ALL_vectors.pkl')]
vector_path = os.path.join('data', 'DLvectors') # doing on vectors and not inputs because vectors have our original CWE label while inputs has the hot-encoded labels
out_paths = [os.path.join(vector_path, 'ALL_vectors_granular_upsampled')]

for inpath, outpath in zip(in_paths, out_paths):
    final_set = get_upsampled_data(inpath)

    # randomseed shuffle; randomSeed=1099
    for i in range(len(final_set)):
        np.random.seed(1099)
        np.random.shuffle(final_set[i])

    print("Samples in new set: ", len(final_set[-1]))
    classes, y_indices = np.unique(final_set[-2], return_inverse=True)
    class_counts = np.bincount(y_indices)
    class_indices = np.split(np.argsort(y_indices, kind='mergesort'), np.cumsum(class_counts)[:-1])
    print(f'Classes: {classes}.\n\nClass Counts:{class_counts}.\n')

    newpath = os.path.join(outpath, "ALL_vectors.pkl")
    f_train = open(newpath, 'wb')
    pickle.dump(final_set, f_train, protocol=pickle.HIGHEST_PROTOCOL)
    f_train.close()
    del final_set
    gc.collect()
    print(f'set saved in {newpath}\n\n')




