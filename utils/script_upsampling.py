#!/usr/bin/env python
# coding: utf-8

# Our goal is to upsample all classes with size < 1500 to at least 1500 samples.
# 
# First, we'll start by testing on a small sample of vectors, like our flattened test vectors.
# 
# Next, we'll create a python script that will run the upsample for our training and test sets.
# 
# Finally, we will rerun our model on the new upsampled data.


import os, pickle, gc

import numpy as np


def get_upsampled_data(filepath):
    with open(filepath, 'rb') as f:
        data = pickle.load(f)

    # Get ratios
    # return the indices of the unique array that can be used to reconstruct the array.
    classes, y_indices = np.unique(data[-2], return_inverse=True)
    class_counts = np.bincount(y_indices)

    # Find the sorted list of instances for each class
    class_indices = np.split(np.argsort(y_indices, kind='mergesort'),
                            np.cumsum(class_counts)[:-1])
    print(f'Classes: {classes}.\n.\nClass Counts:{class_counts}.\n')


    # turn each row into an numpy array so we can easily create subsets through indexing
    data = [ np.array(data[0]), np.array(data[1]), np.array(data[2]), np.array(data[3]), np.array(data[4]), np.array(data[5]), np.array(data[6]) ]


    # Since the numpy functions to upsample didn't work, we'll do it the classic simple python way
    final_set = [[], [], [], [], [],[], []]

    # run on classes with < 1500 samples
    max_size = 1500
    for count, indices in zip(class_counts, class_indices):    
        if count < max_size:
            # repeat samples until we reach our target count
            # walk the samples for each data row
            for i in range(len(data)):
                # subset of the feature for given class
                data_row = data[i][indices]
                if type(data_row) == np.ndarray:
                    data_row = list(data_row)
                
                # walk the array and append as we go along
                n_remaining = max_size - count
                for j in range(n_remaining):
                    # make sure we can keep going even if we run out
                    k = j % count
                    data_row.append(data_row[k])
                final_set[i] += data_row
        else:
            for i in range(len(data)):
                data_row = list(data[i][indices])
                final_set[i] += data_row
    
    return final_set
 

vector_path = os.path.join('data', 'DLvectors') 
in_paths = [os.path.join(vector_path, 'train_162classes_flattened', 'balanced_train.pkl'), os.path.join(vector_path,'test_162classes_flattened', 'balanced_test.pkl')]
out_paths = [os.path.join(vector_path, 'train_162classes_flattened_upsampled'), os.path.join(vector_path, 'test_162classes_flattened_upsampled')]

for inpath, outpath in zip(in_paths, out_paths):
    final_set = get_upsampled_data(inpath)

    # randomseed shuffle; randomSeed=1099
    for i in range(len(final_set)):
        np.random.seed(1099)
        np.random.shuffle(final_set[i])

    print("Samples in set: ", len(final_set[-1]))
    newpath = os.path.join(outpath, "balanced_train.pkl") if 'train' in inpath else os.path.join(outpath, "balanced_test.pkl")
    f_train = open(newpath, 'wb')
    pickle.dump(final_set, f_train, protocol=pickle.HIGHEST_PROTOCOL)
    f_train.close()
    del final_set
    gc.collect()
    print(f'set saved in {newpath}\n\n')




