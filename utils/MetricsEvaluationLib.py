import os

import numpy as np
import pandas as pd
import seaborn as sns
from matplotlib import pyplot as plt
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score

from utils.utils import drop_col_df, output_df_to_excel

def getConfusionMatrix_Multiclass(predicted_labels, true_labels, saveFig=False, path='', modelName=''):
    """
        Prints out confusion matrix and metrics to console.
        Saves output to files in path.

        saveFig := if True, creates a heatmap of predicted vs true labels.
        If saveFig is True and a path is given, then the heatmap will be saved on the path given.
        If saveFig is True and no path is given, then the heatmap will be displayed on the console (e.g. on Jupyter Notebook).
    """
    # output grid table
    confusionMatrix = confusion_matrix(true_labels, predicted_labels)
    print('\nConfusion Matrix')
    print(confusionMatrix)
    print('\nCLASSIFICATION REPORT')
    cr = classification_report(true_labels, predicted_labels, digits=4, zero_division=0)
    print(cr)

    # Output CR to excel
    filename = os.path.join(path, modelName + '_ClassificationReport.xlsx')
    df1 = classification_report_to_dataframe(cr, cut_non_cweid_rows=False)
    output_df_to_excel(df1, filename)
    

    # display heatmap
    if saveFig:
        plt.figure(figsize = (18,8))
        sns.heatmap(confusionMatrix, annot=True, xticklabels=np.unique(true_labels), yticklabels=np.unique(true_labels), cmap='summer')
        plt.xlabel('Predicted Labels')
        plt.ylabel('True Labels')
        if path != '':
            heatmapPath = os.path.join(path, modelName + 'heatmap')
            plt.savefig(heatmapPath)
        else:
            plt.show()
        plt.clf()


    ## Local Metrics
    # - Calculations per class of confusion matrix
    FP = confusionMatrix.sum(axis=0) - np.diag(confusionMatrix)
    FN = confusionMatrix.sum(axis=1) - np.diag(confusionMatrix)
    TP = np.diag(confusionMatrix)
    TN = confusionMatrix.sum() - (FP + FN + TP)
    FP = FP.astype(float)
    FN = FN.astype(float)
    TP = TP.astype(float)
    TN = TN.astype(float)
    # Sensitivity, hit rate, recall, or true positive rate
    # TODO: Use a different way to do TPR to fix ZeroDivisionError
    TPR = TP/(TP+FN)
    # Specificity or true negative rate
    TNR = TN/(TN+FP)
    # Fall out or false positive rate
    FPR = FP/(FP+TN)
    # False negative rate
    FNR = FN/(TP+FN)
    # Overall accuracy for each class (macro)
    ACC = (TP+TN)/(TP+FP+FN+TN)

    NUM_CATEGORIES = len(FP)
    M_FPR = (1/NUM_CATEGORIES) * np.nansum(FPR)

    # Weighted FPR & FNR
    df1 = classification_report_to_dataframe(cr, cut_non_cweid_rows=True)
    df1 = df1.astype(np.float)
    df1 = df1.astype({'id': np.int64, 'support': np.int64})
    df1 = df1.set_index('id')
    print_missing_vals(df1, 'Classification Report Dataframe')
    labels = df1.index.values
    counts = df1.support.values
    total_samples = len(true_labels)

    _sum = 0
    for i, count in enumerate(counts):
        result = FPR[i] * count
        _sum += result if np.isnan(result) != True else 0
        i += 1
    W_FPR = _sum/total_samples

    FNR_sum = 0
    for i, count in enumerate(counts):
        result = FNR[i] * count
        FNR_sum += result if np.isnan(result) != True else 0
        i +=1 
    W_FNR = FNR_sum/total_samples

    metrics = [TP, FP, TN, FN, TPR, TNR, FPR, FNR, ACC, counts]
    metric_names = ['TP', 'FP', 'TN', 'FN', 'TPR', 'TNR', 'FPR', 'FNR', 'ACC', 'Counts']
    df2 = pd.DataFrame(data=metrics, columns=labels, index=metric_names)
    df2 = df2.transpose()
    df2.index.name = 'id'
    print_missing_vals(df2, 'Confusion Matrix Dataframe')
    replace_missing_vals(df2, 0)

    # Confirm indices match in both tabes. 
    indices_match = compare_series(df1, df2, df1_join_name='support', df2_join_name='Counts')
    # if they do, merge the tables...
    if indices_match:
        # Merge sklearn's classification report with our calculated confusion matrix
        df = concat_df(df1, df2)
        # Minor cleanup - remove duplicates 
        # drop_col_df(df, 'recall', axis=1, inplace=True) # b/c Recall == TPR, but lets keep for now in case ours are wrong
        drop_col_df(df, 'Counts', axis=1, inplace=True)
        df.rename({'support': 'counts'}, axis=1, inplace=True)
    else:
        df = pd.DataFrame(data=metrics, columns=labels, index=metric_names)
    
    # Output merged to xlsx
    filename = os.path.join(path, modelName + '_LocalMetrics.xlsx')
    output_df_to_excel(df, filename)

    ## Global Calculations
    # Macro - Calculate metrics for each label, and find their unweighted mean. This does not take label imbalance into account.
    macro_precision = precision_score(true_labels, predicted_labels, average='macro', zero_division=0)
    macro_recall = recall_score(true_labels, predicted_labels, average='macro', zero_division=0)
    macro_fpr = M_FPR
    macro_fnr = (np.nansum(FNR))/(len(labels))
    macro_f1 = f1_score(true_labels, predicted_labels, average='macro', zero_division=0)
    # Weighted - Calculate metrics for each label, and find their average weighted by support (the number of true instances for each label). This alters ‘macro’ to account for label imbalance; it can result in an F-score that is not between precision and recall. Weighted recall is equal to accuracy.
    weighted_precision = precision_score(true_labels, predicted_labels, average='weighted', zero_division=0)
    weighted_recall = recall_score(true_labels, predicted_labels, average='weighted', zero_division=0)
    weighted_fpr = W_FPR
    weighted_fnr = (np.nansum(W_FNR)/len(labels))
    weighted_f1 = f1_score(true_labels, predicted_labels, average='weighted', zero_division=0)
    acc = accuracy_score(true_labels, predicted_labels)

    metrics = [macro_precision, macro_recall, macro_fpr, macro_fnr, macro_f1, weighted_precision, weighted_recall, weighted_fpr, weighted_fnr, weighted_f1, acc]
    metric_names = [ 'Mean Precision', 'Mean Recall', 'Mean FPR', 'Mean FNR', 'Mean F1-score', 'Weighted Precision', 'Weighted Recall', 'Weighted FPR', 'Weighted FNR', 'Weighted F1-score', 'Accuracy']
    filename = os.path.join(path, modelName + '_GlobalMetrics.xlsx')
    col_length = len(metrics)
    metrics = np.array(metrics).reshape((1,col_length))
    df = pd.DataFrame(data=metrics, columns=metric_names, index=None)
    output_df_to_excel(df, filename)

    return metrics


def classification_report_to_dataframe(cr, cut_non_cweid_rows=True):
    """ 
        cr := string; get from sklearn API
        cut_non_cweid_rows    := bool; if true, then returns up to the last cwe id; useful for merging dataframes that need matching lengths.
                                 if false, includes the last 3 rows accuracy, macro, & weighted.
    """
    cr_lines = cr.split('\n')
    header = " ".join(cr_lines[0].split()).split()
    header.insert(0, 'id')
    cr_lines = cr_lines[2:-1]
    if cut_non_cweid_rows:
        cr_lines = cr_lines[:cr_lines.index('')]
    for i, line in enumerate(cr_lines):
        if line == '':
            continue
        if 'accuracy' in line:
            fill_line = line.split()
            fill_line.insert(1, '')
            fill_line.insert(1, '')
            cr_lines[i] = fill_line
            continue
        if 'avg' in line:
            line = line.replace(' avg', '_avg')
        cr_lines[i] = line.split()

    return pd.DataFrame(cr_lines, columns=header)

def print_missing_vals(df, name):
    is_missing_vals = df.isnull().values.any()
    print(f'DF Name: {name}\nMissing values: {is_missing_vals}')
    if is_missing_vals:
        sum_missing = df.isnull().sum()
        print('Sum of missing values: ', sum_missing)
        list_missing = df[df['TPR'].isnull()].index.tolist()
        print('Index of TPR\'s missing: ', list_missing)
        list_missing = df[df['FNR'].isnull()].index.tolist()
        print('Index of FNR\'s missing: ', list_missing)

def replace_missing_vals(df, replace_with=0):
    df.fillna(replace_with, inplace=True)
    assert(df.isnull().values.any() == False)
    print(f'Missing values have been replaced with {replace_with}')

def compare_series(df1, df2, df1_join_name='support', df2_join_name='Counts'):
    # Confirm indices match in both tables
    do_indices_match = df1.index.equals(df2.index)
    print(f'Indices match in both tables: {do_indices_match}')

    return do_indices_match

def concat_df(df1, df2, df1_join_name='support', df2_join_name='Counts'):
    print(f'Concatinating tables by column {df1_join_name} from df1 and column {df2_join_name} from df2...')
    # concat
    frames = [df2, df1]
    df = pd.concat(frames, axis=1)
    return df


# source:  https://www.kaggle.com/nkitgupta/evaluation-metrics-for-multi-class-classification?scriptVersionId=57461587&cellId=55
from sklearn.metrics import roc_auc_score

def roc_auc_score_multiclass(actual_class, pred_class, average = "macro"):
    
    #creating a set of all the unique classes using the actual class list
    unique_class = set(actual_class)
    roc_auc_dict = {}
    for per_class in unique_class:
        
        #creating a list of all the classes except the current class 
        other_class = [x for x in unique_class if x != per_class]

        #marking the current class as 1 and all other classes as 0
        new_actual_class = [0 if x in other_class else 1 for x in actual_class]
        new_pred_class = [0 if x in other_class else 1 for x in pred_class]

        #using the sklearn metrics method to calculate the roc_auc_score
        roc_auc = roc_auc_score(new_actual_class, new_pred_class, average = average)
        roc_auc_dict[per_class] = roc_auc

    return roc_auc_dict
