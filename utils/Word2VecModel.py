import numpy as np
import pandas as pd
from sklearn.base import BaseEstimator
from gensim.models.word2vec import Word2Vec

from utils.utils import getAvgLength, transformVLength

class Word2VecModel(BaseEstimator):
    
    def __init__(self, alpha=0.01, negative=10, epochs=1, sample=0.001, seed=1, vector_size=30, window=5, workers=10):
        # Grid search to find best params for vector_sizes, epochs, window
        # Once we have best params for this, test on dm_mean, dm_concat, & dbow_words
        self.w2v_model = None
        self.alpha = alpha
        self.negative = negative
        self.epochs = epochs
        self.sample = sample
        self.seed = seed
        self.vector_size = vector_size
        self.window = window
        self.workers = workers

    def fit(self, x, y=None):
        # Initialize model
        self.w2v_model = Word2Vec(sentences=x, size=self.vector_size, alpha=self.alpha, window=self.window, min_count=0, max_vocab_size=None, sample=self.sample, seed=self.seed, workers=self.workers, min_alpha=0.0001, sg=1, hs=0, negative=self.negative, iter=self.epochs)

        return self

    def transform(self, x):
        x_vectors = []
        not_found_token = np.zeros((30))
        for i in x:
            doc = []
            for j in i:
                try:
                    v = self.w2v_model[j]
                    doc.append(v)
                except Exception as e:
                    doc.append(not_found_token)
                    print(e)
            x_vectors.append(doc)
        
        avg = getAvgLength(x_vectors)
        x_vectors = transformVLength(x_vectors, avg)
        return x_vectors

    def fit_transform(self, x, y=None):
        self.fit(x)
        return self.transform(x)

