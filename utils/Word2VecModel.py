import numpy as np
from gensim.models.word2vec import Word2Vec

class Word2VecModel():
    
    def __init__(self, vector_size=33, alpha=0.01, negative=10, epochs=1, sample=0.001, seed=1, window=5, workers=10):
        # Grid search to find best params for vector_sizes, epochs, window
        # Once we have best params for this, test on dm_mean, dm_concat, & dbow_words
        self.w2v_model = None
        self.alpha = alpha
        self.negative = negative
        self.epochs = epochs
        self.sample = sample
        self.seed = seed
        self.vector_size = vector_size
        self.window = window
        self.workers = workers

    def fit(self, x, y=None):
        # Initialize model
        self.w2v_model = Word2Vec(sentences=x, size=self.vector_size, alpha=self.alpha, window=self.window, min_count=0, max_vocab_size=None, sample=self.sample, seed=self.seed, workers=self.workers, min_alpha=0.0001, sg=1, hs=0, negative=self.negative, iter=self.epochs)

        return self

    def transform(self, x):
        x_vectors = []
        not_found_token = []
        for i in range(x):
            document = x[i]
            doc = np.zeros((len(document), self.vector_size))
            for j in range(len(document)):
                word = x[i][j]
                try:
                    vector = self.w2v_model[word]
                    doc[i][j] = vector
                except Exception as e:
                    not_found_token.append(word)
                    print(f'This should not happen since we are doing all the data.  Word not found: {word}\n{e}')
            x_vectors.append(doc)

        if len(not_found_token) > 0:
            print('Words not found: ', not_found_token)
        
        return x_vectors

    def fit_transform(self, x, y=None):
        self.fit(x)
        return self.transform(x)

