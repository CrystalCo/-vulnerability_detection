import numpy as np
from sklearn.base import BaseEstimator
from gensim.models.word2vec import Word2Vec

from utils.utils import getAvgLength, transformVLength

class Word2VecModel(BaseEstimator):
    
    def __init__(self, vector_size=33, alpha=0.01, negative=10, epochs=1, sample=0.001, seed=1, window=5, workers=10):
        # Grid search to find best params for vector_sizes, epochs, window
        # Once we have best params for this, test on dm_mean, dm_concat, & dbow_words
        self.w2v_model = None
        self.alpha = alpha
        self.negative = negative
        self.epochs = epochs
        self.sample = sample
        self.seed = seed
        self.vector_size = vector_size
        self.window = window
        self.workers = workers
        import GlobalVars
        GlobalVars.vector_size_g = self.vector_size


    def fit(self, x, y=None):
        # Initialize model
        self.w2v_model = Word2Vec(sentences=x, size=self.vector_size, alpha=self.alpha, window=self.window, min_count=0, max_vocab_size=None, sample=self.sample, seed=self.seed, workers=self.workers, min_alpha=0.0001, sg=1, hs=0, negative=self.negative, iter=self.epochs)

        return self

    def transform(self, x):
        x_vectors = []
        not_found_token = np.zeros((self.vector_size))
        for i in x:
            doc = []
            for j in i:
                try:
                    v = self.w2v_model[j]
                    doc.append(v)
                except Exception as e:
                    doc.append(not_found_token)
                    print(e)
            x_vectors.append(doc)
        
        """
                Instead of choosing the average, we choose the 90th percentile of 
            vector length because we don't know where the focus pointer is in our samples.
            Without the focus pointer, we may miss passing in the vector representing the 
            vulnerable code word(s).
                Thus, by setting the cutoff to 90% we can rest assured that at least 80% of the 
            vectors will contain the focus pointer. See adjustVectorLength.py for more details
            on the focus pointer.
        """
        avg, vectorLengths = getAvgLength(x_vectors)
        percentile = int(len(vectorLengths) * (9/10))
        maxLength = max(avg, vectorLengths[percentile])
        import GlobalVars
        GlobalVars.max_len_g = maxLength
        GlobalVars.vector_size_g = self.vector_size
        x_vectors = transformVLength(x_vectors, maxLength, vector_dim=self.vector_size)
        print(f'Done transforming.  Max Length is {maxLength} and vector size is {self.vector_size}')
        assert len(x_vectors[-1][-1]) == self.vector_size == GlobalVars.vector_size_g, "vector length must match selected vector length"
        return x_vectors

    def fit_transform(self, x, y=None):
        self.fit(x)
        return self.transform(x)

