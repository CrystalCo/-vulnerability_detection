import os
import datetime
import re
import requests
from requests.adapters import HTTPAdapter, Retry

from bs4 import BeautifulSoup

"""
    A collection of tools that help with the preprocessing of the source code slices in order to use
    machine learning models to cluster vulnerability types and categorize them.

    PHASE 1: Collect test case IDs
        1)	Extract SARD IDs & CVE IDs from source code slice titles.
    PHASE 2: Scrape Internet for attributes
        2)	Extract CWE IDs from the SARD website.
        3)	Extract 'Name', ‘Description’, 'Abstraction', & ‘Relationship’ sections from the CWE website.
        4)  Extract CWE IDs for CVEs from multiple websites.
"""

BASE_SARD_URL = 'https://samate.nist.gov/SARD/'

def get_IDs_from_datafile(dataFile):
    data = open(dataFile, 'r')
    ids = [] 
    for line in data:
        ids.append(line.rstrip('\n'))
    data.close()
    # Remove header
    ids.pop(0)
    return ids

def get_ID_dict(dataFile):
    id_dict = dict()
    data = open(dataFile, 'r')
    next(data)  # Skip header
    
    for line in data:
        line = line.rstrip('\n')
        ids = line.split('\t')
        id_dict[ids[0]] = {
            'CWE-ID': ids[1],
            'vulnerable_line_nums': set()
        }

    data.close()
    
    return id_dict

def get_SARD_CVE_IDs(slicepath, numSamples=None):
    """
        Extract SARD test case IDs (for CWEs) & CVE IDs from source code slice titles.
        INPUT:
            slicepath := the path where the slice source files exist. Expects a str.
            numSamples := for testing on a small sample of slices. If None, all slices will be used. Expects int if not None.
        OUTPUT:
            Array of SARD test case IDs as strings.
            Array of CVE IDs as strings.
    """
    CVE_IDs = []
    SARD_IDs = []
    CVE_IDs_count = 0
    SARD_IDs_count = 0

    for filename in os.listdir(slicepath):
        SARD_IDs_by_file = set()
        CVE_IDs_by_file = set()
        if (filename.endswith(".txt") is False):
            continue
        filepath = os.path.join(slicepath, filename)
        f1 = open(filepath)
        slicelists = f1.read().split("------------------------------")
        f1.close()

        if slicelists[0] == '':
            del slicelists[0]
        if slicelists[-1] == '' or slicelists[-1] == '\n' or slicelists[-1] == '\r\n':
            del slicelists[-1]

        if (numSamples is not None):
            # limit number of slices scanned
            slicelists = slicelists[:numSamples]

        sard_local_count = 0   # number of SARD test case IDs found in the file
        cve_local_count = 0    # If missing SARD test case IDs, must be a CVE case
        total_slices_count = 0 
        for slicelist in slicelists:
            sentences = slicelist.split('\n')
            if sentences[0] == '\r' or sentences[0] == '':
                # Remove newlines that appear at the beginning of a title
                del sentences[0]
            if sentences == []:
                continue
            if sentences[-1] == '':
                del sentences[-1]
            if sentences[-1] == '\r':
                del sentences[-1]

            case_ID = sentences[0].split(" ")
            case_ID = case_ID[1].split("/")[0]

            if case_ID.isdigit():
                # Must be SARD ID
                SARD_IDs_by_file.add(case_ID)
                SARD_IDs.append(case_ID)
                sard_local_count += 1
            else:
                # Must be CVE
                CVE_IDs_by_file.add(case_ID)
                CVE_IDs.append(case_ID)
                cve_local_count += 1
            total_slices_count += 1

        CVE_IDs_count += cve_local_count
        SARD_IDs_count += sard_local_count

        print(f'Filename: {filename}')
        if (numSamples):
            print(f'SARD test case IDs by file: {SARD_IDs_by_file}')
            print(f'CVE test case IDs by file: {CVE_IDs_by_file}')
        
        print(f'SARD test case IDs count by file: {sard_local_count}')
        print(f'CVE test case IDs count by file: {cve_local_count}')
        print(f'Total slices extracted: {total_slices_count} \n')
    
    print('Total SARD test case IDs found in files: %s' % SARD_IDs_count)
    print('Total CVE test case IDs found in files: %s' % CVE_IDs_count)
    total = SARD_IDs_count + CVE_IDs_count
    print('Total slices found: %d' % total)
    return CVE_IDs, SARD_IDs

def get_CWEs_from_SARD(SARD_IDs, scrapeOut, dataOut):
    """
        Extract CWE IDs from the SARD website.
        INPUT:
            SARD_IDS := Expects an array of strings with the SARD test case IDs we want to extract from the SARD site.
            scrapeOut := Expects a string path to which we can write the data from the SARD site.
            dataOut := Expects TextIOWrapper (i.e. a file we can write out the results to).
        OUTPUT:
            Writes results to the dataOut file.
    """
    for SARD_ID in SARD_IDs:
        url = BASE_SARD_URL + 'view_testcase.php?tID=' + SARD_ID
        try:
            page = requests.get(url)
        except Exception as e:
            print(f"Exception with url {url}: {e}")
            continue

        set_scrape_out(scrapeOut, page)

        with open(scrapeOut, 'r') as f:
            cwe_id = _parse_CWEID_from_SARD_page(f)
            out = '\t'.join([SARD_ID, cwe_id])
            dataOut.write(out + '\n')

def get_page(url):
    page = None
    try:
        s = requests.Session()
        retries = Retry(total=2,
                        backoff_factor=0.2)
        s.mount('http://', HTTPAdapter(max_retries=retries))
        page = s.get(url)
        
    except Exception as e:
        print(f"{datetime.datetime.now()}  Exception with url {url}:\n{e}\n\n")
    finally:
        return page

def get_vul_line_num_from_SARD(SARD_IDs, scrapeout_path):
    """
        SARD_IDs        := a dictionary with SARD IDs as the keys
        scrapeout_path  := a path to a temporary output txt file
        Extracts vulnerable line number from the SARD website and the line with the vulnerability.
        Updates CWE-ID if they don't match.
    """
    count = 0
    for sard_id in SARD_IDs.keys():
        flawed_filepaths = set()
        nonflawed_filepaths = set()

        # Connect to SARD website:
        testcase_url = BASE_SARD_URL + f'view_testcase.php?tID={str(sard_id)}'
        page = get_page(testcase_url)
        if page == None:
            continue

        prettify_scrapeout(page, scrapeout_path)
        
        with open(scrapeout_path, 'r') as f:
            cwe_delim = '<a target="_blank" href="https://cwe.mitre.org/data/definitions/'
            flaw_delim = 'flaw_line_number'
            flaw_file_delim = 'testcases/'
            flaw_file_extensions = ['.c', '.cpp', '.cc']
            nonflaw_filename_delims = ['/io.c']

            for line in f:
                # Confirm CWE-IDs match
                if cwe_delim in line:
                    cwe_start = line.split(cwe_delim)[1]
                    cwe = cwe_start.split('.html')[0]  # CWE ID    ex. CWE-476
                    if cwe != SARD_IDs[sard_id]['CWE-ID']:
                        print('FOUND CWE-ID MISMATCH. Updating CWE-ID %s with %s for SARD ID %s' % (SARD_IDs[sard_id]['CWE-ID'], cwe, sard_id) )
                        SARD_IDs[sard_id]['CWE-ID'] = cwe

                # Parse "flaw_line_number" for line with vulnerable lines
                if flaw_delim in line:
                    flaw_num = line.split('(')[1].split(',')[0]
                    assert(flaw_num.isdigit())
                    SARD_IDs[sard_id]['vulnerable_line_nums'].add(int(flaw_num))

                # Open File(s) with vul code; can ignore: io.c,std_testcase.h, std_testcase_io.h files
                if ((flaw_file_delim in line) and (any (ext in line for ext in flaw_file_extensions))):
                    # clean the line
                    line = line.strip()
                    # extract the filepath of the vulnerable code
                    flaw_file_path = re.findall(r"['\"](.*?)['\"]", line)
                    # there should only be 1 filepath in a line thanks for SARD site's html struct and out prettify method
                    flaw_file_path = [file for file in flaw_file_path if flaw_file_delim in file][0]

                    if (nonflaw_filename_delims[0] not in flaw_file_path):
                        flawed_filepaths.add(BASE_SARD_URL + flaw_file_path)
                    else:
                        nonflawed_filepaths.add(flaw_file_path)

        
        count += 1
        
        if (count % 250 == 0):
            print(f'\n{datetime.datetime.now()}\tGetting vulnerable line number for SARD {sard_id}.  Count: {count}')
        
        # removing because some vul lines were comments
        # if len(flawed_filepaths) == 1:
            # get_flawed_lines_by_line_number(flawed_filepaths, SARD_IDs[sard_id])

        if len(flawed_filepaths) >= 1:
            # Save the line of code that contain the vulnerability
            get_flawed_lines_by_delim(flawed_filepaths, SARD_IDs[sard_id])
        else:
            # None found; log for review
            print('Could not find a valid filename for the vulnerable code in SARD %s associated with CWE-%s.  Outputting urls to separate file.' % (sard_id, SARD_IDs[sard_id]['CWE-ID']))
            log_path = os.path.join('data', '_data_logs', 'LOG_no_flawed_filepaths.txt')
            log_message = '%s\tNonFlawed URLS: %s\n\n' % (str(SARD_IDs[sard_id]), nonflawed_filepaths)
            with open(log_path, 'a') as f2:
                f2.write(log_message)

def get_flawed_lines_by_line_number(urls, sard_obj):
    """
        Finds vulnerable lines by their line number found in source.
        urls := array of string paths that lead to a SARD testcase's file that contains the vulnerable code
        sard_obj := a dictionary of an SARD ID that contains its vulnerable line number.
    """
    sard_obj['vulnerable_filenames'] = {}

    for url in urls:
        # get the vulnerable code text
        page = get_page(url)
        if page == None:
            continue

        # Parse by delim for files with only 1 filename, but > 1 line number in which 
        # the previous line number is neither the 1 line away or a comment 
        vul_index = [int(x) for x in sard_obj['vulnerable_line_nums']]
        vul_index = vul_index.sort()

        if len(vul_index) > 1 and (vul_index[1] - vul_index[0] != 1):
            get_flawed_lines_by_delim(urls, sard_obj)
            break

        # save flawed filename, flawed line and their previous line to the sard object
        vul_filepath = url.split('/')[-1]
        index = 0 # tracking index should be faster than calling .at() to get index each time 
        lines = page.text.splitlines()
        sard_obj['vulnerable_filenames'][vul_filepath] = {
                'line_before_vul': [],
                'vulnerable_lines': []
            }
        
        if len(vul_index) == 1:
            for line in lines:
                if index+1 == vul_index[0]:
                    # We've reached the vulnerable line
                    sard_obj['vulnerable_filenames'][vul_filepath]['line_before_vul'].append(lines[index-1].lstrip())
                    sard_obj['vulnerable_filenames'][vul_filepath]['vulnerable_lines'].append(line.lstrip())
                    break
                index += 1
        elif (len(vul_index) == 2) and (vul_index[1] - vul_index[0] == 1):
            # if >1 vul line number, check if previous starts with a comment
            for line in lines:
                index += 1
                if (index == vul_index[0]):
                    sard_obj['vulnerable_filenames'][vul_filepath]['line_before_vul'].append(line.lstrip())
                    sard_obj['vulnerable_filenames'][vul_filepath]['vulnerable_lines'].append(lines[index].lstrip())
                    break

def get_flawed_lines_by_delim(urls, sard_obj):
    """
        urls := array of string paths that lead to a SARD testcase's file that contains the vulnerable code
        sard_obj := a dictionary of an SARD ID
    """
    flaw_delims = ['/* POTENTIAL FLAW:', '/* FLAW:', '/* STONESOUP: ', '/* BAD']
    sard_obj['vulnerable_filenames'] = {}

    for url in urls:
        page = get_page(url)
        if page == None:
            continue

        vul_filepath = url.split('/')[-1]
        
        sard_obj['vulnerable_filenames'][vul_filepath] = {
                'line_before_vul': [],
                'vulnerable_lines': [],
                'vulnerable_line_nums': []
            }

        lines = page.text.splitlines()
        index = 0 # tracking index should be faster than calling .at() to get index each time 

        for line in lines:
            if any(delim in line for delim in flaw_delims):
                # Parse/save/write line
                delim_line = line.lstrip()

                # special case - delim on same line as vulnerability
                if '/* BAD */' in delim_line:
                    sard_obj['vulnerable_filenames'][vul_filepath]['line_before_vul'].append(lines[index-1].lstrip())
                    sard_obj['vulnerable_filenames'][vul_filepath]['vulnerable_lines'].append(delim_line)
                    sard_obj['vulnerable_filenames'][vul_filepath]['vulnerable_line_nums'].append(index + 1)
                    index+=1
                    continue

                # if next line is a comment too, merge it with this line and grab the third line as the vul line
                if ('; /* empty statement needed for some flow variants */' not in lines[index+1]) and  ('/*' in lines[index + 1]):
                    delim_line = delim_line + lines[index + 1].lstrip()
                    sard_obj['vulnerable_filenames'][vul_filepath]['line_before_vul'].append(delim_line)
                    sard_obj['vulnerable_filenames'][vul_filepath]['vulnerable_lines'].append(lines[index + 2].lstrip())
                    sard_obj['vulnerable_filenames'][vul_filepath]['vulnerable_line_nums'].append(index + 2)
                    index += 1
                    continue

                # otherwise this line is the prev line and next is vul line
                sard_obj['vulnerable_filenames'][vul_filepath]['line_before_vul'].append(delim_line)
                sard_obj['vulnerable_filenames'][vul_filepath]['vulnerable_lines'].append(lines[index + 1].lstrip())
                sard_obj['vulnerable_filenames'][vul_filepath]['vulnerable_line_nums'].append(index + 1)
                
            index += 1

def get_CWE_data(CWE_IDs, scrapeOut, dataOut):
    """
        Extract 'Name', ‘Description’, 'Abstraction', & ‘Relationship’ sections from the CWE website.
        INPUT:
            CWE_IDs := Expects an array of strings with the CWE ID we want from the CWE site.
            scrapeOut := Expects a string path to which we can write the data from the CWE site.
            dataOut := Expects TextIOWrapper (i.e. a file we can write out the results to).
        OUTPUT:
            Writes results to the dataOut file.
    """
    for CWE_ID in CWE_IDs:
        cid = str(int(CWE_ID)) # Strips 0s from the beginning of the CWE id
        url = 'https://cwe.mitre.org/data/definitions/%s.html' % cid
        page = requests.get(url)
        prettify_scrapeout(page, scrapeOut)
        
        with open(scrapeOut, 'r') as f:
            # [name, abstraction, description, relationships] 
            CWE_Data = _parse_CWE_page(f, cid)
            
            # ['CWE_ID', 'Name', 'Abstraction', 'Description', 'Relationships', 'URL']
            out = '\t'.join([cid, CWE_Data[0], CWE_Data[1], CWE_Data[2], str(CWE_Data[3]), url])
            dataOut.write(out + '\n')

def prettify_scrapeout(page, scrapeOut):
    """
        page        := a request response
        scrapeOut   := a txt file for which we want to output the prettified result of the response page
        
        Returns a cleaner output than set_scrape_out.
        Prettify divides up the lines based on html elements, whereas
        set_scrape_out leaves the output as is. Without prettify, multiples
        elements could be found on the same line, making it messier to parse.
    """
    soup = BeautifulSoup(page.content, 'html.parser')
    with open(scrapeOut, 'w') as f:
        try:
            f.write(soup.prettify())
        except Exception as e:
            raise e

def get_CVE_data(CVE_IDs, scrapeOut, dataOut):
    """
        Extract CWE IDs for CVEs from multiple websites.

        INPUT:
            CVE_IDs := Expects an array of strings of the CVE IDs.
            scrapeOut := Expects a string path to which we can write the data from the NIST & CVEDetails sites.
            dataOut := Expects TextIOWrapper (i.e. a file we can write out the results to).
        OUTPUT:
            Writes results to the dataOut file.
    """
    for CVE_ID in CVE_IDs:
        url1 = 'https://nvd.nist.gov/vuln/detail/%s' % CVE_ID
        url2 = 'https://www.cvedetails.com/cve/%s' % CVE_ID

        # parsing URL1
        page = requests.get(url1)
        set_scrape_out(scrapeOut, page)
        f = open(scrapeOut, 'r')
        CVE_Data1 = _parse_NIST_page(f)

        # parsing URL2
        page = requests.get(url2)
        set_scrape_out(scrapeOut, page)
        f = open(scrapeOut, 'r')
        CVE_Data2 = _parse_CVEDetail_page(f)

        # 'CVE_ID', 'URL1', 'Description1', 'CWE1', 'URL2', 'Description2', 'CWE2'
        out = '\t'.join([CVE_ID, url1, CVE_Data1[0], CVE_Data1[1], url2, CVE_Data2[0], CVE_Data2[1]])
        dataOut.write(out + '\n')

def _parse_CVEDetail_page(dataFile):
    description_delim = 'cvedetailssummary'
    cwe_id_delim1 = '<th>CWE ID'
    cwe_id_delim2 = 'CWE definition">'
    cwe_id_delim3 = '<td>'
    nextLine1 = False
    nextLine2 = False
    CVE_Data = []

    for line in dataFile:
        if description_delim in line:
            nextLine1 = True
            continue
        if nextLine1:
            description = line.split('\t')[1]
            CVE_Data.append(description)
            nextLine1 = False
            continue
        if cwe_id_delim1 in line:
            nextLine2 = True
            continue
        if nextLine2 and cwe_id_delim2 in line:
            cwe_start = line.split(cwe_id_delim2)[1]
            cwe = cwe_start.split('</a')[0]
            CVE_Data.append(cwe)
            nextLine2 = False
        elif nextLine2 and cwe_id_delim3 in line:
            cwe_start = line.split(cwe_id_delim3)[1]
            cwe = cwe_start.split('</td')[0]
            CVE_Data.append(cwe)
            nextLine2 = False
        
    return CVE_Data

def _parse_NIST_page(dataFile):
    description_delim = 'vuln-description">'
    cwe_id_delim1 = '<td data-testid="vuln-CWEs-link-0">'
    cwe_id_delim2 = 'CWE-'  # if CWE ID exists
    cwe_id_delim3 = '<span>'            # if CWE ID doesn't exist
    nextLine = False

    # 'Description1', 'CWE1'
    CVE_Data = []

    for line in dataFile:
        if description_delim in line:
            description_start = line.split(description_delim)[1]
            description = description_start.split('</p')[0]
            CVE_Data.append(description)
        if cwe_id_delim1 in line:
            nextLine = True
            continue
        if nextLine and cwe_id_delim3 in line:
            cwe_start = line.split(cwe_id_delim3)[1]
            cwe = cwe_start.split('</span')[0]
            CVE_Data.append(cwe)
            nextLine = False
        elif nextLine and cwe_id_delim2 in line:
            cwe_start = line.split(cwe_id_delim2)[1]
            cwe = cwe_start.split('</a')[0]
            CVE_Data.append(cwe)
            nextLine = False
    
    return CVE_Data

def _parse_CWEID_from_SARD_page(dataFile):
    cwe_delim = '<a target="_blank" href="https://cwe.mitre.org/data/definitions/'

    for line in dataFile:
        if cwe_delim in line:
            cwe_start = line.split(cwe_delim)[1]
            cwe = cwe_start.split('.html')[0]
            return cwe   # CWE ID    ex. CWE-476

def _parse_CWE_page(dataFile, CWE_ID):
    name_delim = 'text-bottom">'
    name_flag = False
    abstraction_delim = 'Abstraction:'
    abstraction_flag = False
    description_delim = 'oc_%s_Description' % CWE_ID
    description_flag = False
    relationships_delim = 'reltable'
    relationships_count = 0             # Only want the 1st instance of the relationship table, as the first one is typically `Relevant to the view "Research Concepts" (CWE-1000)`, which is the table we want.
    rel_delim = 'primary Weakness'
    rel_id_delim = '<a href="/data/definitions/'
    relationships_flag = False
    relationship_ids = {
        'ChildOf': [],
        'ParentOf': []
    }
    nature = ''
    line_counter = 0

    # name, abstraction, description, relationship ids
    CWE_Data = ["", "", "", {}]

    for line in dataFile:
        if name_delim in line:
            name_flag = True
            continue
        if name_flag:
            name = line.split(': ')[1].rstrip()
            CWE_Data[0] = name
            name_flag = False
            continue
        if abstraction_delim in line:
            abstraction_flag = True
            continue
        if abstraction_flag and line_counter < 1:
            line_counter += 1
            continue
        elif abstraction_flag:
            CWE_Data[1] = line.strip()
            abstraction_flag = False
            continue
        if description_delim in line:
            description_flag = True
            line_counter = 0
            continue
        if description_flag and line_counter < 2:
            line_counter += 1
            continue
        elif description_flag:
            CWE_Data[2] = line.strip()
            description_flag = False
            continue

        if relationships_delim in line:
            relationships_count += 1
            continue
        if relationships_count == 1:
            if rel_delim in line: # new Relationship
                line_counter = 0  # reset line counter
                relationships_flag = True
                continue
            if relationships_flag and line_counter < 1:
                line_counter += 1
                continue
            elif relationships_flag and line_counter == 1:
                # Get Nature of Relationship (either 'ChildOf' or 'ParentOf')
                nature = line.strip()
                line_counter += 1
                continue
            elif relationships_flag and rel_id_delim in line and nature in relationship_ids:
                # Get ID of relationship as long as its a 'ChildOf' or 'ParentOf'
                rel_id = line.split(rel_id_delim)[1]
                rel_id = rel_id.split('.')[0]
                relationship_ids[nature].append(rel_id)
                # Reset flags and variables to indicate end of the extraction of a relationship's attributes 
                relationships_flag = False
                nature = ''
                continue

    CWE_Data[3] = relationship_ids
    return CWE_Data

def set_scrape_out(scrapeOut, page):
    """
        Writes source lines from a scraped webpage (page) to a designated file (scrapeOut).
    """
    with open(scrapeOut, 'w') as f:
        for line in page.text:
            try:
                f.write(line.rstrip('\n'))
            except Exception as e:
                # ignore except error for unidentified characters in html code
                print(f"Exception with writing to scrapeOut file: {e}")
                pass
        # File is automatically closed 

def set_scrapeOut_to_dataOut(scrapeOut, dataOut, IDs, header, get_data_method):
    """
        Extracts the information we need from a file that contains source code (scrapeOut) & 
        inserts it into a tab dilineated file (dataOut).
        INPUT:
            scrapeOut   := txt file with html scraped from a website
            dataOut     := txt file to output the extracted info wanted from scrapeOut, to be separated by tabs
            IDs         := set of IDs for the method being used (e.g. SARD, CWE, CVE)
            header      := header for the dataOut file
            get_data_method := method that will retrieve the html from a web that'll be transformed into the desired dataOut format.
                               requires arguments: IDs, scrapeOut, & dataOut
        OUTPUT:
            Writes results to the dataOut file.
    """
    dataOut = open(dataOut, 'w')
    dataOut.write(header + '\n')
    get_data_method(IDs, scrapeOut, dataOut)
    dataOut.close()

def set_dataOut(dataOut, header, data):
    """
        Tool that outputs an array (data) to a txt file (dataOut) split per line.
    """
    dataOut = open(dataOut, 'w')
    dataOut.write(header + '\n')
    for d in data:
        dataOut.write(d + '\n')
    dataOut.close()

def set_original_id(og_df, og_id_title, new_df):
    """
        Appends the original SARD or CVE IDs to the CWE ID.
        INPUT:
            og_df       := original dataframe (expects cve_df or sard_cwe_df)
            og_id_title := title of the column that contains the original ID (e.g. 'CVE_ID' or 'SARD_ID')
            new_df      := the df we wish to modify (expects cwe_df)
        OUTPUT:
            None;   Modifies the new_df
    """
    og_ids_added = 0
    
    for i, row in og_df.iterrows():
        if row['CWE_ID'] == 'NaN':
            continue
        
        og_id = str(row[og_id_title])
        cwe_id = row['CWE_ID']        
        if type(cwe_id) == str:
            cwe_id = int(cwe_id)
        
        if ('original_ids' not in new_df.loc[cwe_id]) or (('original_ids' in new_df.loc[cwe_id]) and (type(new_df.loc[cwe_id, 'original_ids']) != str)):
            new_df.loc[cwe_id, 'original_ids'] = og_id
            og_ids_added += 1
        else: 
            og_ids = new_df.loc[cwe_id, 'original_ids']
            if og_id not in og_ids:
                new_df.loc[cwe_id, 'original_ids'] = new_df.loc[cwe_id, 'original_ids'] + ' ' + og_id
            
    print(f'New column added to {og_ids_added} rows.')

def test__get_SARD_CVE_IDs(slicePath, numSamples=None):
    """
        1. Collect CVE & SARD test case IDs.
        INPUT:
            slicepath := the path where the slice source files exist. Expects a str.
            numSamples := for testing on a small sample of slices. If None, all slices will be used. Expects int if not None.
        OUTPUT:
            Array of SARD test case IDs as strings.
            Array of CVE IDs as strings.
    """
    CVE_IDs, SARD_IDs = get_SARD_CVE_IDs(slicePath, numSamples)
    if numSamples:
        print('Complete set of SARD test case IDs: %s' % SARD_IDs)
        print('Complete set of CVE test case IDs: %s' % CVE_IDs)
        print()
    return CVE_IDs, SARD_IDs

def test__get_CVE_data(isTest, CVE_IDs, numSamples=None):
    """
        Saves CVE data scraped from the web into a tab delimited format on a txt file.
        Running the entire script with isTest = False will save all the CVE IDs to its own txt file for debugging purposes.
        In case we need to debug only this portion, we can later set isTest to True after having those CVE IDs stored
        in order to speed up the testing.

        INPUT:
            isTest := boolean. False will use CVE_IDs passed in.  True will use a subset of CVE_IDs from CVE_IDs.txt
            CVE_IDs := an array of CVE IDs. Can be empty if isTest = True.
            numSamples := the amount of CVE IDs we want to use for testing from the subset of CVE IDs in CVE_IDs.txt
        OUTPUT:
            CVE data scraped from the web into a tab delimited format on the dataOut file.
    """
    if isTest and type(numSamples) != int:
        raise Exception("numSamples value must be an int > 0 if this is a test.")

    scrapeOut = './data/_temporaries/scrapeout.txt'
    dataOut = './data/CVE/CVE_Data.txt'
    header = '\t'.join(['CVE_ID', 'URL1', 'Description1', 'CWE1', 'URL2', 'Description2', 'CWE2'])
    if not isTest:
        set_scrapeOut_to_dataOut(scrapeOut, dataOut, CVE_IDs, header, get_CVE_data)
    else:
        cve_id_dataFile = './data/CVE/CVE_IDs.txt'
        CVE_samples = get_IDs_from_datafile(cve_id_dataFile)
        if len(CVE_samples) > 0:
            CVE_samples = CVE_samples[:numSamples]
            set_scrapeOut_to_dataOut(scrapeOut, dataOut, CVE_samples, header, get_CVE_data)
        else:
            raise Exception("No CVE IDs found in %s.  Run this file in non-test mode first to generate CVE IDs, then retry in test mode." % cve_id_dataFile)

def test__debug():
    isTest = input("Turn on debug mode? Type 'True' or 'False'. ")
    if isTest == 'True':
        return True
    else:
        return False


