import os
import requests

from bs4 import BeautifulSoup

"""
    A collection of tools that help with the preprocessing of the source code slices in order to use
    machine learning models to cluster vulnerability types and categorize them.

    PHASE 1: Collect test case IDs
        1)	Extract SARD IDs & CVE IDs from source code slice titles.
    PHASE 2: Scrape Internet for attributes
        2)	Extract CWE IDs from the SARD website.
        3)	Extract 'Name', ‘Description’, 'Abstraction', & ‘Relationship’ sections from the CWE website.
        4)  Extract CWE IDs for CVEs from multiple websites.
"""
def get_IDs_from_datafile(dataFile):
    data = open(dataFile, 'r')
    ids = [] 
    for line in data:
        ids.append(line.rstrip('\n'))
    data.close()
    # Remove header
    ids.pop(0)
    return ids

def get_ID_dict(dataFile):
    id_dict = dict()
    data = open(dataFile, 'r')
    next(data)  # Skip header
    
    for line in data:
        line = line.rstrip('\n')
        ids = line.split('\t')
        id_dict[ids[0]] = {
            'CWE-ID': ids[1],
            'vulnerable_line_num': None
        }

    data.close()
    
    return id_dict

def get_SARD_CVE_IDs(slicepath, numSamples=None):
    """
        Extract SARD test case IDs (for CWEs) & CVE IDs from source code slice titles.
        INPUT:
            slicepath := the path where the slice source files exist. Expects a str.
            numSamples := for testing on a small sample of slices. If None, all slices will be used. Expects int if not None.
        OUTPUT:
            Array of SARD test case IDs as strings.
            Array of CVE IDs as strings.
    """
    CVE_IDs = []
    SARD_IDs = []
    CVE_IDs_count = 0
    SARD_IDs_count = 0

    for filename in os.listdir(slicepath):
        SARD_IDs_by_file = set()
        CVE_IDs_by_file = set()
        if (filename.endswith(".txt") is False):
            continue
        filepath = os.path.join(slicepath, filename)
        f1 = open(filepath)
        slicelists = f1.read().split("------------------------------")
        f1.close()

        if slicelists[0] == '':
            del slicelists[0]
        if slicelists[-1] == '' or slicelists[-1] == '\n' or slicelists[-1] == '\r\n':
            del slicelists[-1]

        if (numSamples is not None):
            # limit number of slices scanned
            slicelists = slicelists[:numSamples]

        sard_local_count = 0   # number of SARD test case IDs found in the file
        cve_local_count = 0    # If missing SARD test case IDs, must be a CVE case
        total_slices_count = 0 
        for slicelist in slicelists:
            sentences = slicelist.split('\n')
            if sentences[0] == '\r' or sentences[0] == '':
                # Remove newlines that appear at the beginning of a title
                del sentences[0]
            if sentences == []:
                continue
            if sentences[-1] == '':
                del sentences[-1]
            if sentences[-1] == '\r':
                del sentences[-1]

            case_ID = sentences[0].split(" ")
            case_ID = case_ID[1].split("/")[0]

            if case_ID.isdigit():
                # Must be SARD ID
                SARD_IDs_by_file.add(case_ID)
                SARD_IDs.append(case_ID)
                sard_local_count += 1
            else:
                # Must be CVE
                CVE_IDs_by_file.add(case_ID)
                CVE_IDs.append(case_ID)
                cve_local_count += 1
            total_slices_count += 1

        CVE_IDs_count += cve_local_count
        SARD_IDs_count += sard_local_count

        print(f'Filename: {filename}')
        if (numSamples):
            print(f'SARD test case IDs by file: {SARD_IDs_by_file}')
            print(f'CVE test case IDs by file: {CVE_IDs_by_file}')
        
        print(f'SARD test case IDs count by file: {sard_local_count}')
        print(f'CVE test case IDs count by file: {cve_local_count}')
        print(f'Total slices extracted: {total_slices_count} \n')
    
    print('Total SARD test case IDs found in files: %s' % SARD_IDs_count)
    print('Total CVE test case IDs found in files: %s' % CVE_IDs_count)
    total = SARD_IDs_count + CVE_IDs_count
    print('Total slices found: %d' % total)
    return CVE_IDs, SARD_IDs

def get_CWEs_from_SARD(SARD_IDs, scrapeOut, dataOut):
    """
        Extract CWE IDs from the SARD website.
        INPUT:
            SARD_IDS := Expects an array of strings with the SARD test case IDs we want to extract from the SARD site.
            scrapeOut := Expects a string path to which we can write the data from the SARD site.
            dataOut := Expects TextIOWrapper (i.e. a file we can write out the results to).
        OUTPUT:
            Writes results to the dataOut file.
    """
    for SARD_ID in SARD_IDs:
        url = 'https://samate.nist.gov/SARD/view_testcase.php?tID=' + SARD_ID
        try:
            page = requests.get(url)
        except Exception as e:
            print(f"Exception with url {url}: {e}")
            continue

        set_scrape_out(scrapeOut, page)

        with open(scrapeOut, 'r') as f:
            cwe_id = _parse_SARD_page(f)
            out = '\t'.join([SARD_ID, cwe_id])
            dataOut.write(out + '\n')


def get_vul_line_num_from_SARD(SARD_IDs):
    """
        Extract vulnerable line number that the SARD website.
    """
    for sard_id in SARD_IDs:
        base_url = 'https://samate.nist.gov/SARD/view_testcase.php?tID=%s' % str(sard_id)

        try:
            page = requests.get(base_url)
        except Exception as e:
            print(f"Exception with url {base_url}: {e}")
            continue


    

def get_CWE_data(CWE_IDs, scrapeOut, dataOut):
    """
        Extract 'Name', ‘Description’, 'Abstraction', & ‘Relationship’ sections from the CWE website.
        INPUT:
            CWE_IDs := Expects an array of strings with the CWE ID we want from the CWE site.
            scrapeOut := Expects a string path to which we can write the data from the CWE site.
            dataOut := Expects TextIOWrapper (i.e. a file we can write out the results to).
        OUTPUT:
            Writes results to the dataOut file.
    """
    for CWE_ID in CWE_IDs:
        cid = str(int(CWE_ID)) # Strips 0s from the beginning of the CWE id
        url = 'https://cwe.mitre.org/data/definitions/%s.html' % cid
        page = requests.get(url)
        prettify_scrapeout(page, scrapeOut)
        
        with open(scrapeOut, 'r') as f:
            # [name, abstraction, description, relationships] 
            CWE_Data = _parse_CWE_page(f, cid)
            
            # ['CWE_ID', 'Name', 'Abstraction', 'Description', 'Relationships', 'URL']
            out = '\t'.join([cid, CWE_Data[0], CWE_Data[1], CWE_Data[2], str(CWE_Data[3]), url])
            dataOut.write(out + '\n')

def prettify_scrapeout(page, scrapeOut):
    """
        page        := a request response
        scrapeOut   := a txt file for which we want to output the prettified result of the response page
    """
    soup = BeautifulSoup(page.content, 'html.parser')
    with open(scrapeOut, 'w') as f:
        try:
            f.write(soup.prettify())
        except Exception as e:
            raise e

def get_CVE_data(CVE_IDs, scrapeOut, dataOut):
    """
        Extract CWE IDs for CVEs from multiple websites.

        INPUT:
            CVE_IDs := Expects an array of strings of the CVE IDs.
            scrapeOut := Expects a string path to which we can write the data from the NIST & CVEDetails sites.
            dataOut := Expects TextIOWrapper (i.e. a file we can write out the results to).
        OUTPUT:
            Writes results to the dataOut file.
    """
    for CVE_ID in CVE_IDs:
        url1 = 'https://nvd.nist.gov/vuln/detail/%s' % CVE_ID
        url2 = 'https://www.cvedetails.com/cve/%s' % CVE_ID

        # parsing URL1
        page = requests.get(url1)
        set_scrape_out(scrapeOut, page)
        f = open(scrapeOut, 'r')
        CVE_Data1 = _parse_NIST_page(f)

        # parsing URL2
        page = requests.get(url2)
        set_scrape_out(scrapeOut, page)
        f = open(scrapeOut, 'r')
        CVE_Data2 = _parse_CVEDetail_page(f)

        # 'CVE_ID', 'URL1', 'Description1', 'CWE1', 'URL2', 'Description2', 'CWE2'
        out = '\t'.join([CVE_ID, url1, CVE_Data1[0], CVE_Data1[1], url2, CVE_Data2[0], CVE_Data2[1]])
        dataOut.write(out + '\n')

def _parse_CVEDetail_page(dataFile):
    description_delim = 'cvedetailssummary'
    cwe_id_delim1 = '<th>CWE ID'
    cwe_id_delim2 = 'CWE definition">'
    cwe_id_delim3 = '<td>'
    nextLine1 = False
    nextLine2 = False
    CVE_Data = []

    for line in dataFile:
        if description_delim in line:
            nextLine1 = True
            continue
        if nextLine1:
            description = line.split('\t')[1]
            CVE_Data.append(description)
            nextLine1 = False
            continue
        if cwe_id_delim1 in line:
            nextLine2 = True
            continue
        if nextLine2 and cwe_id_delim2 in line:
            cwe_start = line.split(cwe_id_delim2)[1]
            cwe = cwe_start.split('</a')[0]
            CVE_Data.append(cwe)
            nextLine2 = False
        elif nextLine2 and cwe_id_delim3 in line:
            cwe_start = line.split(cwe_id_delim3)[1]
            cwe = cwe_start.split('</td')[0]
            CVE_Data.append(cwe)
            nextLine2 = False
        
    return CVE_Data

def _parse_NIST_page(dataFile):
    description_delim = 'vuln-description">'
    cwe_id_delim1 = '<td data-testid="vuln-CWEs-link-0">'
    cwe_id_delim2 = 'CWE-'  # if CWE ID exists
    cwe_id_delim3 = '<span>'            # if CWE ID doesn't exist
    nextLine = False

    # 'Description1', 'CWE1'
    CVE_Data = []

    for line in dataFile:
        if description_delim in line:
            description_start = line.split(description_delim)[1]
            description = description_start.split('</p')[0]
            CVE_Data.append(description)
        if cwe_id_delim1 in line:
            nextLine = True
            continue
        if nextLine and cwe_id_delim3 in line:
            cwe_start = line.split(cwe_id_delim3)[1]
            cwe = cwe_start.split('</span')[0]
            CVE_Data.append(cwe)
            nextLine = False
        elif nextLine and cwe_id_delim2 in line:
            cwe_start = line.split(cwe_id_delim2)[1]
            cwe = cwe_start.split('</a')[0]
            CVE_Data.append(cwe)
            nextLine = False
    
    return CVE_Data

def _parse_SARD_page(dataFile):
    cwe_delim = '<a target="_blank" href="https://cwe.mitre.org/data/definitions/'

    for line in dataFile:
        if cwe_delim in line:
            cwe_start = line.split(cwe_delim)[1]
            cwe = cwe_start.split('.html')[0]
            return cwe   # CWE ID    ex. CWE-476

def _parse_CWE_page(dataFile, CWE_ID):
    name_delim = 'text-bottom">'
    name_flag = False
    abstraction_delim = 'Abstraction:'
    abstraction_flag = False
    description_delim = 'oc_%s_Description' % CWE_ID
    description_flag = False
    relationships_delim = 'reltable'
    relationships_count = 0             # Only want the 1st instance of the relationship table, as the first one is typically `Relevant to the view "Research Concepts" (CWE-1000)`, which is the table we want.
    rel_delim = 'primary Weakness'
    rel_id_delim = '<a href="/data/definitions/'
    relationships_flag = False
    relationship_ids = {
        'ChildOf': [],
        'ParentOf': []
    }
    nature = ''
    line_counter = 0

    # name, abstraction, description, relationship ids
    CWE_Data = ["", "", "", {}]

    for line in dataFile:
        if name_delim in line:
            name_flag = True
            continue
        if name_flag:
            name = line.split(': ')[1].rstrip()
            CWE_Data[0] = name
            name_flag = False
            continue
        if abstraction_delim in line:
            abstraction_flag = True
            continue
        if abstraction_flag and line_counter < 1:
            line_counter += 1
            continue
        elif abstraction_flag:
            CWE_Data[1] = line.strip()
            abstraction_flag = False
            continue
        if description_delim in line:
            description_flag = True
            line_counter = 0
            continue
        if description_flag and line_counter < 2:
            line_counter += 1
            continue
        elif description_flag:
            CWE_Data[2] = line.strip()
            description_flag = False
            continue

        if relationships_delim in line:
            relationships_count += 1
            continue
        if relationships_count == 1:
            if rel_delim in line: # new Relationship
                line_counter = 0  # reset line counter
                relationships_flag = True
                continue
            if relationships_flag and line_counter < 1:
                line_counter += 1
                continue
            elif relationships_flag and line_counter == 1:
                # Get Nature of Relationship (either 'ChildOf' or 'ParentOf')
                nature = line.strip()
                line_counter += 1
                continue
            elif relationships_flag and rel_id_delim in line and nature in relationship_ids:
                # Get ID of relationship as long as its a 'ChildOf' or 'ParentOf'
                rel_id = line.split(rel_id_delim)[1]
                rel_id = rel_id.split('.')[0]
                relationship_ids[nature].append(rel_id)
                # Reset flags and variables to indicate end of the extraction of a relationship's attributes 
                relationships_flag = False
                nature = ''
                continue

    CWE_Data[3] = relationship_ids
    return CWE_Data

def set_scrape_out(scrapeOut, page):
    """
        Writes source lines from a scraped webpage (page) to a designated file (scrapeOut).
    """
    with open(scrapeOut, 'w') as f:
        for line in page.text:
            try:
                f.write(line)
            except Exception as e:
                # ignore except error for unidentified characters in html code
                print(f"Exception with writing to scrapeOut file: {e}")
                pass

def set_scrapeOut_to_dataOut(scrapeOut, dataOut, IDs, header, get_data_method):
    """
        Extracts the information we need from a file that contains source code (scrapeOut) & 
        inserts it into a tab dilineated file (dataOut).
        INPUT:
            scrapeOut   := txt file with html scraped from a website
            dataOut     := txt file to output the extracted info wanted from scrapeOut, to be separated by tabs
            IDs         := set of IDs for the method being used (e.g. SARD, CWE, CVE)
            header      := header for the dataOut file
            get_data_method := method that will retrieve the html from a web that'll be transformed into the desired dataOut format.
                               requires arguments: IDs, scrapeOut, & dataOut
        OUTPUT:
            Writes results to the dataOut file.
    """
    dataOut = open(dataOut, 'w')
    dataOut.write(header + '\n')
    get_data_method(IDs, scrapeOut, dataOut)
    dataOut.close()

def set_dataOut(dataOut, header, data):
    """
        Tool that outputs an array (data) to a txt file (dataOut) split per line.
    """
    dataOut = open(dataOut, 'w')
    dataOut.write(header + '\n')
    for d in data:
        dataOut.write(d + '\n')
    dataOut.close()

def set_original_id(og_df, og_id_title, new_df):
    """
        Appends the original SARD or CVE IDs to the CWE ID.
        INPUT:
            og_df       := original dataframe (expects cve_df or sard_cwe_df)
            og_id_title := title of the column that contains the original ID (e.g. 'CVE_ID' or 'SARD_ID')
            new_df      := the df we wish to modify (expects cwe_df)
        OUTPUT:
            None;   Modifies the new_df
    """
    og_ids_added = 0
    
    for i, row in og_df.iterrows():
        if row['CWE_ID'] == 'NaN':
            continue
        
        og_id = str(row[og_id_title])
        cwe_id = row['CWE_ID']        
        if type(cwe_id) == str:
            cwe_id = int(cwe_id)
        
        if ('original_ids' not in new_df.loc[cwe_id]) or (('original_ids' in new_df.loc[cwe_id]) and (type(new_df.loc[cwe_id, 'original_ids']) != str)):
            new_df.loc[cwe_id, 'original_ids'] = og_id
            og_ids_added += 1
        else: 
            og_ids = new_df.loc[cwe_id, 'original_ids']
            if og_id not in og_ids:
                new_df.loc[cwe_id, 'original_ids'] = new_df.loc[cwe_id, 'original_ids'] + ' ' + og_id
            
    print(f'New column added to {og_ids_added} rows.')

def test__get_SARD_CVE_IDs(slicePath, numSamples=None):
    """
        1. Collect CVE & SARD test case IDs.
        INPUT:
            slicepath := the path where the slice source files exist. Expects a str.
            numSamples := for testing on a small sample of slices. If None, all slices will be used. Expects int if not None.
        OUTPUT:
            Array of SARD test case IDs as strings.
            Array of CVE IDs as strings.
    """
    CVE_IDs, SARD_IDs = get_SARD_CVE_IDs(slicePath, numSamples)
    if numSamples:
        print('Complete set of SARD test case IDs: %s' % SARD_IDs)
        print('Complete set of CVE test case IDs: %s' % CVE_IDs)
        print()
    return CVE_IDs, SARD_IDs

def test__get_CVE_data(isTest, CVE_IDs, numSamples=None):
    """
        Saves CVE data scraped from the web into a tab delimited format on a txt file.
        Running the entire script with isTest = False will save all the CVE IDs to its own txt file for debugging purposes.
        In case we need to debug only this portion, we can later set isTest to True after having those CVE IDs stored
        in order to speed up the testing.

        INPUT:
            isTest := boolean. False will use CVE_IDs passed in.  True will use a subset of CVE_IDs from CVE_IDs.txt
            CVE_IDs := an array of CVE IDs. Can be empty if isTest = True.
            numSamples := the amount of CVE IDs we want to use for testing from the subset of CVE IDs in CVE_IDs.txt
        OUTPUT:
            CVE data scraped from the web into a tab delimited format on the dataOut file.
    """
    if isTest and type(numSamples) != int:
        raise Exception("numSamples value must be an int > 0 if this is a test.")

    scrapeOut = './data/CVE/scrapeOut.txt'
    dataOut = './data/CVE/CVE_Data.txt'
    header = '\t'.join(['CVE_ID', 'URL1', 'Description1', 'CWE1', 'URL2', 'Description2', 'CWE2'])
    if not isTest:
        set_scrapeOut_to_dataOut(scrapeOut, dataOut, CVE_IDs, header, get_CVE_data)
    else:
        cve_id_dataFile = './data/CVE/CVE_IDs.txt'
        CVE_samples = get_IDs_from_datafile(cve_id_dataFile)
        if len(CVE_samples) > 0:
            CVE_samples = CVE_samples[:numSamples]
            set_scrapeOut_to_dataOut(scrapeOut, dataOut, CVE_samples, header, get_CVE_data)
        else:
            raise Exception("No CVE IDs found in %s.  Run this file in non-test mode first to generate CVE IDs, then retry in test mode." % cve_id_dataFile)

def test__debug():
    isTest = input("Turn on debug mode? Type 'True' or 'False'. ")
    if isTest == 'True':
        return True
    else:
        return False


