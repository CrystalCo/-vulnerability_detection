import os

from get_CWE_methods import get_ID_dict, get_vul_line_num_from_SARD
from utils import get_SARD, get_sentences, get_vul_filename, levenshtein_ratio_and_distance


# Write a script that cross checks the vulnerability label and line 

# How are we going to verify if a slice is labeled correctly?
# 2 Cases:
#   Case 1) SARD
#       - We already have a script that extracts info from the SARD website.
#       - That confirms the type of vulnerability.
#       - We already have a list of SARD test case IDs we can use to scrape again.




# Pull SARD IDs
# Using CWE_IDs.txt because SARD_IDs.txt contains duplicates
cwe_ids_file = os.path.join('data', 'CWE', 'CWE_IDs.txt')
#SARD_IDs = get_ID_dict(cwe_ids_file)

# test data
SARD_IDs = {
    '152959': {
        'CWE-ID': '824',
        'vulnerable_line_num': None
    },
    '120623': {
        'CWE-ID': '78',
        'vulnerable_line_num': None
    }, 
    '94200': {
        'CWE-ID': '319',
        'vulnerable_line_num': None
    }
}

scrapeout_path = os.path.join('data', '_temporaries', 'scrapeout.txt')
get_vul_line_num_from_SARD(SARD_IDs, scrapeout_path)
print(SARD_IDs)

# Save SARD_IDs dict
scrapeout_path = os.path.join('data', '_temporaries', 'sard_vulnerabilities.txt')
with open(scrapeout_path, 'w') as f:
    f.write(str(SARD_IDs))

# Next step, once we have all the vul line numbers per SARD,
# walk through the parsed, raw files,
nonvulnerable_filepath = os.path.join('data', 'slicesByLabel', 'nonvulnerable_slices.txt')
vulnerable_filepath = os.path.join('data', 'slicesByLabel', 'vulnerable_slices.txt')
splitter = '------------------------------\n'

# slicepath = os.path.join('data', 'slicesSource')
# for filename in os.listdir(slicepath):
#     if(filename.endswith('.txt') is False):
#         continue
#     print('Slice File To be Processed: ', filename)
#     f1 = open(os.path.join(slicepath, filename), 'r')

# TEST
filename = os.path.join('data', 'slicesSource', 'Arithmetic expression.txt')
f1 = open(filename, 'r')
slices = f1.read().split(splitter)# split each slice 
f1.close()

# Remove whitespace buffers in first and last lines of slice
if slices[0] == '':
    del slices[0]
if slices[-1] == '' or slices[-1] == '\n' or slices[-1] == '\r\n':
    del slices[-1]

for vul_slice in slices:
    sentences = get_sentences(vul_slice)
    # get header, which contains the filename and SARD number
    sard = get_SARD(sentences[0])   # returns a string
    vul_filename = get_vul_filename(sentences[0])

    # get vulnerable label (for stats)
    label_from_slice = sentences[-1].strip()
    matching_label_count = 0 
    nonmatching_label_count = 0 

    # match the SARD & filename with the ones in our dictionary
    if sard not in SARD_IDs.keys():
        continue

    vulnerable_obj = None if not SARD_IDs[sard]['vulnerable_filenames'][vul_filename] else SARD_IDs[sard]['vulnerable_filenames'][vul_filename]

    # Skip CVE slices
    if vulnerable_obj == None:
        continue

    isVulnerable = False


    # Check if this slice is vulnerable - validate vulnerable syntax in there
    for vulnerable_line in vulnerable_obj['vulnerable_lines']:
        for sentence in sentences:
            #  strip all spaces in both strings to check for equality
            vul_line_stripped = vulnerable_line.replace(' ', '')
            sentence_stripped = sentence.replace(' ', '')

            if vul_line_stripped == sentence_stripped:
                isVulnerable = True
                
                # Update stats
                if label_from_slice == '1':
                    matching_label_count += 1
                else:
                    nonmatching_label_count += 1
                
                # Ensure slice is correctly labeled
                sentences[-1] = '1'
                break
            
            # In case the vulnerable line is in the code but doesn't exactly match the stripped version, compute the similarity distance
            similarity_distance = levenshtein_ratio_and_distance(vul_line_stripped, sentence_stripped, True)

            # TODO: Test a sample batch if 0.95 is a good confidence threshold
            # Check using slices that have the vulnerable line vs their fixed line
            if similarity_distance > 0.95:
                isVulnerable = True

                if label_from_slice == '1':
                    matching_label_count += 1
                else:
                    nonmatching_label_count += 1
                
                # Ensure slice is correctly labeled
                sentences[-1] = '1'
                break
        if isVulnerable:
            break

    # piece sentences back together as a slice
    sentences.append(splitter)
    new_slice = '\n'.join(sentences)

    if isVulnerable:
        # Put the slices that contain vulnerabilities in one file
        with open(vulnerable_filepath, 'a') as f:
            f.write(new_slice)
    else:
        # place other slices in separate file
        with open(nonvulnerable_filepath, 'a') as f:
            f.write(new_slice)





