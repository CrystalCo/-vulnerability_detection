import gc, os, pickle

import numpy as np

from utils.utils import init_nested_arrays


def combine_vectors_from_files(vectorPath, vectorTypePath, randomSeed=None, max_samples=None):
    """
        Combines vectors into one file & saves it into a single pkl file.

        vectorPath          := str; the vector path desired. 'data/vector/' for W2V vectors, 'data/vectorD2V/' for D2V vectors
        vectorTypePath := str; the path to save the output file
        randomSeed          := int; if None, does not randomly shuffle the files
        max_samples         := int; if None, combines all the vectors, else stops at this number.  Helpful for debugging.
    """
    print('Combining vectors into a single file...')
    count = 0
    folders = os.listdir(vectorPath)
    group_set = [] # [list of tokens, labels, function list in each program, filenames, vulnerability types, group ids, test case ids]

    # collect metadata from each sample
    for folder in folders:
        if count == max_samples:
            break
        if folder.endswith('DS_Store'):
            continue

        dirs = os.path.join(vectorPath, folder)
        for filename in os.listdir(dirs): #./data/vector/8090
            filepath = os.path.join(dirs, filename)
            f = open(filepath, 'rb')
            data = pickle.load(f)
            if group_set == []:
                group_set = init_nested_arrays(len(data))
            isVul = data[1][0]
            groupid = 0 if not isVul else data[-2][0][1] # assign non-vulnerable samples to group ID 0
            data[-2][0][1] = groupid
            for n in range(len(data)):
                group_set[n].append(data[n][0])

        if group_set[0] == []:
            continue
        count += 1
        if count % 10000 == 0:
            print(f'Done with {count} folders.')

    total_samples = len(group_set[-1])
    print('Total samples: ', total_samples)

    if randomSeed is not None:
        for i in range(len(group_set)):
            np.random.seed(randomSeed)
            np.random.shuffle(group_set[i])
    
    allpath = os.path.join(vectorTypePath, 'ALL_vectors.pkl')
    f_train = open(allpath, 'wb')#  './data/MLvectors/ALL_vectors.pkl'
    pickle.dump(group_set, f_train, protocol=pickle.HIGHEST_PROTOCOL)
    f_train.close()
    del group_set
    gc.collect()
    print(f'All vectors saved in {allpath}\n')


