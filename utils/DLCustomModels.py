import os

import keras
import tensorflow as tf
from keras.models import Sequential
from keras.layers.core import Masking, Dense, Dropout
from keras.layers.recurrent import GRU, LSTM
from keras.layers.wrappers import Bidirectional

"""
    These functions allow us to use Sklearn's Grid Search function, 
    by wrapping these functions in the Keras Classifer, and passing the 
    keras classifier to GridSearch as our estimator.

    In order for the scoring method to work in grid search, 
    the metrics in these functions must be set to 'accuracy'.
"""

def create_bgru_model(density=9, maxlen=100, vector_size=33, layers=2, dropout=0.1, optimizer='adam', activation='sigmoid', metrics=['accuracy']):
    print(f'\nBuild BGRU Model with maxlen {maxlen} and vector size {vector_size}')
    model = Sequential()
    model.add(Masking(mask_value=0.0, input_shape=(maxlen, vector_size)))
    for _ in range(1, layers):
        model.add(Bidirectional(GRU(units=256, activation='sigmoid', recurrent_activation='hard_sigmoid', return_sequences=True)))
        model.add(Dropout(dropout))
    model.add(Bidirectional(GRU(units=256, activation='sigmoid', recurrent_activation='hard_sigmoid')))
    model.add(Dropout(dropout))
    model.add(Dense(density, activation=activation))
    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=metrics)
    model.summary()

    return model

def create_blstm_model(maxlen=100, density=9, vector_size=30, layers=2, dropout=0.1, optimizer='adam', activation='sigmoid', metrics=['accuracy']):
    print('\nBuild model...')
    model = Sequential()
    model.add(Masking(mask_value=0.0, input_shape=(maxlen, vector_size)))
    for i in range(1, layers):
        model.add(Bidirectional(LSTM(units=256, activation='sigmoid', recurrent_activation='sigmoid', use_bias=True, kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal', bias_initializer='zeros', unit_forget_bias=True, kernel_regularizer=None, recurrent_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, recurrent_constraint=None, bias_constraint=None, dropout=0.0, recurrent_dropout=0.0, implementation=2, return_sequences=True, return_state=False, go_backwards=False, stateful=False, unroll=False)))
    model.add(Bidirectional(LSTM(units=256, activation='sigmoid', recurrent_activation='hard_sigmoid')))
    model.add(Dropout(dropout))
    model.add(Dense(density, activation=activation))          
    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=metrics)
    model.summary()
 
    return model    


"""
    See https://keras.io/guides/distributed_training/ for more details.
"""
def make_or_restore_model(checkpoint_dir, get_compiled_model):
    # Either restore the latest model, or create a fresh one
    # if there is no checkpoint available.
    checkpoints = [checkpoint_dir + "/" + name for name in os.listdir(checkpoint_dir)]
    if checkpoints:
        latest_checkpoint = max(checkpoints, key=os.path.getctime)
        print("Restoring from", latest_checkpoint)
        return keras.models.load_model(latest_checkpoint)
    print("Creating a new model")
    return get_compiled_model()


def run_training(checkpoint_dir, get_compiled_model):
    # Create a MirroredStrategy.
    strategy = tf.distribute.MirroredStrategy()

    # Open a strategy scope and create/restore the model
    with strategy.scope():
        model = make_or_restore_model(checkpoint_dir, get_compiled_model)

    callbacks = [
        # This callback saves a SavedModel every epoch
        # We include the current epoch in the folder name.
        keras.callbacks.ModelCheckpoint(
            filepath=checkpoint_dir + "/ckpt-{epoch}", save_freq="epoch"
        )
    ]
    model.fit(
        # train_dataset,
        # epochs=epochs,
        callbacks=callbacks,
        # validation_data=val_dataset,
        verbose=2,
    )

