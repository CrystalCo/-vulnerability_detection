import os, datetime

import numpy as np
import pandas as pd
import keras
import tensorflow as tf
from keras.models import Sequential, Input
from keras.layers.core import Masking, Dense, Dropout
from keras.layers.recurrent import LSTM,GRU
from keras.layers.wrappers import Bidirectional

from SYSE_1_isVulnerable.preprocess_dl_Input_version5 import process_sequences_shape
from utils.transformDataDimensions import generator_of_data
from utils.utils import GetDataXY, get_latest_epoch, get_predicted_class_and_accuracy, map_pred_to_cat, output_df_to_excel


# These functions allow us to use Sklearn's Grid Search function,
# by wrapping these functions in the Keras Classifer, and passing the
# keras classifier to GridSearch as our estimator.

# In order for the scoring method to work in grid search,
# the metrics in these functions must be set to 'accuracy'.

def create_bgru_model(avg=100, vector_size=30, layers=2, dropout=0.2, optimizer='adam', density=16, activation_fn='sigmoid', dense_activation_fn='softmax', recurrent_activation='hard_sigmoid', metrics=['CategoricalAccuracy', 'Recall'], mask=True, units=256):
    print(f'\nBuild BGRU Model with avg {avg} and vector size {vector_size}')

    # A Sequential model is appropriate for a plain stack of layers where each layer has exactly one input tensor and one output tensor. 
    # In mathematics, a tensor is an algebraic object that describes a multilinear relationship between sets of algebraic objects related to a vector space.
    model = Sequential()

    # All layers in Keras need to know the shape of their inputs in order to be able to create their weights. (avg*30)
    # In input_shape, you don't need to specify batch size.  Instead takes in (timesteps, features) => (avg, vector_size) => (1, 6420).  When fitting with generator, the shape will be (batch_size, timesteps, features) => (64, 1, 6420).
    # We can't do shape=(vector_size,) ==> (None, vector_size) === (1, vector_size), because Bidirectional layers require 3D+ input shape
    model.add(Input(shape=(avg, vector_size)))

    # The masking layer will skip any token rows with all zeros. 
    # Dimensions of tokens transformed using W2V cause variations in dimensions per sample, 
    # which then get padded with zeros if the # of tokens in that program are < the avg size).
    if mask:
        model.add(Masking(mask_value=0.0, input_shape=(avg, vector_size)))

    # layers are used to construct a deeplearning model; tensors are used to define the dataflow thru the model
    # layers=2 in our case, since end exclusive, so we should have only 1 layer in this for loop.
    # This is our first layer which takes in the original input sequence.
    # Sigmoid activation for multiclass labels.
    # Dropout should be added if our model had high training accuracy but poor validation accuracy (aka over-fitting).
    for _ in range(1, layers):
        # A RNN layer can also return the entire sequence of outputs for each sample (one vector per timestep per sample), if you set return_sequences=True (default = False). The shape of this output is (batch_size, timesteps, units).
        # The output of GRU will be a 3D tensor of shape (batch_size, timesteps, units) == (64, 1, 256) b/c return_seq=T
        # Note that LSTM has 2 state tensors, but GRU only has one. Probably doesn't matter though since we have return_state=False.  If set to True, can be used to resume the RNN execution later, or to initialize another RNN. This setting is commonly used in the encoder-decoder sequence-to-sequence model, where the encoder final state is used as the initial state of the decoder.
        model.add(Bidirectional(GRU(units=units, activation=activation_fn, recurrent_activation=recurrent_activation, return_sequences=True)))
        model.add(Dropout(dropout))
    model.add(Bidirectional(GRU(units=units, activation=activation_fn, recurrent_activation=recurrent_activation)))
    model.add(Dropout(dropout))

    # Dense() outputs a function which takes a tensor as input and outputs a tensor.
    # 2D input of (batch_size, input_dim) would return (batch_size, units)
    # For example, if our input is (1, 7280), our output would be (64, 256)
    # where 30 = vector_size and 9 is the number of units we want as output from the density layer
    model.add(Dense(density, activation=dense_activation_fn))
    # the optimizer's learning_rate: A Tensor, floating point value, or a schedule that is a tf.keras.optimizers.schedules.LearningRateSchedule, or a callable that takes no arguments and returns the actual value to use. The learning rate. Defaults to 0.001.
    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=metrics)
    model.summary()

    return model

def create_blstm_model(avg=100, vector_size=30, layers=2, dropout=0.1, optimizer='adam', density=9, activation_fn='softmax', dense_activation_fn='softmax', recurrent_activation='hard_sigmoid', metrics=['accuracy'], mask=True, units=256):
    print(f'\nBuild BLSTM Model with input dimensions {avg} * vector size {vector_size}')
    model = Sequential()
    model.add(Input(shape=(avg, vector_size)))
    if mask:
        model.add(Masking(mask_value=0.0, input_shape=(avg, vector_size)))
    for _ in range(1, layers):
        model.add(Bidirectional(LSTM(units=units, 
        activation=activation_fn, recurrent_activation=recurrent_activation, 
        implementation=2, return_sequences=True)))
    model.add(Bidirectional(LSTM(units=units, activation=activation_fn, recurrent_activation=recurrent_activation)))
    model.add(Dropout(dropout))
    model.add(Dense(density, activation=dense_activation_fn))
    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=metrics)
    model.summary()
 
    return model


def fit_custom_dl_model(myKerasModel, weightpath, traindataSet_path, batch_size, maxlen, vector_dim, RANDOMSEED, output_signature_shape_y, epochs=10, callbacks=None, latest_epoch=0):
    """
        Fit with multiclass labels.
        Keras model .fit and .compile methods will handle the distribution of dataset across replicas 
        for us with MirroredStrategy. https://www.tensorflow.org/api_docs/python/tf/distribute/MirroredStrategy
    """
    print('\nFitting model with training set...')
    X, y = GetDataXY(traindataSet_path, y_index=-2, randomseed=RANDOMSEED)
    assert(len(X) == len(y))
    return fit_with_generator(myKerasModel, weightpath, X, y, batch_size, maxlen, vector_dim, epochs, callbacks, latest_epoch, output_signature_shape_y)

def fit_with_generator(myKerasModel, weightpath, X, y, batch_size, avg, vector_dim, epochs, callbacks, latest_epoch, output_signature_shape_y):
    # Requires arrays to be of type numpy in order to determine the dimensions
    x_train = np.array(X)
    y = np.array(y)
    assert x_train.shape[0] == len(X), f'x_train converted to array, but 1st dimension number of samples {x_train.shape[0]} != length of X {len(X)}'
    assert x_train.shape[1] == avg, f'x_train converted to array, but 2nd dimension row length {x_train.shape[1]} != calculated {avg}'
    if (type(x_train) != np.ndarray or type(x_train[0]) != np.ndarray or type(x_train[0][0]) != np.ndarray):
        try:
            x_train = np.reshape(x_train, (len(X), avg, vector_dim))
        except:
            print('x_train to array did not reshape properly. Might mean our data is ragged and should be checked bc at this point the row lengths should be the same (avg). Will try process_sequences_shape next, but lets confirm our shapes are even:')
            print(f'Len data: {len(x_train)} should be # of samples')
            print(f'Len data[0]: {len(x_train[0])} should be # of vector size')
            x_train = process_sequences_shape(X, avg, vector_dim)
            pass
    print(f'Shape of x_train: ', x_train.shape)
    
    train_generator = generator_of_data(x_train, y, batch_size, avg, vector_dim)
    steps_epoch = int(len(x_train) / batch_size)
    options = tf.data.Options()
    options.experimental_distribute.auto_shard_policy = tf.data.experimental.AutoShardPolicy.DATA
    x_train = tf.data.Dataset.from_generator(lambda: train_generator,
                                        # The elements generated by generator must be compatible with the given output_signature argument
                                        output_signature=(
                                            tf.TensorSpec(shape=(batch_size, avg, vector_dim), dtype=tf.float32),    # Is this the Input shape or X shape?   I think it is the shape of what is getting returned from our generator
                                            tf.TensorSpec(shape=(batch_size, output_signature_shape_y), dtype=tf.float32) # Is this the Dense shape or y shape? I think it is the 2nd shape of what is getting returned from our generator
                                        ))
    x_train = x_train.with_options(options)
    return fit_by_epoch(myKerasModel, weightpath, x_train, epochs, callbacks, latest_epoch, steps_epoch)

def fit_by_epoch(myKerasModel, weightpath, X, epochs, callbacks, latest_epoch, steps_epoch):
    t1 = datetime.datetime.now()
    print(f'Fitting model start: {t1}')
    myKerasModel.fit(X, steps_per_epoch=steps_epoch, epochs=epochs, callbacks=callbacks, initial_epoch=latest_epoch)
    t2 = datetime.datetime.now()
    train_time = t2 - t1
    print(f'Fitting model time total: {train_time}\n')
    myKerasModel.save_weights(weightpath)
    return myKerasModel

def predictMulticlassLabel(mymodel, X, y, avg, vecdim):
    print('\nPredicting....')
    print(f'Length of dataset: {len(X)}\nAvg: {avg}\tVector size: {vecdim}')

    # Predict entire test set
    try:
        x_test = np.reshape(X, (len(X), avg, vecdim))
    except Exception as e:
        print(f'Error: {e}. Going to reshape using process_sequence_shape method...')
        x_test = process_sequences_shape(X, avg, vecdim)
    y = np.array(y)
    print(f'Shape of x_test: {x_test.shape}')
    t1 = datetime.datetime.now()
    y_pred_thresholds = mymodel.predict(x=x_test, verbose=1)
    t2 = datetime.datetime.now()
    train_time = t2 - t1
    print(f'Prediction time total: {train_time}\n')

    # Evaluate - get thresholds
    outputs = mymodel.evaluate(x_test, y)
    if not isinstance(outputs, list):
        outputs = [outputs]
    for name, output in zip(mymodel.metrics_names, outputs):
        print(f'{name} on predicted test dataset: {output}')

    y_pred_thresholds, y_pred = get_predicted_class_and_accuracy(y_pred_thresholds)
    return y, y_pred, y_pred_thresholds

def decode_predictions(y_test, y_pred, labelEncoder):
    """Expects the LabelBinarizer as the labelEncoder"""
    #transform the real labels file to the same shape of predicted for model evaluation
    decoded_y_test = labelEncoder.inverse_transform(y_test)
    decoded_y_pred = map_pred_to_cat(y_pred, labelEncoder)
    assert decoded_y_pred.shape == decoded_y_test.shape, f'Predicted array shape {decoded_y_pred.shape} should match True label array shape {decoded_y_test.shape}'
    return decoded_y_test, decoded_y_pred

def output_predictions_summary(decoded_y_test, decoded_y_pred, output_y_pred, modelname, vtype_labels=None, testids=None):
    if (vtype_labels is not None and testids is not None):
        d = {'TestID': testids, 'DLOutput': output_y_pred, 'PredLabel': decoded_y_pred, 'RealLabel': decoded_y_test, 'Vtype': vtype_labels}
    else:
        d = {'DLOutput': output_y_pred, 'PredLabel': decoded_y_pred, 'RealLabel': decoded_y_test}

    df = pd.DataFrame(data=d)
    model = modelname.split('_')[0]
    filename = os.path.join('metrics', model.lower(), 'OutputSummary_' + modelname + '_ClassificationReport.xlsx')
    output_df_to_excel(df, filename)
 
    return output_y_pred, decoded_y_pred, decoded_y_test



def make_or_restore_model(checkpoint_dir, get_compiled_model):
    """
        Either restore the latest model, or create a fresh one if there is no checkpoint available.
        See https://keras.io/guides/distributed_training/ for more details. 
    """
    checkpoints = [checkpoint_dir + "/" + name for name in os.listdir(checkpoint_dir)]
    if checkpoints:
        latest_checkpoint = max(checkpoints, key=os.path.getctime)
        print("Restoring from", latest_checkpoint)
        epoch = get_latest_epoch(latest_checkpoint)
        print("Latest epoch: ", epoch)
        return int(epoch), keras.models.load_model(latest_checkpoint)
    print("Creating a new model")
    return 0, get_compiled_model()

def fit_without_generator(myKerasModel, weightpath, X_train, y_train, batch_size, avg, vector_dim, epochs, callbacks, latest_epoch=0):
    # Make sure you have enough training samples to do it this way.
    try:
        x_train = np.reshape(X_train, (len(X_train), avg, vector_dim))
    except:
        x_train = process_sequences_shape(X_train, avg, vector_dim)
        pass
    y_train = np.array(y_train)    
    options = tf.data.Options()
    options.experimental_distribute.auto_shard_policy = tf.data.experimental.AutoShardPolicy.DATA
    dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))
    dataset = dataset.with_options(options)
    dataset = dataset.batch(batch_size)
    steps_epoch = int(len(x_train)  / batch_size)
    return fit_by_epoch(myKerasModel, weightpath, dataset, epochs, callbacks, latest_epoch, steps_epoch)


