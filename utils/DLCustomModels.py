import os
import pickle
import datetime

import numpy as np
import pandas as pd
import keras
import tensorflow as tf
from keras.models import Sequential, Input
from keras.layers.core import Masking, Dense, Dropout
from keras.layers.recurrent import LSTM,GRU
from keras.layers.wrappers import Bidirectional

from SYSE_1_isVulnerable.preprocess_dl_Input_version5 import process_sequences_shape
from utils.transformDataDimensions import generator_of_data, transformInput
from utils.utils import get_latest_epoch, get_predicted_class_and_accuracy, map_pred_to_cat


# These functions allow us to use Sklearn's Grid Search function,
# by wrapping these functions in the Keras Classifer, and passing the
# keras classifier to GridSearch as our estimator.

# In order for the scoring method to work in grid search,
# the metrics in these functions must be set to 'accuracy'.

# Activation functions set to sigmoid as is best for multiclass classification.


def create_bgru_model(maxlen=100, vector_size=30, layers=2, dropout=0.1, optimizer='adam', density=9, activation_fn='sigmoid', metrics=['accuracy'], mask=True):
    print(f'\nBuild BGRU Model with maxlen {maxlen} and vector size {vector_size}')

    # A Sequential model is appropriate for a plain stack of layers where each layer has exactly one input tensor and one output tensor. 
    # In mathematics, a tensor is an algebraic object that describes a multilinear relationship between sets of algebraic objects related to a vector space.
    model = Sequential()

    # All layers in Keras need to know the shape of their inputs in order to be able to create their weights. (avg*30)
    # In input_shape, you don't need to specify batch size.  Instead takes in (timesteps, features) --> (avg, 30)
    model.add(Input(shape=(maxlen, vector_size)))

    # The masking layer will skip any token rows with all zeros. 
    # Dimensions of tokens transformed using W2V cause variations in dimensions per sample, 
    # which then get padded with zeros if the # of tokens in that program are < the avg size).
    if mask:
        model.add(Masking(mask_value=0.0, input_shape=(maxlen, vector_size)))

    # layers are used to construct a deeplearning model; tensors are used to define the dataflow thru the model
    # layers=2 in our case, since end exclusive, so we should have only 1 layer in this for loop. 
    # This is our first layer which takes in the original input sequence.
    # Sigmoid activation for multiclass labels.
    for _ in range(1, layers):
        model.add(Bidirectional(GRU(units=256, activation='sigmoid', recurrent_activation='hard_sigmoid', return_sequences=True)))
        model.add(Dropout(dropout))
    model.add(Bidirectional(GRU(units=256, activation='sigmoid', recurrent_activation='hard_sigmoid')))
    model.add(Dropout(dropout))

    # Dense() outputs a function which takes a tensor as input and outputs a tensor.
    # 2D input of (batch_size, input_dim) would return (batch_size, units)
    # For example, if our input is (25, 30), our output would be (25, 9)
    model.add(Dense(density, activation=activation_fn))
    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=metrics)
    model.summary()

    return model

def create_blstm_model(maxlen=100, vector_size=30, layers=2, dropout=0.1, optimizer='adam', density=9, activation_fn='sigmoid', metrics=['accuracy'], mask=True):
    print(f'\nBuild BLSTM Model with maxlen {maxlen} and vector size {vector_size}')
    model = Sequential()
    model.add(Input(shape=(maxlen, vector_size)))
    if mask:
        model.add(Masking(mask_value=0.0, input_shape=(maxlen, vector_size)))
    for _ in range(1, layers):
        model.add(Bidirectional(LSTM(units=256, activation='sigmoid', recurrent_activation='sigmoid', use_bias=True, kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal', bias_initializer='zeros', unit_forget_bias=True, kernel_regularizer=None, recurrent_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, recurrent_constraint=None, bias_constraint=None, dropout=0.0, recurrent_dropout=0.0, implementation=2, return_sequences=True, return_state=False, go_backwards=False, stateful=False, unroll=False)))
    model.add(Bidirectional(LSTM(units=256, activation='sigmoid', recurrent_activation='hard_sigmoid')))
    model.add(Dropout(dropout))
    model.add(Dense(density, activation=activation_fn))
    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=metrics)
    model.summary()
 
    return model


def fit_custom_dl_model(myKerasModel, weightpath, traindataSet_path, batch_size, maxlen, vector_dim, RANDOMSEED, epochs=10, useGenerator=True, callbacks=None, latest_epoch=0):
    """
        Fit with multiclass labels.
        Keras model .fit and .compile methods will handle the distribution of dataset across replicas 
        for us with MirroredStrategy. https://www.tensorflow.org/api_docs/python/tf/distribute/MirroredStrategy
    """
    print('\nFitting model with training set...')
    dataset = []
    labels = []

    for filename in os.listdir(traindataSet_path):
        if filename.endswith(".pkl"):
            print('filename: ',filename)
            f = open(os.path.join(traindataSet_path, filename),"rb")
            data = pickle.load(f)
            f.close()
            dataset = data[0]
            labels = data[-2]
    print(len(dataset), len(labels))
    print(labels)

    np.random.seed(RANDOMSEED)
    np.random.shuffle(dataset)
    np.random.seed(RANDOMSEED)
    np.random.shuffle(labels)

    if useGenerator:
        return fit_with_generator(myKerasModel, weightpath, dataset, labels, batch_size, maxlen, vector_dim, epochs, callbacks, latest_epoch)
    return fit_without_generator(myKerasModel, weightpath, dataset, labels, batch_size, maxlen, vector_dim, epochs, callbacks)


def fit_with_generator(myKerasModel, weightpath, dataset, labels, batch_size, maxlen, vector_dim, epochs, callbacks, latest_epoch):
    # Requires arrays to be of type numpy in order to determine the dimensions
    # dataset = np.array(dataset)
    try:
        dataset = np.reshape(dataset, (len(dataset), maxlen, vector_dim))
    except:
        dataset = process_sequences_shape(dataset, maxlen, vector_dim)
        pass
    
    train_generator = generator_of_data(dataset, labels, batch_size, maxlen, vector_dim)
    steps_epoch = int(len(dataset) / batch_size)
    options = tf.data.Options()
    options.experimental_distribute.auto_shard_policy = tf.data.experimental.AutoShardPolicy.DATA
    dataset = tf.data.Dataset.from_generator(lambda: train_generator, 
                                        output_signature=(
                                            tf.TensorSpec(shape=(batch_size, maxlen, vector_dim), dtype=tf.float32),
                                            tf.TensorSpec(shape=(batch_size, len(labels[0])), dtype=tf.float32) 
                                        ))
    dataset = dataset.with_options(options)
    return fit_by_epoch(myKerasModel, weightpath, dataset, epochs, callbacks, latest_epoch, steps_epoch)

def fit_without_generator(myKerasModel, weightpath, X_train, y_train, batch_size, maxlen, vector_dim, epochs, callbacks, latest_epoch=0):
    # Make sure you have enough training samples to do it this way.
    try:
        x_train = np.reshape(X_train, (len(X_train), maxlen, vector_dim))
    except:
        x_train = process_sequences_shape(X_train, maxlen, vector_dim)
        pass
    y_train = np.array(y_train)    
    options = tf.data.Options()
    options.experimental_distribute.auto_shard_policy = tf.data.experimental.AutoShardPolicy.DATA
    dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))
    dataset = dataset.with_options(options)
    dataset = dataset.batch(batch_size)
    steps_epoch = int(len(x_train)  / batch_size)
    return fit_by_epoch(myKerasModel, weightpath, dataset, epochs, callbacks, latest_epoch, steps_epoch)

def fit_by_epoch(myKerasModel, weightpath, dataset, epochs, callbacks, latest_epoch, steps_epoch):
    t1 = datetime.datetime.now()
    print(f'Fitting model start: {t1}')
    myKerasModel.fit(dataset, steps_per_epoch=steps_epoch, epochs=epochs, callbacks=callbacks, initial_epoch=latest_epoch)
    t2 = datetime.datetime.now()
    train_time = t2 - t1
    print(f'Fitting model time total: {train_time}\n')
    myKerasModel.save_weights(weightpath)
    return myKerasModel

def make_or_restore_model(checkpoint_dir, get_compiled_model):
    """
        Either restore the latest model, or create a fresh one if there is no checkpoint available.
        See https://keras.io/guides/distributed_training/ for more details. 
    """
    checkpoints = [checkpoint_dir + "/" + name for name in os.listdir(checkpoint_dir)]
    if checkpoints:
        latest_checkpoint = max(checkpoints, key=os.path.getctime)
        print("Restoring from", latest_checkpoint)
        epoch = get_latest_epoch(latest_checkpoint)
        print("Latest epoch: ", epoch)
        return int(epoch), keras.models.load_model(latest_checkpoint)
    print("Creating a new model")
    return 0, get_compiled_model()


def predictMulticlassLabel(mymodel, testpath, maxlen, vecdim, myoptimizer, modelname, randomSeed, labelEncoder, saveOutput=True):
    print('\nPredicting....')
    dataset = []
    y_test = []
    predicted_labels = []
    output_y_pred = []
    vtype_labels = []
    for filename in os.listdir(testpath):
        if filename.endswith(".pkl"):
            f = open(os.path.join(testpath, filename),"rb")
            data = pickle.load(f)
            f.close()
            dataset = data[0]
            y_test = data[-2]
            vtype_labels = data[4]

    print(f'Length of dataset: {len(dataset)}\nAvg: {maxlen}')
    if (len(dataset) == 1):#predict 1 program
        oneprogram = dataset[0]
        y_pred = mymodel.predict(x=transformInput(oneprogram , maxlen, vecdim), batch_size=1)
    else:
        try:
            x_test = np.reshape(dataset, (len(dataset), maxlen, vecdim))
        except Exception as e:
            print(f'Error: {e}. Going to reshape using process_sequence_shape method...')
            x_test = process_sequences_shape(dataset, maxlen, vecdim)
        y_pred = mymodel.predict(x=x_test, verbose=1)

    outputs = mymodel.evaluate(x_test, y_test)
    outputs_dict = dict()
    if not isinstance(outputs, list):
        outputs = [outputs]
    for name, output in zip(mymodel.metrics_names, outputs):
        print(f'{name} on predicted test dataset: {output}')
        outputs_dict[name] = output

    output_y_pred, predicted_labels = get_predicted_class_and_accuracy(y_pred)

    #transform the real labels file to the same shape of predicted for model evaluation
    decoded_y_test = labelEncoder.inverse_transform(y_test)
    decoded_y_pred = map_pred_to_cat(predicted_labels, labelEncoder)
    assert decoded_y_pred.shape == decoded_y_test.shape, f'Predicted array shape {decoded_y_pred.shape} should match True label array shape {decoded_y_test.shape}'

    if saveOutput:
        d = {'TestID': data[-1], 'DLOutput':output_y_pred, 'PredLabel': decoded_y_pred, 'RealLabel': decoded_y_test, 'Vtype': vtype_labels}
        df = pd.DataFrame(data=d)
        df.to_excel('OutputSummary_' + modelname + '.xlsx')
 
    return output_y_pred, decoded_y_pred, decoded_y_test, outputs_dict


