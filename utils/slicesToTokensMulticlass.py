import os
import pickle

from SYSE_1_isVulnerable.mapping import mapping, create_tokens
from utils.utils import get_SARD, get_sentences, get_focus_pointer, get_multiclass_label, get_type

def tokenizeSlices_Multiclass(slicepath, corpuspath, multiclasspath, totalSamples, saveIndividualFiles=True):
    """
        Converts the words & symbols from the samples into tokens 
        that will be used to create vectors.  Expects multiclass labels.
    
        Trying to differentiate between non vulnerable and vulnerability types returned imbalanced data 
        (356,121 non-vulnerable out of 420,627 samples), and low performance scores for training and testing, 
        even with downsampling.
        Thus, since we are trying to predict vulnerability types, we will omit non-vulnerable samples.
        
        tokenizeSlices() and its corresponding file `2_Application_Codes.py` perform significantly well at 
        predicting whether a code slice has a vulnerability or not. 
        This method and the files where it is used in should be considered a supplement, like a 2nd phase
        to run samples that were flagged as vulnerable in the first phase `2_Application_Codes.py`.
        Models perform best when done sequentially instead of trying to predict vulnerability status AND vulnerability type if applicable.
    """

    all_data = [ [], [], [], [], [], [], [] ]
    count = 0

    for filename in os.listdir(slicepath):
        if(filename.endswith(".txt") is False):
            continue
        cnt = tokenizeSlicesPerFile(slicepath, filename, corpuspath, multiclasspath, totalSamples, all_data, saveIndividualFiles)
        count += cnt

    all_tokensPath = os.path.join(corpuspath, '..', '..', 'tokens', 'ALL_tokens.pkl') # save to data/tokens/ALL_tokens.pkl
    save_tokens_to_file(all_data, all_tokensPath, count)

def tokenizeSlicesPerFile(slicepath, filename, corpuspath, multiclasspath, totalSamples, all_data, saveIndividualFiles=True):
    """
        all_data := an array of nested arrays which will hold our tokenized slices. 
        Ex. [ [], [], [], [], [], [], [] ]
    """
    print("Slice File To be Processed: ", filename)
    myType = get_type(filename)
    savingFile = open(os.path.join(slicepath, filename), 'r')
    slicelists = savingFile.read().split("------------------------------")#aa. split each slide (each program in .txt file)
    savingFile.close()

    if slicelists[0] == '':
        del slicelists[0]
    if slicelists[-1] == '' or slicelists[-1] == '\n' or slicelists[-1] == '\r\n':
        del slicelists[-1]

    count = 0
    for slicelist in slicelists:
        if count == totalSamples:
            break
        count = count+1
        # Split slicelist into sentences
        slice_corpus = []
        sentences = get_sentences(slicelist)
        if sentences == []:
            continue

        # Get metadata of slice
        header = sentences[0]
        testcase_id = get_testcase_id(corpuspath, header)
        isVulnerable = int(sentences[-1])  # binary label that indicates if slice has a vulnerability or not
        sard = get_SARD(header)  # Assign class label (aka the pillar CWE ID assigned)
        class_label = get_multiclass_label(sard, multiclasspath, 'CWE-ID')
        if class_label is None:
            continue # skip slices that don't have a label b/c we won't be able to classify them 
        focuspointer = get_focus_pointer(header)
        fp_index = 0
        firstFP = False
        filenameCorpus = str(testcase_id)
        folder_path = os.path.join(corpuspath, str(testcase_id))
        savefilename = os.path.join(folder_path, filenameCorpus + '.pkl')
        slicefile_corpus = []
        slicefile_labels = []
        slicefile_filenames = []
        slicefile_func = []
        slicefile_groups = []
        sentences = sentences[1:] # removes header

        for sentence in sentences:
            if ';' in sentence.split(" ")[-1]:
                sentence = sentence.replace(';', '')

            start = str.find(sentence,r'printf("')
            if start != -1:
                start = str.find(sentence,r'");')
                sentence = sentence[:start+2] # don't know why the +2
            
            fm = str.find(sentence,'/*')
            if fm != -1:
                sentence = sentence[:fm]
            else:
                fm = str.find(sentence,'//')
                if fm != -1:
                    sentence = sentence[:fm]
            sentence = sentence.strip()

            # save index of focuspointer so we know somewhat where it was in case it gets tokenized
            if (focuspointer in sentence.split(' ')) and (firstFP == False):
                fp_index += sentence.split(' ').index(focuspointer)
                firstFP = True

            # Transform words to tokens
            list_tokens = create_tokens(sentence)
            
            # increment token_counter so we know what index the first focus pointer is
            if (firstFP == False):
                fp_index += len(list_tokens)
            slice_corpus.append(list_tokens)

        slicefile_labels.append(isVulnerable)  
        slicefile_filenames.append(header)
        slicefile_groups.append(class_label)
        
        # Create list of functions
        slice_corpus = mapping(slice_corpus)
        slice_func = slice_corpus
        slice_func = list(set(slice_func))
                
        if slice_func == []:
            slice_func = ['main']
        sample_corpus = [] # stores the flattened list of words

        # flattens array of sentence arrays into an 1D array of words
        for sentence in slice_corpus: 
            list_tokens = create_tokens(sentence)
            sample_corpus = sample_corpus + list_tokens
                    
        slicefile_corpus.append(sample_corpus)
        slicefile_func.append(slice_func)
        
        if saveIndividualFiles:
            if (testcase_id not in os.listdir(corpuspath)) and (not os.path.exists(folder_path)):
                os.mkdir(folder_path)
            savingFile = open(savefilename, 'wb')
            pickle.dump([slicefile_corpus, slicefile_labels, slicefile_func, slicefile_filenames, myType, slicefile_groups, [filenameCorpus]], savingFile)
            savingFile.close()

        # save files with focuspointer data attached so we can use the index of 
        # first occurence of focuspointer as our midpoint when adjusting vector lengths
        fp_data = [slicefile_corpus, slicefile_labels, [fp_index], slicefile_filenames, myType, slicefile_groups, [filenameCorpus]]
        for i in range(len(fp_data)):
            all_data[i].append(fp_data[i][0])

    return count

def save_tokens_to_file(all_data, all_tokensPath, count):
    print ("Total Corpus Files: ", count)

    savingFile = open(all_tokensPath, 'wb')
    pickle.dump(all_data, savingFile)
    savingFile.close()
    for i in range(len(all_data) - 1):
        assert len(all_data[i]) == len(all_data[i+1]), "Lengths of each index in data is off."


def get_testcase_id(corpuspath, sentence):
    testcase_id = sentence.split(' ')[0] 
    if testcase_id in os.listdir(corpuspath):
        # avoid duplicate testcase id numbers
        testcase_id = int(testcase_id) + 1000000
    else:
        testcase_id = int(testcase_id)
    return testcase_id


