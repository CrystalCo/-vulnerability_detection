import os

from SYSE_1_isVulnerable.mapping import mapping, create_tokens
from utils.utils import get_SARD, get_sentences, get_focus_pointer, get_multiclass_label, get_type


def tokenizeSlicesPerFile(slicepath, inputFilename, multiclasspath, numSamples, allData, focusPointerIndex=-2):
    """
        Converts the words & symbols from the samples into tokens that will be used to create vectors. Expects multiclass labels.
    
        Trying to differentiate between non vulnerable and vulnerability types returned imbalanced data 
        (356,121 non-vulnerable out of 420,627 samples), and low performance scores for training and testing, 
        even with downsampling.
        Thus, since we are trying to predict vulnerability types, we will omit non-vulnerable samples.
        
        tokenizeSlices() and its corresponding file `2_Application_Codes.py` perform significantly well at 
        predicting whether a code slice has a vulnerability or not. 
        This method and the files where it is used in should be considered a supplement, like a 2nd phase
        to run samples that were flagged as vulnerable in the first phase `2_Application_Codes.py`.
        Models perform best when done sequentially instead of trying to predict vulnerability status AND vulnerability type if applicable.

        slicepath := str; absolute or relative input path of source file
        inputFilename := str; inputFilename containing source slices
        multiclasspath := str; path that contains the ground truth labels
        numSamples := int; Max Num of slice samples from each file
        all_data := an array of nested arrays which will hold our tokenized slices. 
        Ex. [ [], [], [], [], [], [], [] ]
        focusPointerIndex := int; focus pointers are found in the headers of each slice; -2=by keyword, -1=by line number  
    """
    print("Slice File To be Processed: ", inputFilename)
    savingFile = open(os.path.join(slicepath, inputFilename), 'r')
    slicesList = savingFile.read().split("------------------------------")#aa. split each slide (each program in .txt file)
    slicesList = clean_slicelist(slicesList)
    savingFile.close()

    myType = get_type(inputFilename)
    count = 0

    for slicelist in slicesList:
        if count == numSamples:
            break
        count = count+1

        # Split slicelist into sentences
        sentences = get_sentences(slicelist)
        if sentences == []:
            continue

        # Get metadata of slice
        slice_corpus = []
        header = sentences[0]
        testcase_id = int(header.split(' ')[0])
        isVulnerable = int(sentences[-1])  # label that indicates if slice has a vulnerability or not
        sentences = sentences[1:-1] # removes header & label

        if (focusPointerIndex == -2):
            # save focus pointer index based on the token since we don't have line numbers next to these samples
            sard = get_SARD(header)  # Assign class label (aka the pillar CWE ID assigned)
            class_label = get_multiclass_label(sard, multiclasspath, 'CWE-ID')
            if class_label is None:
                continue # skip slices that don't have a label b/c we won't be able to classify them 
            # save files with focuspointer data attached so we can use the index of 
            # first occurence of focuspointer as our midpoint when adjusting vector lengths
            focuspointer = get_focus_pointer(header, focusPointerIndex)
            fp_index = 0
            firstFP = False

            for sentence in sentences:
                # Cleanup
                if ';' in sentence.split(" ")[-1]:
                    sentence = sentence.replace(';', '')
                sentence = remove_print_statements(sentence)
                sentence = remove_comments(sentence)

                # save index of focuspointer so we know somewhat where it was in case it gets tokenized
                if (focuspointer in sentence.split(' ')) and (firstFP == False):
                    fp_index += sentence.split(' ').index(focuspointer)
                    firstFP = True

                # Transform words to tokens
                list_tokens = create_tokens(sentence)
                
                # increment token_counter so we know what index the first focus pointer is
                if (firstFP == False):
                    fp_index += len(list_tokens)
                slice_corpus.append(list_tokens)
        elif(focusPointerIndex == -1):
            # Save focus pointer index based on the line # since we have line #s at the end of each sentence
            # We'll use muVulDeePecker's labels for now.  Can map their labels to ours later.
                # map it to their assigned labels for 1-to-1
                # for 1-to-many, use CWE found in header
            # NOTE: their CWE-labels dont exaactly match what's on the SARD website (ex. SARD ID 68361 is assigned as CWE-122 by SARD, but labeled 3 (CWE-119) by muVulDeePecker's dataset).

            # Lets get their label.
            class_label = isVulnerable
            if isVulnerable != 0:
                isVulnerable = 1

            myType = 'API'
            focuspointer = int(get_focus_pointer(header, focusPointerIndex))
            fp_index = 0
            foundFP = False

            for sentence in sentences:
                words = sentence.split(" ")
                # First, extract line numbers at the end of each sentence
                if (words[-1].isdigit()):
                    line_num = int(words[-1])
                    # save index of focuspointer so we can avg around this center when normalizing row lengths later
                    if ((line_num == focuspointer) and (foundFP == False)):
                        fp_index += 1
                        foundFP = True
                    # Remove line number from sentence
                    words = words[:-1]

                # Clean the sentence of unnescessary tokens
                if ';' in words[-1]:
                    words[-1] = words[-1].replace(';', '')
                sentence = " ".join(words)
                sentence = remove_print_statements(sentence)
                sentence = remove_comments(sentence)

                # Transform words to tokens
                list_tokens = create_tokens(sentence)
                
                # increment token_counter so we know what index the focus pointer is at
                if (foundFP == False):
                    fp_index += len(list_tokens)
                slice_corpus.append(list_tokens)
        else:
            raise("Focus pointer for that index is not defined. Must be -1 or -2. Create additional code for focus pointers of any other type.")
        
        # Tokenizes variable names & flattens arrays of rows to arrays of strings 
        slice_corpus = mapping(slice_corpus)                

        # flattens array of rows into an 1D array of tokens
        sample_corpus = []
        for sentence in slice_corpus: 
            list_tokens = create_tokens(sentence)
            sample_corpus = sample_corpus + list_tokens
        
        allData[0].append(sample_corpus)
        allData[1].append(isVulnerable)
        allData[2].append(fp_index)
        allData[3].append(header)
        allData[4].append(myType)
        allData[5].append(class_label)
        allData[6].append(testcase_id)

    return count


def clean_slicelist(slicelist):
    # create clean_slicelist if these are used/valuable
    if slicelist[0] == '':
        del slicelist[0]
    if slicelist[-1] == '' or slicelist[-1] == '\n' or slicelist[-1] == '\r\n':
        del slicelist[-1]
    return slicelist

def remove_print_statements(sentence):
    start = str.find(sentence,r'printf("')
    if start != -1:
        start = str.find(sentence,r'");')
        sentence = sentence[:start+2] # don't know why the +2
    return sentence

def remove_comments(sentence):
    fm = str.find(sentence,'/*')
    if fm != -1:
        sentence = sentence[:fm]
    else:
        fm = str.find(sentence,'//')
        if fm != -1:
            sentence = sentence[:fm]
    sentence = sentence.strip()
    return sentence

