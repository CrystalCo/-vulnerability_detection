import os
import pickle

from SYSE_1_isVulnerable.mapping import mapping, create_tokens
from utils.utils import get_SARD, get_sentences, get_focus_pointer, get_multiclass_label, get_type


def tokenizeSlicesPerFile(slicepath, filename, corpuspath, multiclasspath, numSamples, allData, focusPointerIndex=0):
    """
        Converts the words & symbols from the samples into tokens that will be used to create vectors. Expects multiclass labels.
    
        Trying to differentiate between non vulnerable and vulnerability types returned imbalanced data 
        (356,121 non-vulnerable out of 420,627 samples), and low performance scores for training and testing, 
        even with downsampling.
        Thus, since we are trying to predict vulnerability types, we will omit non-vulnerable samples.
        
        tokenizeSlices() and its corresponding file `2_Application_Codes.py` perform significantly well at 
        predicting whether a code slice has a vulnerability or not. 
        This method and the files where it is used in should be considered a supplement, like a 2nd phase
        to run samples that were flagged as vulnerable in the first phase `2_Application_Codes.py`.
        Models perform best when done sequentially instead of trying to predict vulnerability status AND vulnerability type if applicable.

        slicepath := str; absolute or relative input path of source file
        filename := str; filename containing source slices
        corpuspath := str; path for output files per sample
        multiclasspath := str; path that contains the ground truth labels
        numSamples := int; Max Num of slice samples from each file
        all_data := an array of nested arrays which will hold our tokenized slices. 
        Ex. [ [], [], [], [], [], [], [] ]
        focusPointerIndex := int; focus pointers are found in the headers of each slice; 0=by keyword, 1=by line number  
    """
    assert(focuspointer == 0 or focuspointer == 1, "Focus pointer for that index is not defined. Must be 1 or 0. Create additional code for focus pointers of any other type.")
    print("Slice File To be Processed: ", filename)
    savingFile = open(os.path.join(slicepath, filename), 'r')
    slicesList = savingFile.read().split("------------------------------")#aa. split each slide (each program in .txt file)
    slicesList = clean_slicelist(slicesList)
    savingFile.close()

    myType = get_type(filename)
    count = 0

    for slicelist in slicesList:
        if count == numSamples:
            break
        count = count+1

        # Split slicelist into sentences
        sentences = get_sentences(slicelist)
        if sentences == []:
            continue

        # Get metadata of slice
        slice_corpus = []
        slicefile_corpus = []
        slicefile_labels = []
        slicefile_filenames = []
        slicefile_func = []
        slicefile_class = []
        sentences = sentences[1:] # removes header
        header = sentences[0]
        testcase_id = int(sentence.split(' ')[0])
        if testcase_id in os.listdir(corpuspath):
            testcase_id = testcase_id + 1000000 # avoid duplicate testcase id numbers

        if (focusPointerIndex == 0):
            sard = get_SARD(header)  # Assign class label (aka the pillar CWE ID assigned)
            class_label = get_multiclass_label(sard, multiclasspath, 'CWE-ID')
            if class_label is None:
                continue # skip slices that don't have a label b/c we won't be able to classify them 
            focuspointer = get_focus_pointer(header, focusPointerIndex)
            fp_index = 0
            firstFP = False
            isVulnerable = int(sentences[-1])  # binary label that indicates if slice has a vulnerability or not        

            for sentence in sentences:
                if ';' in sentence.split(" ")[-1]:
                    sentence = sentence.replace(';', '')

                start = str.find(sentence,r'printf("')
                if start != -1:
                    start = str.find(sentence,r'");')
                    sentence = sentence[:start+2] # don't know why the +2
                
                fm = str.find(sentence,'/*')
                if fm != -1:
                    sentence = sentence[:fm]
                else:
                    fm = str.find(sentence,'//')
                    if fm != -1:
                        sentence = sentence[:fm]
                sentence = sentence.strip()

                # save index of focuspointer so we know somewhat where it was in case it gets tokenized
                if (focuspointer in sentence.split(' ')) and (firstFP == False):
                    fp_index += sentence.split(' ').index(focuspointer)
                    firstFP = True

                # Transform words to tokens
                list_tokens = create_tokens(sentence)
                
                # increment token_counter so we know what index the first focus pointer is
                if (firstFP == False):
                    fp_index += len(list_tokens)
                slice_corpus.append(list_tokens)
        elif(focusPointerIndex == 1):
            # Option 1) we use muVulDeePecker's labels
            # Option 2) we map muVulDeePecker's labels to our labels (only using the vulnerable samples)
            sard = get_SARD(header)  # Assign class label (aka the pillar CWE ID assigned)
            class_label = get_multiclass_label(sard, multiclasspath, 'CWE-ID')
            
            if class_label is None:
                continue # skip slices that don't have a label b/c we won't be able to classify them 
            focuspointer = get_focus_pointer(header, focusPointerIndex)
            fp_index = 0
            firstFP = False
            isVulnerable = int(sentences[-1])  # binary label that indicates if slice has a vulnerability or not        
            isVulnerable = 1 if isVulnerable != 0 else 0

            for sentence in sentences:
                # TODO: Account for line numbers at the end of each sentence
                
                if ';' in sentence.split(" ")[-1]:
                    sentence = sentence.replace(';', '')

                start = str.find(sentence,r'printf("')
                if start != -1:
                    start = str.find(sentence,r'");')
                    sentence = sentence[:start+2] # don't know why the +2
                
                fm = str.find(sentence,'/*')
                if fm != -1:
                    sentence = sentence[:fm]
                else:
                    fm = str.find(sentence,'//')
                    if fm != -1:
                        sentence = sentence[:fm]
                sentence = sentence.strip()

                # save index of focuspointer so we know somewhat where it was in case it gets tokenized
                if (focuspointer in sentence.split(' ')) and (firstFP == False):
                    fp_index += sentence.split(' ').index(focuspointer)
                    firstFP = True

                # Transform words to tokens
                list_tokens = create_tokens(sentence)
                
                # increment token_counter so we know what index the first focus pointer is
                if (firstFP == False):
                    fp_index += len(list_tokens)
                slice_corpus.append(list_tokens)



        slicefile_labels.append(isVulnerable)
        slicefile_filenames.append(header)
        slicefile_class.append(class_label)
        
        # Create list of functions
        slice_corpus = mapping(slice_corpus)
        slice_func = slice_corpus
        slice_func = list(set(slice_func))
                
        if slice_func == []:
            slice_func = ['main']
        sample_corpus = [] # stores the flattened list of words

        # flattens array of sentence arrays into an 1D array of words
        for sentence in slice_corpus: 
            list_tokens = create_tokens(sentence)
            sample_corpus = sample_corpus + list_tokens
                    
        slicefile_corpus.append(sample_corpus)
        slicefile_func.append(slice_func)

        # save files with focuspointer data attached so we can use the index of 
        # first occurence of focuspointer as our midpoint when adjusting vector lengths
        fp_data = [slicefile_corpus, slicefile_labels, [fp_index], slicefile_filenames, myType, slicefile_class, [str(testcase_id)]]
        for i in range(len(fp_data)):
            allData[i].append(fp_data[i][0])

    return count

def save_tokens_to_file(all_data, all_tokensPath, count):
    print ("Total Corpus Files: ", count)

    savingFile = open(all_tokensPath, 'wb')
    pickle.dump(all_data, savingFile)
    savingFile.close()
    for i in range(len(all_data) - 1):
        assert len(all_data[i]) == len(all_data[i+1]), "Lengths of each index in data is off."


def clean_slicelist(slicelist):
    # create clean_slicelist if these are used/valuable
    if slicelist[0] == '':
        del slicelist[0]
    if slicelist[-1] == '' or slicelist[-1] == '\n' or slicelist[-1] == '\r\n':
        del slicelist[-1]
    return slicelist

