import os, pickle

import numpy as np
from sklearn.preprocessing import LabelBinarizer


def concat_datasets(data1, data2):
    """
        Input := array of arrays; most likely derived from getDataset().
    """
    assert len(data1) == len(data2), "datasets must be of the same length"
    
    for i in range(len(data1)):
        data1[i] = data1[i] + data2[i]        
    
    return data1

def downsample(data, downsampleLabel, downsampleNum):
    """
        data := array of arrays; most likely derived from getDataset().
        downsampleLabel := int; group id from which to downsample
        downsampleNum := int; the maximum number of samples for the downsample label
    """
    count = 0
    sections = len(data)
    downsampled_data = init_nested_arrays(sections)
    
    for i in range(len(data[0])):
        if (data[-2][i] == downsampleLabel) and (count >= downsampleNum):
            continue
        if data[-2][i] == downsampleLabel:
            count += 1
        for n in range(sections):
            downsampled_data[n].append(data[n][i])            
    return downsampled_data

def encode_target(categories):
    """
        Transforms categorical target to one hot encoded matrix.
        Returns a dictionary mapping the group id to its encoded counterpart, 
        and the fitted LabelBinarizer instance to help decode later.
    """
    lb = LabelBinarizer()
    lb.fit(categories)
    print(f'\nOriginal categories: {lb.classes_}')
    # Label encoding e.g. [Class A, Class B, Class C] ==> [1,0,0], [0,1,0], [0,0,1]]
    encoded_y = lb.transform(categories) 
    print(f'Encoded classes:\n{encoded_y}\n')

    mapping = {}
    for i in range(len(categories)):
        mapping[categories[i]] =  encoded_y[i] 

    return mapping, lb

def get_category(arr):
    """
        Flattens groupid array for all samples into the assigned groupids only.
        Example: [[664, 664], [703, 555]] --> [664, 703]
    """
    result = [x[0] for x in arr]
    return result

def getDataset(dataset_path, getBalanced=True, RANDOMSEED=None):
    print("Getting dataset...")
    dataset = []

    filetype = "balanced" if getBalanced else "ALL"
    for filename in os.listdir(dataset_path):
        if (filename.endswith(".pkl")) and (filetype in filename):
            print(filename)
            f = open(os.path.join(dataset_path, filename),"rb")
            data = pickle.load(f)
            f.close()
            dataset = data

    if RANDOMSEED:
        for i in range(len(dataset)):
            np.random.seed(RANDOMSEED)
            np.random.shuffle(dataset[i])

    return dataset

def get_SARD(sentence0):
    header = sentence0.split(' ')[1]
    sard = header.split('/')[0]
    return sard

def get_sard_cve_ids(filenames):
    ids = []
    for f in filenames:
        x = get_SARD(f)
        ids.append(x)
    return ids

def get_predicted_class_and_accuracy(labels):
    accuracies = []
    categories = []
    for l in labels:
        # get index of best accuracy to determine which category it predicted
        best_prediction = 0 
        category = 0
        for index, acc in enumerate(l):
            if acc > best_prediction:
                best_prediction = acc
                category = index
        accuracies.append(best_prediction)
        categories.append(category)
    return accuracies, categories

def getAvgLength(arr):
    totalSamples = len(arr)
    totalVectorLen = 0

    for a in arr:
        totalVectorLen += len(a)

    meanLen = int(totalVectorLen/totalSamples)
    print("Mean Vector Length" ,  meanLen)
    return meanLen

def init_nested_arrays(num):
    arr = []
    for i in range(num):
        arr.append([])
    return arr

def map_pred_to_cat(y_pred, labelEncoder):
    """
        Converts predicted labels into their corresponding 
        original group id.
        y_true := an array of ints; shape(len(y_pred),)
        labelEncoder := fitted instance of LabelBinarizer()
    """
    result = []
    for label in y_pred:
        result.append(labelEncoder.classes_[label])
    assert(len(y_pred) == len(result))
    return np.array(result)

def num_classes(datapath):
    """
        Gets the total number of unique classes that exist.
    """
    for filename in os.listdir(datapath):
        if filename.endswith(".pkl"):
            f = open(os.path.join(datapath, filename),"rb")
            data = pickle.load(f)
            f.close()
            categories = get_category(data[-2])
            return np.unique(categories)

def transformVLength(X, maxlen, vector_dim=30):
    cut_count = 0
    fill_0_count = 0
    no_change_count = 0
    fill_0 = [0]*vector_dim
    totallen = 0
    threshold = maxlen * vector_dim
    print ("threshold: " ,threshold)
    if (maxlen != 0):
        new_X = []
        for x in X:  
             #len(x) is how many symbols in 1 program. ex. 79 
            if len(x) <  maxlen:
                x = x + [fill_0] * (maxlen - len(x))
                new_X.append(x)
                fill_0_count += 1

            elif len(x) == maxlen:
                new_X.append(x)
                no_change_count += 1
                    
            else:
                startpoint = int(threshold - round(maxlen / 2.0))
                endpoint =  int(startpoint + maxlen)
                if startpoint < 0:
                    startpoint = 0
                    endpoint = maxlen
                if endpoint >= len(x):
                    startpoint = -maxlen
                    endpoint = None
                new_X.append(x[startpoint:endpoint])
                cut_count += 1
            totallen = totallen + len(x)
    X = new_X
    print ("New Vector Length: ", len(X[0]))
    return X

