import os, pickle

import numpy as np
import pandas as pd
from collections import Counter
from sklearn.preprocessing import LabelBinarizer


def concat_datasets(data1, data2):
    """
        Input := array of arrays; most likely derived from getDataset().
    """
    assert len(data1) == len(data2), "datasets must be of the same length"
    
    for i in range(len(data1)):
        data1[i] = data1[i] + data2[i]        
    
    return data1

def drop_classes_by_count(classes, class_counts, class_indices, min_num):
    """
        Updates the classes, class_counts, class_indices used in splitTrainTest by dropping the indices that 
        aren't in our target range.

        classes       := numpy arrays
        class_counts  := numpy arrays
        class_indices := numpy arrays
        min_num    := minimum number of samples a class should have to make it into our final set
    """
    for i in classes:
        try:
            drop_index = np.where(class_counts < min_num)[0][0]
        except:
            continue
        print(f'Dropping class {str(classes[drop_index])}. Class count: {class_counts[drop_index]}.')
        class_counts = np.delete(class_counts, drop_index, 0)
        class_indices = np.delete(class_indices, drop_index, 0)
        classes = np.delete(classes, drop_index, 0)

    return classes, class_counts, class_indices

def drop_classes_by_type(vType, classes, class_counts, class_indices):
    """
        Updates the classes, class_counts, class_indices used in splitTrainTest by dropping the indices that 
        aren't of vType.

        vType         := str; drops classes that are not equal to the vulernable syntax characteristic type pass in.
                       options: AE, ARR, API, or PTR
        classes       := numpy arrays
        class_counts  := numpy arrays
        class_indices := numpy arrays
    """
    dropClasses = [c for c in classes if c != vType]
    for i in dropClasses:
        try:
            drop_index = np.where(classes == i)[0][0]
        except:
            print(f'Drop class {i} not found in classes. Continuing down our list.')
            continue
        print(f'Dropping class {str(i)}. Class count: {class_counts[drop_index]}.')
        class_counts = np.delete(class_counts, drop_index, 0)
        class_indices = np.delete(class_indices, drop_index, 0)
        classes = np.delete(classes, drop_index, 0)

    return classes, class_counts, class_indices

def drop_col_df(df, name, axis=1, inplace=True):
    df.drop(name, axis=axis, inplace=inplace)

def drop_non_vulnerable_samples(inputpath, outputpath):
    # Drop non-vulnerable samples
    data = getDataset(inputpath, getBalanced=False)
    classes, class_counts, class_indices = get_class_indices_by_index(data, index=1)
    classes, class_counts, class_indices = drop_classes_by_type(1, classes, class_counts, class_indices)
    new_set = [[], [], [], [], [],[], []]
    for indices in class_indices:
        for i in indices:
            for group_index in range(len(data)):
                new_set[group_index].append(data[group_index][i])
    save_data_to_file(outputpath, 'ALL_vectors.pkl', new_set)

def hot_encode_target(categories):
    """
        Transforms categorical target to one hot encoded matrix.
        Returns a dictionary mapping the group id to its encoded counterpart, 
        and the fitted LabelBinarizer instance to help decode later.
    """
    lb = LabelBinarizer()
    lb.fit(categories)
    print(f'\nCategories in our vector train path:\n{lb.classes_}')
    print(f'Total number of classes: {len(lb.classes_)}')
    # Label encoding e.g. [Class A, Class B, Class C] ==> [1,0,0], [0,1,0], [0,0,1]]
    encoded_y = lb.transform(categories) 
    print(f'Encoded classes:\n{encoded_y}\n')

    mapping = {}
    for i in range(len(categories)):
        mapping[categories[i]] =  encoded_y[i]

    return mapping, lb

def getDataset(dataset_path, getBalanced=True, RANDOMSEED=None):
    """
        dataset_path := str; path of pkl file from which to get the data from
        getBalanced := boolean; if True, gets files with the name 'balanced' in it 
        RANDOMSEED := int
    """
    dataset = []

    filetype = "balanced" if getBalanced else "ALL"
    for filename in os.listdir(dataset_path):
        if (filename.endswith(".pkl")) and (filetype in filename):
            print("Getting dataset...", os.path.join(dataset_path, filename))
            f = open(os.path.join(dataset_path, filename),"rb")
            dataset = pickle.load(f)
            f.close()

    if RANDOMSEED:
        for i in range(len(dataset)):
            np.random.seed(RANDOMSEED)
            np.random.shuffle(dataset[i])

    return dataset

def getDatasetSectionByIndex(dataset_path, getBalanced=True, RANDOMSEED=None, index=-2):
    """
        dataset_path := str; path of pkl file from which to get the data from
        getBalanced := boolean; if True, gets files with the name 'balanced' in it 
        RANDOMSEED := int
    """
    dataset = getDataset(dataset_path, getBalanced, RANDOMSEED)
    return dataset[index]

def getDatasetXY(dataset_path, getBalanced=True, RANDOMSEED=None, y_index=-2):
    """
        dataset_path := str; path of pkl file from which to get the data from
        getBalanced := boolean; if True, gets files with the name 'balanced' in it 
        RANDOMSEED := int
    """
    dataset = getDataset(dataset_path, getBalanced, RANDOMSEED)
    return dataset[0], dataset[y_index]

def get_class_indices_by_index(dataset, index):
    # Get ratios
    classes, y_indices = np.unique(dataset[index], return_inverse=True)
    class_counts = np.bincount(y_indices)

    if len(class_counts) < 2:
        raise ValueError("The least populated class in y has only 1 member, which is too few. The minimum number of groups for any class cannot be less than 2.")

    # Find the sorted list of instances for each class
    class_indices = np.split(np.argsort(y_indices, kind='mergesort'), np.cumsum(class_counts)[:-1])
    print(f'Classes: {classes}.\n\nClass Counts:{class_counts}.\n')
    return classes,class_counts,class_indices

def get_focus_pointer(sentence, index=0):
    """
        Returns focus pointer from header.
        sentence :=  str; 1st sentence from our original source slice. For example, "2 79848/CWE134_Uncontrolled_Format_String__char_environment_vfprintf_52a.c strcpy 70"
        index  :=  int; -2=word (ex. strcpy), -1=line number (ex. 70)
    """
    focuspointer = sentence.split(".c")[-1].split(' ')[index]
    if focuspointer[0] == '':
        focuspointer = focuspointer[1:]
    if len(focuspointer) < 1:
        raise("no focuspointer found")
    elif len(focuspointer) == 1:
        focuspointer = focuspointer[0]
    else:
        focuspointer = ''.join(focuspointer)
    return focuspointer

def get_latest_epoch(filename):
    epoch = filename.split('-')[-1]
    return epoch

def get_multiclass_dict(multiclasspath):
    """
        multiclasspath := str; path where SARD_CVE_to_groups.csv lives which maps original SARD or CVE IDs to their root CWE-ID.
        Returns a dict with the root CWE-IDs as the keys.
    """
    df = pd.read_csv(multiclasspath, index_col=0)
    categories = np.unique(df['Group ID'])
    categoryDict = {}

    for c in categories:
        categoryDict[c] = []
    return categoryDict

def get_multiclass_label(sard, multiclasspath, id_column='CWE-ID'):
    """
        Matches the SARD ID to its corresponding class (found in multiclasspath).
        The corresponding class is an ID that categorizes the type of CWE vulnerability the sample would fall into.
    """
    label = None
    df = pd.read_csv(multiclasspath, index_col=0)
    try:
        label = int(df.loc[df['Original ID'] == sard][id_column].item())
    except Exception as e:
        pass
        
    return label

def get_predicted_class_and_accuracy(labels):
    accuracies = []
    categories = []
    for l in labels:
        # get index of best accuracy to determine which category it predicted
        best_prediction = 0 
        category = 0
        for index, acc in enumerate(l):
            if acc > best_prediction:
                best_prediction = acc
                category = index
        accuracies.append(best_prediction)
        categories.append(category)
    return accuracies, categories

def get_SARD(header):
    """
        header := str; header in a slice that contains the SARD test case ID.

        Returns the SARD ID in string format.
    """
    head = header.split(' ')[1]
    sard = head.split('/')[0]
    return sard

def get_sard_cve_ids(filenames):
    ids = []
    for f in filenames:
        x = get_SARD(f)
        ids.append(x)
    return ids

def get_sentences(slicelist):
    sentences = slicelist.split('\n')
    if sentences[0] == '\r' or sentences[0] == '':
        del sentences[0]
    if sentences[-1] == '':
        del sentences[-1]
    if sentences[-1] == '\r':
        del sentences[-1]
    return sentences

def get_type(filename):
    """
        filename := str; name of a source code file
        Returns the syntax vulnerability type found in the filename of the source code file if it exists in the filename.
    """
    myType = 'Others'
    if "API" in filename:
        myType = 'API'
    elif "Array" in filename:
        myType = 'ARR'
    elif "Pointer" in filename:
        myType = 'PTR'
    elif "Arithmetic" in filename:
        myType = 'AE'
    return myType

def get_vul_filename(header):
    """
        header := str; header in a slice that contains a c filename.

        Returns the filename contained in the slice.
    """
    head = header.split(' ')[1]
    vul_filename = head.split('/')[1]
    if not vul_filename.endswith('.c'):
        print('Does not contain a c filename: ', header)
    return vul_filename
    
# gather all vulnerable samples
def get_vul_samples(new_set):
    vul_set = [[], [], [], [], [],[], []]

    for i in range(len(new_set[0])):
        if new_set[1][i] == 1:
            vul_set[0].append(new_set[0][i])
            vul_set[1].append(new_set[1][i])
            vul_set[2].append(new_set[2][i])
            vul_set[3].append(new_set[3][i])
            vul_set[4].append(new_set[4][i])
            vul_set[5].append(new_set[5][i])
            vul_set[6].append(new_set[6][i])
    return vul_set

def init_nested_arrays(num):
    arr = []
    for _ in range(num):
        arr.append([])
    return arr

def levenshtein_ratio_and_distance(s, t, ratio_calc = False):
    """ levenshtein_ratio_and_distance:
        Calculates levenshtein distance between two strings.
        If ratio_calc = True, the function computes the
        levenshtein distance ratio of similarity between two strings
        For all i and j, distance[i,j] will contain the Levenshtein
        distance between the first i characters of s and the
        first j characters of t
    """
    # Initialize matrix of zeros
    rows = len(s)+1
    cols = len(t)+1
    distance = np.zeros((rows,cols),dtype = int)

    # Populate matrix of zeros with the indeces of each character of both strings
    for i in range(1, rows):
        for k in range(1,cols):
            distance[i][0] = i
            distance[0][k] = k

    # Iterate over the matrix to compute the cost of deletions,insertions and/or substitutions    
    for col in range(1, cols):
        for row in range(1, rows):
            if s[row-1] == t[col-1]:
                cost = 0 # If the characters are the same in the two strings in a given position [i,j] then the cost is 0
            else:
                # In order to align the results with those of the Python Levenshtein package, if we choose to calculate the ratio
                # the cost of a substitution is 2. If we calculate just distance, then the cost of a substitution is 1.
                if ratio_calc == True:
                    cost = 2
                else:
                    cost = 1
            distance[row][col] = min(distance[row-1][col] + 1,      # Cost of deletions
                                 distance[row][col-1] + 1,          # Cost of insertions
                                 distance[row-1][col-1] + cost)     # Cost of substitutions
    if ratio_calc == True:
        # Computation of the Levenshtein Distance Ratio
        Ratio = ((len(s)+len(t)) - distance[row][col]) / (len(s)+len(t))
        return Ratio
    else:
        # print(distance) # Uncomment if you want to see the matrix showing how the algorithm computes the cost of deletions,
        # insertions and/or substitutions
        # This is the minimum number of edits needed to convert string a to string b
        return "The strings are {} edits away".format(distance[row][col])

def map_pred_to_cat(y_pred, labelEncoder):
    """
        Converts predicted labels into their corresponding 
        original group id.
        y_true := an array of ints; shape(len(y_pred),)
        labelEncoder := fitted instance of LabelBinarizer()
    """
    result = []
    for label in y_pred:
        result.append(labelEncoder.classes_[label])
    assert(len(y_pred) == len(result))
    return np.array(result)

def print_group_counts(categoryDict):
    print('Categories found in the slice samples')
    print('GROUP ID\tCOUNT')
    for c in categoryDict:
        count = len(categoryDict[c])
        print(f'{c}\t{count}')

def print_num_samples(datapath, getBalanced=True):
    data = getDataset(datapath, getBalanced)
    print(f'Total number of samples in datapath set: {len(data[-2])}')

def output_df_to_excel(df, filename):
    df.to_excel(filename)

def randomly_shuffle_dataset(dataset, randomSeed=1099):
    """ 
        inplace shuffle of our entire dataset
        dataset     := nested arrays that include vects, labels, ids, etc.
        randomSeed  := int
    """
    for i in range(len(dataset)):
        np.random.seed(randomSeed)
        np.random.shuffle(dataset[i])

def save_data_to_file(dirPath, filename, data):
    # Save to a file so we don't have to load all the data every time
    aPath = os.path.join(dirPath, filename)
    aFile = open(aPath, 'wb')
    pickle.dump(data, aFile, protocol=pickle.HIGHEST_PROTOCOL)
    aFile.close()
    print(f'All vectors saved in {aFile}\n')

def split_train_test_sets(class_counts, class_indices, split, group_set):
    # Split into training and test sets
    train_set = [[], [], [], [], [],[], []]
    test_set  = [[], [], [], [], [],[], []]

    for count, indices in zip(class_counts, class_indices):
        training_split = int(count*split) + 1
        label_indices_train = indices[:training_split]
        label_indices_test = indices[training_split:]

        for i in label_indices_train:
            for group_index in range(len(group_set)):
                train_set[group_index].append(group_set[group_index][i])

        for i in label_indices_test:
            for group_index in range(len(group_set)):
                test_set[group_index].append(group_set[group_index][i])

    return train_set, test_set

def set_newdataset(input_output_path, labels):
    # Update muVulDeePecker's labels into ours
    new_set = [[], [], [], [], [], [], []]
    data = getDataset(input_output_path, False)
    y = data[-2]
    for i in range(len(y)):
        # Keep only the labels that match ours
        if (y[i] in labels['label'].values):
            data[-2][i] = labels.loc[labels['label'] == y[i]]['ID'].item()
            for j in range(len(data)):
                new_set[j].append(data[j][i])
    save_data_to_file(input_output_path, 'ALL_vectors.pkl', new_set)

def summarize_distribution(y, mapping):
    # summarize distribution
    counter = Counter(y)
    for k,v in counter.items():
        per = v / len(y) * 100
        print('Encoded Label=%d,\tClass=%d,\tn=%d (%.3f%%)' % (k, mapping[k], v, per))
