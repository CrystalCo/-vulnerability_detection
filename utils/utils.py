import os, pickle

import numpy as np
import pandas as pd
from matplotlib import pyplot as plt
from sklearn.preprocessing import LabelBinarizer


def concat_datasets(data1, data2):
    """
        Input := array of arrays; most likely derived from getDataset().
    """
    assert len(data1) == len(data2), "datasets must be of the same length"
    
    for i in range(len(data1)):
        data1[i] = data1[i] + data2[i]        
    
    return data1

def downsample(data, downsampleLabel, downsampleNum):
    """
        data := array of arrays; most likely derived from getDataset().
        downsampleLabel := int; group id from which to downsample
        downsampleNum := int; the maximum number of samples for the downsample label
    """
    if type(data[-2][0]) is not int:
        raise TypeError("Expecting the target column to be a list of ints. If type is a list of lists, please flatten and replace target column with flattened list of ints.  Try `[x[1] for x in data[-2]]` for original group label or `[x[0] for x in data[-2]]` for merged labels.")

    count = 0
    sections = len(data)
    downsampled_data = init_nested_arrays(sections)
    
    for i in range(len(data[0])):
        if (data[-2][i] == downsampleLabel) and (count >= downsampleNum):
            continue
        if data[-2][i] == downsampleLabel:
            count += 1
        for n in range(sections):
            downsampled_data[n].append(data[n][i])
    return downsampled_data

def drop_classes(vType, classes, class_counts, class_indices):
    """
        Updates the classes, class_counts, class_indices used in splitTrainTest by dropping the indices that 
        aren't of vType.

        vType         := str; drops classes that are not equal to the vulernable syntax characteristic type pass in.
                       options: AE, ARR, API, or PTR
        classes       := numpy arrays
        class_counts  := numpy arrays
        class_indices := numpy arrays
    """
    dropClasses = [c for c in classes if c != vType]
    for i in dropClasses:
        try:
            drop_index = np.where(classes == i)[0][0]
        except:
            print(f'Drop class {i} not found in classes. Continuing down our list.')
            continue
        print(f'Dropping class {str(i)}. Drop index: {drop_index}. Class count: {class_counts[drop_index]}.')
        class_counts = np.delete(class_counts, drop_index, 0)
        class_indices = np.delete(class_indices, drop_index, 0)
        classes = np.delete(classes, drop_index, 0)

    return classes, class_counts, class_indices

def drop_col_df(df, name, axis=1, inplace=True):
    df.drop(name, axis=axis, inplace=inplace)

def encode_target(categories):
    """
        Transforms categorical target to one hot encoded matrix.
        Returns a dictionary mapping the group id to its encoded counterpart, 
        and the fitted LabelBinarizer instance to help decode later.
    """
    lb = LabelBinarizer()
    lb.fit(categories)
    print(f'\nCategories in our vector train path:\n{lb.classes_}')
    print(f'Total number of classes: {len(lb.classes_)}')
    # Label encoding e.g. [Class A, Class B, Class C] ==> [1,0,0], [0,1,0], [0,0,1]]
    encoded_y = lb.transform(categories) 
    print(f'Encoded classes:\n{encoded_y}\n')

    mapping = {}
    for i in range(len(categories)):
        mapping[categories[i]] =  encoded_y[i]

    return mapping, lb

def flatten_categories(arr, index=0):
    """
        Flattens groupid array for all samples into the ids from selected index.
        index = 0 = group ids;  index = 1 = original CWE ID
        Example: [[664, 664], [703, 555]] --> [664, 703]
    """
    result = [x[index] for x in arr]
    return result

def getDataset(dataset_path, getBalanced=True, RANDOMSEED=None):
    """
        dataset_path := str; path of pkl file from which to get the data from
        getBalanced := boolean; if True, gets files with the name 'balanced' in it 
        RANDOMSEED := int
    """
    dataset = []

    filetype = "balanced" if getBalanced else "ALL"
    for filename in os.listdir(dataset_path):
        if (filename.endswith(".pkl")) and (filetype in filename):
            print("Getting dataset...", os.path.join(dataset_path, filename))
            f = open(os.path.join(dataset_path, filename),"rb")
            dataset = pickle.load(f)
            f.close()

    if RANDOMSEED:
        for i in range(len(dataset)):
            np.random.seed(RANDOMSEED)
            np.random.shuffle(dataset[i])

    return dataset

def get_focus_pointer(sentence):
    """
        Returns focus pointer from header.
    """
    focuspointer = sentence.split(".c")[-1].split(' ')[:-1]
    if focuspointer[0] == '':
        focuspointer = focuspointer[1:]
    if len(focuspointer) < 1:
        raise("no focuspointer found")
    elif len(focuspointer) == 1:
        focuspointer = focuspointer[0]
    else:
        focuspointer = ''.join(focuspointer)
    return focuspointer

def get_latest_epoch(filename):
    epoch = filename.split('-')[-1]
    return epoch

def get_multiclass_dict(multiclasspath):
    """
        multiclasspath := str; path where SARD_CVE_to_groups.csv lives which maps original SARD or CVE IDs to their root CWE-ID.
        Returns a dict with the root CWE-IDs as the keys.
    """
    df = pd.read_csv(multiclasspath, index_col=0)
    categories = np.unique(df['Group ID'])
    categoryDict = {}

    for c in categories:
        categoryDict[c] = []
    return categoryDict

def get_unique_cwes(obsolete=[]):
    """
        pass in a list of obsolete ids if needed. Ex. obsolete = [264, 17, 534, 16]
    """
    df = pd.read_csv(os.path.join('data', 'CWE', 'CWE_DF.csv'), index_col=0)
    if len(obsolete) > 0:
        df = df.drop(obsolete)
    return df.index

def get_classes_gt(multiclasspath, num=900):
    """
        multiclasspath := str; path where SARD_CVE_to_CWE.csv lives which maps original SARD or CVE IDs to their root CWE-ID.
        Returns a dict with the root CWE-IDs as the keys.
    """
    df = pd.read_csv(multiclasspath, index_col=0)
    classes_dict = {}
    for _, row in df.iterrows():
        if row['CWE-ID'] not in classes_dict:
            classes_dict[row['CWE-ID']] = row['counts']
        else:
            classes_dict[row['CWE-ID']] += row['counts']

    # Returns class labels with counts greater than num.
    classes = []
    for k, v in classes_dict.items():
        if v > num:
            classes.append(k)

    return classes

def get_multiclass_label(sard, multiclasspath, id_column='CWE-ID'):
    """
        Matches the SARD ID to its corresponding class (found in multiclasspath).
        The corresponding class is an ID that categorizes the type of CWE vulnerability the sample would fall into.
    """
    label = None
    df = pd.read_csv(multiclasspath, index_col=0)
    try:
        label = int(df.loc[df['Original ID'] == sard][id_column].item())
    except Exception as e:
        pass
        
    return label

def get_predicted_class_and_accuracy(labels):
    accuracies = []
    categories = []
    for l in labels:
        # get index of best accuracy to determine which category it predicted
        best_prediction = 0 
        category = 0
        for index, acc in enumerate(l):
            if acc > best_prediction:
                best_prediction = acc
                category = index
        accuracies.append(best_prediction)
        categories.append(category)
    return accuracies, categories

def get_SARD(header):
    """
        header := str; header in a slice that contains the SARD test case ID.

        Returns the SARD ID in string format.
    """
    head = header.split(' ')[1]
    sard = head.split('/')[0]
    return sard

def get_vul_filename(header):
    """
        header := str; header in a slice that contains a c filename.

        Returns the filename contained in the slice.
    """
    head = header.split(' ')[1]
    vul_filename = head.split('/')[1]
    if not vul_filename.endswith('.c'):
        print('Does not contain a c filename: ', header)
    return vul_filename

def get_sard_cve_ids(filenames):
    ids = []
    for f in filenames:
        x = get_SARD(f)
        ids.append(x)
    return ids

def get_sentences(slicelist):
    sentences = slicelist.split('\n')
    if sentences[0] == '\r' or sentences[0] == '':
        del sentences[0]
    if sentences[-1] == '':
        del sentences[-1]
    if sentences[-1] == '\r':
        del sentences[-1]
    return sentences

def get_type(filename):
    """
        filename := str; name of a source code file
        Returns the syntax vulnerability type found in the filename of the source code file if it exists in the filename.
    """
    myType = 'Others'
    if "API" in filename:
        myType = ['API']
    elif "Array" in filename:
        myType = ['ARR']
    elif "Pointer" in filename:
        myType = ['PTR']
    elif "Arithmetic" in filename:
        myType = ['AE']
    return myType

def init_nested_arrays(num):
    arr = []
    for _ in range(num):
        arr.append([])
    return arr

def levenshtein_ratio_and_distance(s, t, ratio_calc = False):
    """ levenshtein_ratio_and_distance:
        Calculates levenshtein distance between two strings.
        If ratio_calc = True, the function computes the
        levenshtein distance ratio of similarity between two strings
        For all i and j, distance[i,j] will contain the Levenshtein
        distance between the first i characters of s and the
        first j characters of t
    """
    # Initialize matrix of zeros
    rows = len(s)+1
    cols = len(t)+1
    distance = np.zeros((rows,cols),dtype = int)

    # Populate matrix of zeros with the indeces of each character of both strings
    for i in range(1, rows):
        for k in range(1,cols):
            distance[i][0] = i
            distance[0][k] = k

    # Iterate over the matrix to compute the cost of deletions,insertions and/or substitutions    
    for col in range(1, cols):
        for row in range(1, rows):
            if s[row-1] == t[col-1]:
                cost = 0 # If the characters are the same in the two strings in a given position [i,j] then the cost is 0
            else:
                # In order to align the results with those of the Python Levenshtein package, if we choose to calculate the ratio
                # the cost of a substitution is 2. If we calculate just distance, then the cost of a substitution is 1.
                if ratio_calc == True:
                    cost = 2
                else:
                    cost = 1
            distance[row][col] = min(distance[row-1][col] + 1,      # Cost of deletions
                                 distance[row][col-1] + 1,          # Cost of insertions
                                 distance[row-1][col-1] + cost)     # Cost of substitutions
    if ratio_calc == True:
        # Computation of the Levenshtein Distance Ratio
        Ratio = ((len(s)+len(t)) - distance[row][col]) / (len(s)+len(t))
        return Ratio
    else:
        # print(distance) # Uncomment if you want to see the matrix showing how the algorithm computes the cost of deletions,
        # insertions and/or substitutions
        # This is the minimum number of edits needed to convert string a to string b
        return "The strings are {} edits away".format(distance[row][col])

def map_pred_to_cat(y_pred, labelEncoder):
    """
        Converts predicted labels into their corresponding 
        original group id.
        y_true := an array of ints; shape(len(y_pred),)
        labelEncoder := fitted instance of LabelBinarizer()
    """
    result = []
    for label in y_pred:
        result.append(labelEncoder.classes_[label])
    assert(len(y_pred) == len(result))
    return np.array(result)

def num_classes(datapath):
    """
        datapath := str; path to pkl file that contains classes in nested list format; 
                   example: [[697, 697], [664, 0], ...] where the first index of inner list 
                   is the merged group id, and the 2nd is the original
        Gets the total number of unique classes that exist. 
    """
    for filename in os.listdir(datapath):
        if filename.endswith(".pkl"):
            f = open(os.path.join(datapath, filename),"rb")
            data = pickle.load(f)
            f.close()
            if type(data[-2][0]) is not int:
                return np.unique(flatten_categories(data[-2]))
            return np.unique(data[-2])

def print_group_counts(categoryDict):
    print('Categories found in the slice samples')
    print('GROUP ID\tCOUNT')
    for c in categoryDict:
        count = len(categoryDict[c])
        print(f'{c}\t{count}')

def print_num_samples(datapath, getBalanced=True):
    data = getDataset(datapath, getBalanced)
    print(f'Total number of samples in datapath set: {len(data[-2])}')

def output_df_to_excel(df, filename):
    df.to_excel(filename)

def randomly_shuffle_dataset(dataset, randomSeed=1099):
    """ 
        inplace shuffle of our entire dataset
        dataset     := nested arrays that include vects, labels, ids, etc.
        randomSeed  := int
    """
    for i in range(len(dataset)):
        np.random.seed(randomSeed)
        np.random.shuffle(dataset[i])

def save_data_to_file(dirPath, filename, data):
    # Save to a file so we don't have to load all the data every time
    aPath = os.path.join(dirPath, filename)
    aFile = open(aPath, 'wb')
    pickle.dump(data, aFile, protocol=pickle.HIGHEST_PROTOCOL)
    aFile.close()
    print(f'All vectors saved in {aFile}\n')

def split_train_test_sets(class_counts, class_indices, split, group_set):
    # Split into training and test sets
    train_set = [[], [], [], [], [],[], []]
    test_set  = [[], [], [], [], [],[], []]

    for count, indices in zip(class_counts, class_indices):
        training_split = int(count*split) + 1
        label_indices_train = indices[:training_split]
        label_indices_test = indices[training_split:]

        for i in label_indices_train:
            for group_index in range(len(group_set)):
                train_set[group_index].append(group_set[group_index][i])

        for i in label_indices_test:
            for group_index in range(len(group_set)):
                test_set[group_index].append(group_set[group_index][i])

    return train_set, test_set

    
