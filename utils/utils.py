import os, pickle

import numpy as np
from sklearn.preprocessing import LabelBinarizer


def concat_datasets(data1, data2):
    """
        Input := array of arrays; most likely derived from getDataset().
    """
    assert len(data1) == len(data2), "datasets must be of the same length"
    
    for i in range(len(data1)):
        data1[i] = data1[i] + data2[i]        
    
    return data1

def downsample(data, downsampleLabel, downsampleNum):
    """
        data := array of arrays; most likely derived from getDataset().
        downsampleLabel := int; group id from which to downsample
        downsampleNum := int; the maximum number of samples for the downsample label
    """
    if type(data[-2][0]) is not int:
        raise TypeError("Expecting the target column to be a list of ints. If type is a list of lists, please flatten and replace target column with flattened list of ints.  Try `[x[1] for x in data[-2]]` for original group label or `[x[0] for x in data[-2]]` for merged labels.")

    count = 0
    sections = len(data)
    downsampled_data = init_nested_arrays(sections)
    
    for i in range(len(data[0])):
        if (data[-2][i] == downsampleLabel) and (count >= downsampleNum):
            continue
        if data[-2][i] == downsampleLabel:
            count += 1
        for n in range(sections):
            downsampled_data[n].append(data[n][i])            
    return downsampled_data

def save_data_to_file(dirPath, filename, data):
    # Save to a file so we don't have to load all the data every time
    balancedPath = os.path.join(dirPath, filename)
    balanceFile = open(balancedPath, 'wb')
    pickle.dump(data, balanceFile, protocol=pickle.HIGHEST_PROTOCOL)
    balanceFile.close()
    print(f'All vectors saved in {balanceFile}\n')

def encode_target(categories):
    """
        Transforms categorical target to one hot encoded matrix.
        Returns a dictionary mapping the group id to its encoded counterpart, 
        and the fitted LabelBinarizer instance to help decode later.
    """
    lb = LabelBinarizer()
    lb.fit(categories)
    print(f'\nOriginal categories: {lb.classes_}')
    # Label encoding e.g. [Class A, Class B, Class C] ==> [1,0,0], [0,1,0], [0,0,1]]
    encoded_y = lb.transform(categories) 
    print(f'Encoded classes:\n{encoded_y}\n')

    mapping = {}
    for i in range(len(categories)):
        mapping[categories[i]] =  encoded_y[i] 

    return mapping, lb

def flatten_categories(arr):
    """
        Flattens groupid array for all samples into the assigned groupids only.
        Example: [[664, 664], [703, 555]] --> [664, 703]
    """
    result = [x[0] for x in arr]
    return result

def getDataset(dataset_path, getBalanced=True, RANDOMSEED=None):
    """
        dataset_path := str; path of pkl file from which to get the data from
        getBalanced := boolean; if True, gets files with the name 'balanced' in it 
        RANDOMSEED := int
    """
    print("Getting dataset...")
    dataset = []

    filetype = "balanced" if getBalanced else "ALL"
    for filename in os.listdir(dataset_path):
        if (filename.endswith(".pkl")) and (filetype in filename):
            print(filename)
            f = open(os.path.join(dataset_path, filename),"rb")
            data = pickle.load(f)
            f.close()
            dataset = data

    if RANDOMSEED:
        for i in range(len(dataset)):
            np.random.seed(RANDOMSEED)
            np.random.shuffle(dataset[i])

    return dataset

def get_SARD(sentence0):
    header = sentence0.split(' ')[1]
    sard = header.split('/')[0]
    return sard

def get_sard_cve_ids(filenames):
    ids = []
    for f in filenames:
        x = get_SARD(f)
        ids.append(x)
    return ids

def get_predicted_class_and_accuracy(labels):
    accuracies = []
    categories = []
    for l in labels:
        # get index of best accuracy to determine which category it predicted
        best_prediction = 0 
        category = 0
        for index, acc in enumerate(l):
            if acc > best_prediction:
                best_prediction = acc
                category = index
        accuracies.append(best_prediction)
        categories.append(category)
    return accuracies, categories

def getAvgLength(arr):
    """
        Return avg length of the inner arrays within an array,
        and the list of all the lengths in case you want to pick a value other than the avg.
    """
    totalSamples = len(arr)
    totalVectorLen = 0
    vectorLengths = []

    for a in arr:
        v_length = len(a) 
        totalVectorLen += v_length
        vectorLengths.append(v_length)

    vectorLengths.sort()

    meanLen = int(totalVectorLen/totalSamples)
    return meanLen, vectorLengths

def init_nested_arrays(num):
    arr = []
    for i in range(num):
        arr.append([])
    return arr

def map_pred_to_cat(y_pred, labelEncoder):
    """
        Converts predicted labels into their corresponding 
        original group id.
        y_true := an array of ints; shape(len(y_pred),)
        labelEncoder := fitted instance of LabelBinarizer()
    """
    result = []
    for label in y_pred:
        result.append(labelEncoder.classes_[label])
    assert(len(y_pred) == len(result))
    return np.array(result)

def num_classes(datapath):
    """
        dataset := str; path to pkl file that contains classes in nested list format; 
                   example: [[697, 697], [664, 0], ...] where the first index of inner list 
                   is the merged group id, and the 2nd is the original
        Gets the total number of unique classes that exist. 
    """
    for filename in os.listdir(datapath):
        if filename.endswith(".pkl"):
            f = open(os.path.join(datapath, filename),"rb")
            data = pickle.load(f)
            f.close()
            if type(data[-2][0]) is not int:
                return np.unique(flatten_categories(data[-2]))
            return np.unique(data[-2])

def transformVLength(X, maxlen, vector_dim=30):
    """
        Transforms the length of inner arrays within an array 
        to 
    """
    fill_0 = [0] * vector_dim
    if (maxlen != 0):
        new_X = []
        for x in X:  
            # len(x) is how many symbols in 1 program.
            if len(x) <  maxlen:
                x = x + [fill_0] * (maxlen - len(x))
                new_X.append(x)
            else:
                # length of vector is > maxlen
                new_X.append(x[:maxlen])
    print ("New Vector Length: ", len(new_X[0]))
    return new_X

