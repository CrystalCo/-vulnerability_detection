import os, pickle

import numpy as np
import pandas as pd
from sklearn.preprocessing import LabelBinarizer


def concat_datasets(data1, data2):
    """
        Input := array of arrays; most likely derived from getDataset().
    """
    assert len(data1) == len(data2), "datasets must be of the same length"
    
    for i in range(len(data1)):
        data1[i] = data1[i] + data2[i]        
    
    return data1

def downsample(data, downsampleLabel, downsampleNum):
    """
        data := array of arrays; most likely derived from getDataset().
        downsampleLabel := int; group id from which to downsample
        downsampleNum := int; the maximum number of samples for the downsample label
    """
    if type(data[-2][0]) is not int:
        raise TypeError("Expecting the target column to be a list of ints. If type is a list of lists, please flatten and replace target column with flattened list of ints.  Try `[x[1] for x in data[-2]]` for original group label or `[x[0] for x in data[-2]]` for merged labels.")

    count = 0
    sections = len(data)
    downsampled_data = init_nested_arrays(sections)
    
    for i in range(len(data[0])):
        if (data[-2][i] == downsampleLabel) and (count >= downsampleNum):
            continue
        if data[-2][i] == downsampleLabel:
            count += 1
        for n in range(sections):
            downsampled_data[n].append(data[n][i])            
    return downsampled_data

def save_data_to_file(dirPath, filename, data):
    # Save to a file so we don't have to load all the data every time
    balancedPath = os.path.join(dirPath, filename)
    balanceFile = open(balancedPath, 'wb')
    pickle.dump(data, balanceFile, protocol=pickle.HIGHEST_PROTOCOL)
    balanceFile.close()
    print(f'All vectors saved in {balanceFile}\n')

def encode_target(categories):
    """
        Transforms categorical target to one hot encoded matrix.
        Returns a dictionary mapping the group id to its encoded counterpart, 
        and the fitted LabelBinarizer instance to help decode later.
    """
    lb = LabelBinarizer()
    lb.fit(categories)
    print(f'\nOriginal categories: {lb.classes_}')
    # Label encoding e.g. [Class A, Class B, Class C] ==> [1,0,0], [0,1,0], [0,0,1]]
    encoded_y = lb.transform(categories) 
    print(f'Encoded classes:\n{encoded_y}\n')

    mapping = {}
    for i in range(len(categories)):
        mapping[categories[i]] =  encoded_y[i] 

    return mapping, lb

def flatten_categories(arr):
    """
        Flattens groupid array for all samples into the assigned groupids only.
        Example: [[664, 664], [703, 555]] --> [664, 703]
    """
    result = [x[0] for x in arr]
    return result

def getDataset(dataset_path, getBalanced=True, RANDOMSEED=None):
    """
        dataset_path := str; path of pkl file from which to get the data from
        getBalanced := boolean; if True, gets files with the name 'balanced' in it 
        RANDOMSEED := int
    """
    print("Getting dataset...")
    dataset = []

    filetype = "balanced" if getBalanced else "ALL"
    for filename in os.listdir(dataset_path):
        if (filename.endswith(".pkl")) and (filetype in filename):
            print(filename)
            f = open(os.path.join(dataset_path, filename),"rb")
            data = pickle.load(f)
            f.close()
            dataset = data

    if RANDOMSEED:
        for i in range(len(dataset)):
            np.random.seed(RANDOMSEED)
            np.random.shuffle(dataset[i])

    return dataset


def get_focus_pointer(sentence):
    """
        Returns focus pointer from header.
    """
    focuspointer = sentence.split(".c")[-1].split(' ')[:-1]
    if focuspointer[0] == '':
        focuspointer = focuspointer[1:]
    if len(focuspointer) < 1:
        raise("no focuspointer found")
    elif len(focuspointer) == 1:
        focuspointer = focuspointer[0]
    else:
        focuspointer = ''.join(focuspointer)
    return focuspointer

def get_multiclass_dict(multiclasspath):
    """
        multiclasspath := str; path where SARD_CVE_to_groups.csv lives which maps original SARD or CVE IDs to their root CWE-ID.
        Returns a dict with the root CWE-IDs as the keys.
    """
    df = pd.read_csv(multiclasspath, index_col=0)
    categories = np.unique(df['Group ID'])
    categoryDict = {}

    for c in categories:
        categoryDict[c] = []
    return categoryDict


def get_multiclass_groupid(sard, multiclasspath):
    """
        Matches the SARD ID to its corresponding class (found in multiclasspath).
        The corresponding class is a group ID that categorizes the type of CWE vulnerability the sample would fall into.
    """
    group_id = None
    groups_df = pd.read_csv(multiclasspath, index_col=0)
    try:
        group_id = int(groups_df.loc[groups_df['Original ID'] == sard]['Group ID'].item())
    except:
        print(f'SARD {sard} does not have a group id. Skipping the tokenization of this sample.')
        
    return group_id


def get_predicted_class_and_accuracy(labels):
    accuracies = []
    categories = []
    for l in labels:
        # get index of best accuracy to determine which category it predicted
        best_prediction = 0 
        category = 0
        for index, acc in enumerate(l):
            if acc > best_prediction:
                best_prediction = acc
                category = index
        accuracies.append(best_prediction)
        categories.append(category)
    return accuracies, categories

def get_SARD(sentence0):
    """
        sentence0 := str; header in a slice that contains the SARD test case ID
        Returns the SARD ID
    """
    header = sentence0.split(' ')[1]
    sard = header.split('/')[0]
    return sard

def get_sard_cve_ids(filenames):
    ids = []
    for f in filenames:
        x = get_SARD(f)
        ids.append(x)
    return ids

def get_type(filename):
    """
        filename := str; name of a source code file
        Returns the syntax vulnerability type found in the filename of the source code file if it exists in the filename.
    """
    myType = 'Others'
    if "API" in filename:
        myType = ['API']
    elif "Array" in filename:
        myType = ['ARR']
    elif "Pointer" in filename:
        myType = ['PTR']
    elif "Arithmetic" in filename:
        myType = ['AE']
    return myType

def init_nested_arrays(num):
    arr = []
    for i in range(num):
        arr.append([])
    return arr

def map_pred_to_cat(y_pred, labelEncoder):
    """
        Converts predicted labels into their corresponding 
        original group id.
        y_true := an array of ints; shape(len(y_pred),)
        labelEncoder := fitted instance of LabelBinarizer()
    """
    result = []
    for label in y_pred:
        result.append(labelEncoder.classes_[label])
    assert(len(y_pred) == len(result))
    return np.array(result)

def num_classes(datapath):
    """
        datapath := str; path to pkl file that contains classes in nested list format; 
                   example: [[697, 697], [664, 0], ...] where the first index of inner list 
                   is the merged group id, and the 2nd is the original
        Gets the total number of unique classes that exist. 
    """
    for filename in os.listdir(datapath):
        if filename.endswith(".pkl"):
            f = open(os.path.join(datapath, filename),"rb")
            data = pickle.load(f)
            f.close()
            if type(data[-2][0]) is not int:
                return np.unique(flatten_categories(data[-2]))
            return np.unique(data[-2])

def print_group_counts(categoryDict):
    print('Categories found in the slice samples')
    print('GROUP ID\tCOUNT')
    for c in categoryDict:
        count = len(categoryDict[c])
        print(f'{c}\t{count}')


