import os, sys

VUL_PATH = os.environ.get('VUL_PATH')
sys.path.insert(1, VUL_PATH)

from SYSE_2_vulnerabilityType.VulnerabilityClassification import VulnerabilityClassification
from utils.CrossValidation import CrossValidation
from utils.DLCustomModels import create_bgru_model
from utils.DLCustomModels import create_bgru_model, create_blstm_model
from utils.transformDataDimensions import FlattenVectors
from utils.utils import GetData, GetDataByIndex, SaveData, preprocess_data_filter_by_count


def main(VUL_TYPE, vectorsALLdir='ALL_vectors', MODEL_TYPE='bgru', model_fn=create_bgru_model, min_num=None, k=5):
    RANDOMSEED = 1099
    CLASS_TYPE = f'{VUL_TYPE}_vulnerable_min_{min_num}_SySe'
    BATCHSIZE = 64
    LAYERS = 2
    DROPOUT = 0.2
    EPOCHS = 60
    OPTIMIZER = 'ADAM'
    ACTIVATION_FN = 'sigmoid'
    RECURRENT_ACTIVATION = 'hard_sigmoid'
    DENSE_ACT_FN = 'softmax'
    UNITS = 256
    METRICS = ['CategoricalAccuracy', 'Recall']

    inputsRootPath = os.path.join('data', 'DLinputs')
    vectorRootPath = os.path.join('data', 'DLvectors')
    vectorsALLPath = os.path.join(vectorRootPath, vectorsALLdir)
    vectorTrainPath = os.path.join(vectorRootPath, 'train')
    vectorTestPath = os.path.join(vectorRootPath, 'test')
    dlInputsTrainPath = os.path.join(inputsRootPath, 'train')
    dlInputsTestPath = os.path.join(inputsRootPath, 'test')

    checkpoint_dir = './ckpt_%s_%s_%s' % (MODEL_TYPE, str(BATCHSIZE), CLASS_TYPE)
    model_name = '%s_epochs_%s_opt_%s_%s_%s_%sunits_%s' % (MODEL_TYPE.upper(), EPOCHS, OPTIMIZER, ACTIVATION_FN, DENSE_ACT_FN, UNITS, CLASS_TYPE)
    metrics_path = os.path.join(f'metrics', MODEL_TYPE)
    weights_path = os.path.join('model', model_name + '_weights')

    VC = VulnerabilityClassification(build_model=model_fn, useGenerator=True,
        metricsPath=metrics_path, randomSeed=RANDOMSEED, m_epochs=EPOCHS,
        modelName=model_name, batch_size=BATCHSIZE, mask=True, dropout=DROPOUT, layers=LAYERS, vectorsALLPath=vectorsALLPath,
        vectorTrainPath=vectorTrainPath, vectorTestPath=vectorTestPath,
        inputsTrainPath=dlInputsTrainPath, inputsTestPath=dlInputsTestPath,
        checkpoint_dir=checkpoint_dir, weightpath=weights_path, optimizer=OPTIMIZER,
        activation_fn=ACTIVATION_FN, recurrent_activation=RECURRENT_ACTIVATION, dense_activation_fn=DENSE_ACT_FN,
        units=UNITS, metrics=METRICS)


    print('Starting %s & %s' % (CLASS_TYPE, MODEL_TYPE))

    # Drop classes that don't meet the min # of samples threshold
    def drop_classes_by_count():
        input_path = os.path.join(VC.vectorsALLPath, 'ALL_vectors.pkl')
        output_path = os.path.join(vectorRootPath, 'vectors', 'ALL_vectors.pkl')
        data = GetData(input_path)
        data = preprocess_data_filter_by_count(min_num, data)
        SaveData(output_path, data)
        VC.vectorsALLPath = os.path.join(vectorRootPath, 'vectors')
    drop_classes_by_count()

    # To flatten dataset, we must average out the dimension of the dataset which contains the tokens. Averages out the row length per sample based on focuspointer.
    print('\nAdjusting Vector Length...')
    VC.get_avg_length(VC.vectorsALLPath)
    print("Hard coding avg to be 214 to match what we used to have.")
    VC.avg = 214
    VC.SetVLength(VC.vectorsALLPath, VC.vectorsALLPath)

    # Flatten 3D vectors to 2D
    FlattenVectors(VC.vectorsALLPath, VC.vectorsALLPath, VC.avg, VC.vector_size)

    # Don't adjust the row length since they're already flattened. Manually set the new vector length & avg.
    VC.vector_size = len(GetDataByIndex(VC.vectorsALLPath, index=0)[0])
    VC.avg = 1
    print(f'Avg: {VC.avg}\tVector length: {VC.vector_size}\n')

    CV = CrossValidation(VC, k=k)
    CV.main()


VUL_TYPE = 'ALL'
vectorsALLdir='ALL_vulnerable_vectors_sysevr'
min_number = 100
k = 2

### BGRU ###
main(VUL_TYPE, vectorsALLdir=vectorsALLdir, min_num=min_number, k=k)

### BLSTM ###
# main(VUL_TYPE, vectorsALLdir=vectorsALLdir, MODEL_TYPE='blstm', model_fn=create_blstm_model, min_num=min_number)




